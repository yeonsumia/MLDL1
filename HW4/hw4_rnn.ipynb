{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4_rnn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Environment Setup"],"metadata":{"id":"xvh969t1VJzj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"kO4EhYqCTdbt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654011359220,"user_tz":-540,"elapsed":17612,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"1c59e695-1e3d-437b-cd86-e084ed932fca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd /content/drive/MyDrive/study/Github/MLDL1/HW4"],"metadata":{"id":"gyi5Ljv8TpjO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654011359495,"user_tz":-540,"elapsed":280,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"a7b16c6e-cb9d-49fc-9f76-05df22208e6f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/study/Github/MLDL1/HW4\n"]}]},{"cell_type":"code","source":["! pip install torchtext"],"metadata":{"id":"fDk49qpsU_BK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653992763597,"user_tz":-540,"elapsed":4871,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"34e9db97-b497-461f-f73c-353b88e43258"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n","Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n"]}]},{"cell_type":"code","source":["import math\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torchtext\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset"],"metadata":{"id":"NMAuwnOwVBOu","executionInfo":{"status":"ok","timestamp":1654011410924,"user_tz":-540,"elapsed":3071,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","import modules you need\n","\"\"\""],"metadata":{"id":"I6fOONHU7aMR","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1653986693401,"user_tz":-540,"elapsed":9,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"1c0fae30-4ad1-45a4-9a46-7b39cca1e657"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport modules you need\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE)) ## should be 1.11.0 and cuda\n","print(\"Using torchtext version: {}\".format(torchtext.__version__)) ## should be 0.12.0"],"metadata":{"id":"yn5FYiiQVC2k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654011412879,"user_tz":-540,"elapsed":274,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"9748f4aa-d6bf-46ce-df25-09c6e8da48b4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using PyTorch version: 1.11.0+cu113, Device: cuda\n","Using torchtext version: 0.12.0\n"]}]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"E4E1p3w-VLTq"}},{"cell_type":"code","source":["\"\"\"\n","Load AG_NEWS dataset and set up the tokenizer and encoder pipeline.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","train_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n","\n","tokenizer = get_tokenizer('basic_english')\n","\n","def tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n","encoder.set_default_index(encoder[\"<unk>\"])\n","\n","text_pipeline = lambda x: encoder(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"],"metadata":{"id":"NXSORPxqn7lU","colab":{"base_uri":"https://localhost:8080/","height":532},"executionInfo":{"status":"error","timestamp":1654011415645,"user_tz":-540,"elapsed":289,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"2652b100-3dc9-469c-9742-e175d75d705d"},"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a85bb69e81e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAG_NEWS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/datasets_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/datasets_utils.py\u001b[0m in \u001b[0;36mnew_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_check_default_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/datasets/ag_news.py\u001b[0m in \u001b[0;36mAG_NEWS\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_module_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchdata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         raise ModuleNotFoundError(\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;34m\"Package `torchdata` not found. Please install following instructions at `https://github.com/pytorch/data`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: Package `torchdata` not found. Please install following instructions at `https://github.com/pytorch/data`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["def collate_batch(batch):\n","    \"\"\"\n","    Creates a batch of encoded text, label and token length tensors.\n","\n","    Question (a)\n","    - Length of token sequence in each batch is determined by \n","      the average of token length of all sequences in each batch.\n","    - Text tensors are stacked with dimension of (TOKEN_LENGTH, BATCH),\n","      for easier process in RNN model.\n","    - Token length tensors are used to index the last valid hidden token for classification.\n","\n","    Inputs\n","    - list of tuples, each containing an integer label and a text input\n","    - number of tuples in the list == BATCH SIZE\n","    Returns\n","    - text_list: batch of encoded long type text tensors with size (TOKEN_LENGTH, BATCH)\n","    - label_list: batch of label tensors with size (BATCH)\n","    - len_list: batch of token length tensors with size (BATCH)\n","    \"\"\"\n","\n","    text_list, label_list, len_list = [], [], []\n","    \n","    ### COMPLETE HERE ###\n","    tmp_text_list = []\n","    tokenizer = get_tokenizer('basic_english')\n","    encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n","    for (label, text) in batch:\n","      processed_txt = torch.tensor(text_pipeline(text), dtype=torch.int64)\n","      len_list.append(processed_txt.size(0))\n","      tmp_text_list.append(processed_txt)\n","      label_list.append(label_pipeline(label))\n","      \n","    TOKEN_LENGTH = int(sum(len_list) / len(len_list))\n","\n","    for processed_txt in tmp_text_list:\n","      if processed_txt.size(0) >= TOKEN_LENGTH:\n","        processed_txt = processed_txt[:TOKEN_LENGTH]\n","      else:\n","        processed_txt = torch.cat([processed_txt, torch.zeros(TOKEN_LENGTH-processed_txt.size(0))])\n","      text_list.append(processed_txt)\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    text_list = torch.stack(text_list, dim=1).long() # append text in dim=1 to make size=(TOKEN_LENGTH, BATCH) \n","    ### COMPLETE HERE ###\n","    \n","    assert text_list.size(1) == len(batch)\n","\n","    return text_list, label_list, len_list"],"metadata":{"id":"jpb0L6hvmh2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Load the data loader.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","BATCH_SIZE = 512\n","\n","train_dataset = to_map_style_dataset(train_data)\n","test_dataset = to_map_style_dataset(test_data)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)"],"metadata":{"id":"nJXoUuq0NgiV","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1653992583065,"user_tz":-540,"elapsed":5,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"5986c8fc-5713-4c9f-8dfb-ddec3974772e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-684e3083b271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_map_style_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_map_style_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"cell_type":"code","source":["\"\"\"\n","Print out the first batch in the train loader.\n","Check if the collate function is implemented correctly.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","for batch_x, batch_y, len_x in train_dataloader:\n","    print(batch_x.size)\n","    print(batch_x[:10])\n","    print(batch_y[:10])\n","    print(len_x[:10])\n","    break"],"metadata":{"id":"4X2_aQKhxWV3","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1653992583430,"user_tz":-540,"elapsed":4,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"f2701614-54ea-496d-e987-a539e184019f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ac826289525f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","source":["\"\"\"\n","Plot the sequence length distribution of the batches in the train dataloader.\n","Make sure that all batches have difference sequence lengths.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","batch_len = []\n","for batch_x, _, _ in train_dataloader:\n","    batch_len.append(batch_x.size(0))\n","plt.hist(batch_len)"],"metadata":{"id":"95s-BUHc37O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"kR_cAYRUVbor"}},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, input_size, hidden_size, num_class):\n","        \"\"\"\n","        Define the model weight parameters and initialize the weights.\n","\n","        Question (b)\n","        - Complete the dimension and shape of the weights and biases.\n","        - Use the model parameters (vocab_size, input_size, hidden_size, num_class).\n","        \"\"\"\n","        super(RNN, self).__init__()\n","\n","        ### COMPLETE HERE ###\n","        whh_size = (input_size, 1)\n","        wxh_size = (input_size, 1)\n","        why_size = \n","        bhh_size = \n","        bxh_size = \n","        bhy_size = \n","        ### COMPLETE HERE ###\n","\n","        kwargs = {'device': DEVICE, 'dtype': torch.float}\n","        self.hidden = hidden_size\n","        self.num_class = num_class\n","        self.embedding = nn.Embedding(vocab_size, input_size)\n","        self.W_hh = nn.parameter.Parameter(torch.empty(whh_size, **kwargs))\n","        self.W_xh = nn.parameter.Parameter(torch.empty(wxh_size, **kwargs))\n","        self.W_hy = nn.parameter.Parameter(torch.empty(why_size, **kwargs))\n","        self.b_hh = nn.parameter.Parameter(torch.empty(bhh_size, **kwargs))\n","        self.b_xh = nn.parameter.Parameter(torch.empty(bxh_size, **kwargs))\n","        self.b_hy = nn.parameter.Parameter(torch.empty(bhy_size, **kwargs))\n","\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        \"\"\"\n","        Initialize the parameters with Kaiming uniform initialization.\n","\n","        Do NOT modify this method.\n","        \"\"\"\n","        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hh)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_hh, -bound, bound)\n","        nn.init.kaiming_uniform_(self.W_xh, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_xh)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_xh, -bound, bound)\n","        nn.init.kaiming_uniform_(self.W_hy, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hy)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_hy, -bound, bound)\n","\n","    def forward(self, inputs, length):\n","        \"\"\"\n","        Question (c)\n","        - Passes a sequence of tokens into the recurrent network.\n","        - Randomly initialize h_0 with appropriate shape.\n","        - We do not want to use a hidden cell of a zero-padded token for classification!\n","        - Index the hidden state of the last valid token (excluding the zero-padding)\n","          based on the token length of each example in the batch.\n","\n","        Inputs\n","        - a batch of encoded token sequences with shape (SEQ_LEN, BATCH_SIZE)\n","        - a batch of token lengths with shape (BATCH_SIZE)\n","        Returns\n","        - Softmax probabilites for each class with shape (BATCH_SIZE, NUM_CLASS)\n","        \"\"\"\n","        \n","        ### COMPLETE HERE ###\n","\n","        ### COMPLETE HERE ###\n","\n","        return softmax_probs\n","    \n","    def compute_loss(self, prediction, label):\n","        \"\"\"\n","        Question (d)\n","        - Compute the cross entropy loss and the number of correct predictions\n","        - Do NOT use loss function in torch.nn library ex) nn.CrossEntropyLoss()\n","        - Hint: use torch.nn.functional.one_hot(tensor, num_classes=?) to generate one-hot encodings\n","\n","\n","        Inputs\n","        - prediction: output from self.forward(inputs) with shape (BATCH_SIZE, NUM_CLASS)\n","        - label: integer labels of the batch inputs with shape (BATCH_SIZE)\n","        Returns\n","        - cross entropy loss of the batch (float) and number of correct predictions (integer)\n","        \"\"\"\n","        loss = 0\n","        correct = 0\n","\n","        ### COMPLETE HERE ###\n","        \n","        ### COMPLETE HERE ###\n","\n","        return loss, correct"],"metadata":{"id":"9CMsOucsVbVE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Modules"],"metadata":{"id":"zIdypCAFscfV"}},{"cell_type":"code","source":["class ScheduledOptim():\n","    \"\"\"\n","    Learning rate scheduler.\n","\n","    Do NOT modify.\n","    \"\"\"\n","\n","    def __init__(self, optimizer, n_warmup_steps, decay_rate):\n","        self._optimizer = optimizer\n","        self.n_warmup_steps = n_warmup_steps\n","        self.decay = decay_rate\n","        self.n_steps = 0\n","        self.initial_lr = optimizer.param_groups[0]['lr']\n","        self.current_lr = optimizer.param_groups[0]['lr']\n","\n","    def zero_grad(self):\n","        self._optimizer.zero_grad()\n","    \n","    def step(self):\n","        self._optimizer.step()\n","    \n","    def get_lr(self):\n","        return self.current_lr\n","    \n","    def update(self):\n","        if self.n_steps < self.n_warmup_steps:\n","            lr = self.n_steps / self.n_warmup_steps * self.initial_lr\n","        elif self.n_steps == self.n_warmup_steps:\n","            lr = self.initial_lr\n","        else:\n","            lr = self.current_lr * self.decay\n","        \n","        self.current_lr = lr\n","        for param_group in self._optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","        self.n_steps += 1"],"metadata":{"id":"o9lUSQJjsfrQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Functions for training and evaluating the model.\n","\n","Question (e)\n","- There has been minor changes with the model forward operation and loss computation.\n","  Compare the updates with the train, evaluate functions that we have previously used,\n","  and complete the train and evaluate function that works for the current model architecture.\n","- Use the methods of the scheduler to perform necessary operations on the optimizer.\n","- Do NOT change the arguments given to the train, evaluate functions.\n","\"\"\"\n","\n","def train(model, train_loader, scheduler):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    \n","    ### COMPLETE HERE ###\n","    tqdm_bar = tqdm(train_loader)\n","\n","    for text, label, length in tqdm_bar:\n","        text = text.to(DEVICE)\n","        label = label.to(DEVICE)\n","        length = length.to(DEVICE)\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = 100. * correct / len(train_loader.dataset)\n","    ### COMPLETE HERE ###\n","    \n","    return train_loss, train_acc\n","\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    ### COMPLETE HERE ###\n","    \n","    ### COMPLETE HERE ###\n","\n","    return test_loss, test_acc"],"metadata":{"id":"pgisHLbUsLkM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"a1qXY-CDNIia"}},{"cell_type":"code","source":["\"\"\"\n","Question (f)\n","- Train your RNN model and obtain the test accuracy of 70%.\n","- Select the input size, hidden size of your choice\n","- Try various optimizer type, learning rate and scheduler options for the best performance.\n","\"\"\"\n","\n","### COMPLETE HERE ###\n","EPOCHS = 0\n","vocab_size = 0\n","input_size = 0\n","hidden_size = 0\n","num_class = 0\n","\n","model = None\n","optimizer = None\n","scheduler = None\n","### COMPLETE HERE ###\n","\n","for epoch in range(1, EPOCHS + 1):\n","    loss_train, accu_train = train(model, train_dataloader, scheduler)\n","    loss_val, accu_val = evaluate(model, valid_dataloader)\n","    lr = scheduler.get_lr()\n","    print('-' * 83)\n","    print('| end of epoch {:2d} | lr: {:5.4f} | train accuracy: {:8.3f} | '\n","          'valid accuracy {:8.3f} '.format(epoch, lr, accu_train, accu_val))\n","    print('-' * 83)"],"metadata":{"id":"EA061dLNNKg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JDMiURv9hjfb"},"execution_count":null,"outputs":[]}]}