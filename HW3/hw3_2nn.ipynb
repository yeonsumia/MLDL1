{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_2nn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"er0RD438gRLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653264382825,"user_tz":-540,"elapsed":14942,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"5c9b4f49-fbea-4ee1-e988-f0ea4e6b7c9a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Jfeql_8sgnKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653264383266,"user_tz":-540,"elapsed":445,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"fd151743-8ac5-4d54-f745-0ac3fce48781"},"source":["%cd /content/drive/MyDrive/study/Github/MLDL1/HW3"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/study/Github/MLDL1/HW3\n"]}]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","metadata":{"id":"OyammZP8hI7P","executionInfo":{"status":"ok","timestamp":1653264422795,"user_tz":-540,"elapsed":993,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["#Utils"]},{"cell_type":"code","metadata":{"id":"xuQB6W2U5ZE2","executionInfo":{"status":"ok","timestamp":1653264423828,"user_tz":-540,"elapsed":4,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["def sigmoid(z):\n","    \"\"\"\n","    Do NOT modify this function\n","    \"\"\"\n","    return 1/(1+np.exp(-z))\n","\n","def softmax(X):\n","    \"\"\"\n","    Do NOT modify this function\n","    \"\"\"\n","    logit = np.exp(X-np.amax(X, axis=1, keepdims=True))\n","    numer = logit\n","    denom = np.sum(logit, axis=1, keepdims=True)\n","    return numer/denom\n","\n","def relu(X):\n","    return np.where(X>0,X,0)\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["#2-Layer Neural Network"]},{"cell_type":"code","metadata":{"id":"mA5udiGmhRb5","executionInfo":{"status":"ok","timestamp":1653264673652,"user_tz":-540,"elapsed":276,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (a)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n","        \n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","\n","        params = {}\n","        bound = 1/np.sqrt(input_dim)\n","        params[\"W1\"] = np.random.uniform(-bound, bound, size=(input_dim,num_hiddens))\n","        params[\"b1\"] = np.zeros((num_hiddens,))\n","        params[\"W2\"] = np.random.uniform(-bound, bound, size=(num_hiddens,num_classes))\n","        params[\"b2\"] = np.zeros((num_classes,))\n","\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structure is given by\n","\n","          y = softmax(sigmoid(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (b)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","\n","        ff_dict = {}\n","        ff_dict[\"s1\"] = sigmoid(X.dot(self.params[\"W1\"]) + self.params[\"b1\"])\n","        ff_dict[\"s2\"] = softmax(ff_dict[\"s1\"].dot(self.params[\"W2\"]) + self.params[\"b2\"])\n","        y = ff_dict[\"s2\"]\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (c)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        # loss function: cross entropy\n","        grads_softmax = ff_dict[\"s2\"] - np.where(Y == 1,1,0)\n","        grads[\"db2\"] = grads_softmax.sum(axis=0)\n","        grads[\"dW2\"] = ff_dict[\"s1\"].T.dot(grads_softmax)\n","\n","        grads_s1 = grads_softmax.dot(self.params[\"W2\"].T)\n","        grads_sigmoid = grads_s1*(ff_dict[\"s1\"]*(1-ff_dict[\"s1\"]))\n","        grads[\"db1\"] = grads_sigmoid.sum(axis=0)\n","        grads[\"dW1\"] = X.T.dot(grads_sigmoid)\n","\n","        return grads\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","        \n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"        \n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["#Load MNIST"]},{"cell_type":"code","metadata":{"id":"48ooR6YIxYhC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653264685842,"user_tz":-540,"elapsed":1708,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"b4d60710-a0e6-4f6c-a826-c75cfea463be"},"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 10)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 10)\n","\n","Set validation data aside\n","Training data shape:  (48000, 784)\n","Training labels shape:  (48000, 10)\n","Validation data shape:  (12000, 784)\n","Validation labels shape:  (12000, 10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["#Training & Evaluation"]},{"cell_type":"code","metadata":{"id":"IlnC_rerHPaN","executionInfo":{"status":"ok","timestamp":1653264461766,"user_tz":-540,"elapsed":286,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["### \n","# Question (d)\n","# Tune the hyperparameters with validation data, \n","# and print the results by running the lines below.\n","###"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cWb6xg0NxOs","executionInfo":{"status":"ok","timestamp":1653264459939,"user_tz":-540,"elapsed":300,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["# hyperparameter tuning\n","# lr_lst = np.random.uniform(1,3,size=5)\n","# n_epochs_lst = [15, 20, 25]\n","# batch_size_lst = [128, 256, 512]\n","# num_hiddens_lst = [32, 64, 128]\n","# combinations = [(lr, n_epochs, batch_size, num_hiddens) \n","#                   for lr in lr_lst for n_epochs in n_epochs_lst \n","#                     for batch_size in batch_size_lst for num_hiddens in num_hiddens_lst]\n","# for i, (lr, n_epochs, batch_size, num_hiddens) in enumerate(combinations):\n","#   print('Combination {:02} - lr: {:.3f}, n_epochs: {}, batch_size: {}, num_hiddens: {}'.\\\n","#                       format(i, lr, n_epochs, batch_size, num_hiddens))\n","#   model = TwoLayerNN(input_dim=784, num_hiddens=num_hiddens, num_classes=10)\n","#   model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTCqVT4S0Tm5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653265094906,"user_tz":-540,"elapsed":70193,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"18045115-a625-43d1-819b-71a9bee09d8f"},"source":["# the best combination of hyperparameters from cross-validation\n","model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)\n","lr, n_epochs, batch_size = 0.390, 25, 128\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.181 0.948, valid loss/acc: 0.195 0.942\n","epoch 01 - train loss/acc: 0.120 0.965, valid loss/acc: 0.144 0.957\n","epoch 02 - train loss/acc: 0.098 0.970, valid loss/acc: 0.123 0.963\n","epoch 03 - train loss/acc: 0.076 0.978, valid loss/acc: 0.108 0.967\n","epoch 04 - train loss/acc: 0.070 0.980, valid loss/acc: 0.105 0.968\n","epoch 05 - train loss/acc: 0.049 0.987, valid loss/acc: 0.089 0.972\n","epoch 06 - train loss/acc: 0.043 0.988, valid loss/acc: 0.088 0.973\n","epoch 07 - train loss/acc: 0.038 0.990, valid loss/acc: 0.081 0.975\n","epoch 08 - train loss/acc: 0.031 0.993, valid loss/acc: 0.080 0.974\n","epoch 09 - train loss/acc: 0.026 0.995, valid loss/acc: 0.076 0.975\n","epoch 10 - train loss/acc: 0.022 0.996, valid loss/acc: 0.077 0.976\n","epoch 11 - train loss/acc: 0.020 0.996, valid loss/acc: 0.075 0.976\n","epoch 12 - train loss/acc: 0.019 0.996, valid loss/acc: 0.079 0.977\n","epoch 13 - train loss/acc: 0.016 0.997, valid loss/acc: 0.078 0.975\n","epoch 14 - train loss/acc: 0.016 0.997, valid loss/acc: 0.082 0.975\n","epoch 15 - train loss/acc: 0.013 0.998, valid loss/acc: 0.077 0.976\n","epoch 16 - train loss/acc: 0.013 0.997, valid loss/acc: 0.081 0.976\n","epoch 17 - train loss/acc: 0.009 0.999, valid loss/acc: 0.076 0.976\n","epoch 18 - train loss/acc: 0.008 0.999, valid loss/acc: 0.077 0.977\n","epoch 19 - train loss/acc: 0.008 1.000, valid loss/acc: 0.077 0.977\n","epoch 20 - train loss/acc: 0.007 1.000, valid loss/acc: 0.076 0.977\n","epoch 21 - train loss/acc: 0.006 1.000, valid loss/acc: 0.076 0.978\n","epoch 22 - train loss/acc: 0.006 1.000, valid loss/acc: 0.078 0.977\n","epoch 23 - train loss/acc: 0.005 1.000, valid loss/acc: 0.077 0.978\n","epoch 24 - train loss/acc: 0.005 1.000, valid loss/acc: 0.077 0.977\n"]}]},{"cell_type":"code","metadata":{"id":"hpPsAlXU0T_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653265102483,"user_tz":-540,"elapsed":303,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"84386811-a459-4611-be72-642c574bbab3"},"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Final test loss = 0.073, acc = 0.980\n"]}]},{"cell_type":"markdown","metadata":{"id":"5hh1PZpk_g0I"},"source":["# Extra Credit (Optional)"]},{"cell_type":"code","metadata":{"id":"0R0n6y9_AgXc","executionInfo":{"status":"ok","timestamp":1653264777602,"user_tz":-540,"elapsed":311,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","    \"\"\"\n","    initializes parameters with He Initialization.\n","\n","    Question (e)\n","    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n","    \n","    Inputs\n","    - input_dim\n","    - num_hiddens\n","    - num_classes\n","    Returns\n","    - params: a dictionary with the initialized parameters.\n","    \"\"\"\n","    params = {}\n","    mu, sigma1, sigma2 = 0, np.sqrt(2/input_dim), np.sqrt(2/num_hiddens)\n","    params[\"W1\"] = np.random.normal(mu,sigma1,size=(input_dim,num_hiddens))\n","    params[\"b1\"] = np.zeros((num_hiddens,))\n","    params[\"W2\"] = np.random.normal(mu,sigma2,size=(num_hiddens,num_classes))\n","    params[\"b2\"] = np.zeros((num_classes,))\n","\n","    return params\n","\n","def forward_relu(self, X):\n","    \"\"\"\n","    Defines and performs the feed forward step of a two-layer neural network.\n","    Specifically, the network structue is given by\n","\n","        y = softmax(relu(X W1 + b1) W2 + b2)\n","\n","    where X is the input matrix of shape (N, D), y is the class distribution matrix\n","    of shape (N, C), N is the number of examples (either the entire dataset or\n","    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","    Question (e)\n","\n","    Inputs\n","        X: the input matrix of shape (N, D)\n","\n","    Returns\n","        y: the output of the model\n","        ff_dict: a dictionary containing all the fully connected units and activations.\n","    \"\"\"\n","    ff_dict = {}\n","    ff_dict[\"s1\"] = relu(X.dot(self.params[\"W1\"]) + self.params[\"b1\"])\n","    ff_dict[\"s2\"] = softmax(ff_dict[\"s1\"].dot(self.params[\"W2\"]) + self.params[\"b2\"]) \n","    y = ff_dict[\"s2\"]\n","\n","    return y, ff_dict\n","\n","def backward_relu(self, X, Y, ff_dict):\n","    \"\"\"\n","    Performs backpropagation over the two-layer neural network, and returns\n","    a dictionary of gradients of all model parameters.\n","\n","    Question (e)\n","\n","    Inputs:\n","        - X: the input matrix of shape (B, D), where B is the number of examples\n","            in a mini-batch, D is the feature dimensionality.\n","        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","            where B is the number of examples in a mini-batch, C is the number\n","            of classes.\n","        - ff_dict: the dictionary containing all the fully connected units and\n","            activations.\n","\n","    Returns:\n","        - grads: a dictionary containing the gradients of corresponding weights\n","            and biases.\n","    \"\"\"\n","    grads = {}\n","    grads_softmax = ff_dict[\"s2\"] - np.where(Y==1,1,0)\n","    grads[\"dW2\"] = ff_dict[\"s1\"].T.dot(grads_softmax)\n","    grads[\"db2\"] = np.sum(grads_softmax, axis=0)\n","\n","    grads_s1 = grads_softmax.dot(self.params[\"W2\"].T)\n","    grads_relu = grads_s1 * np.where(ff_dict[\"s1\"]>0,1,0)\n","    grads[\"dW1\"] = X.T.dot(grads_relu)\n","    grads[\"db1\"] = np.sum(grads_relu, axis=0)\n","\n","    return grads\n","\n","TwoLayerNNRelu = copy.copy(TwoLayerNN)\n","TwoLayerNNRelu.initialize_parameters = initialize_parameters\n","TwoLayerNNRelu.forward = forward_relu\n","TwoLayerNNRelu.backward = backward_relu"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qY3T98XvEIP","executionInfo":{"status":"ok","timestamp":1653264789281,"user_tz":-540,"elapsed":267,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["### \n","# Question (e)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"],"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# hyperparameter tuning\n","# lr_lst = np.random.uniform(0.1,0.3,size=5)\n","# n_epochs_lst = [15, 20, 25]\n","# batch_size_lst = [128, 256, 512]\n","# num_hiddens_lst = [32, 64, 128]\n","# combinations = [(lr, n_epochs, batch_size, num_hiddens) \n","#                   for lr in lr_lst for n_epochs in n_epochs_lst \n","#                     for batch_size in batch_size_lst for num_hiddens in num_hiddens_lst]\n","# for i, (lr, n_epochs, batch_size, num_hiddens) in enumerate(combinations):\n","#   print('Combination {:02} - lr: {:.3f}, n_epochs: {}, batch_size: {}, num_hiddens: {}'.\\\n","#                       format(i, lr, n_epochs, batch_size, num_hiddens))\n","#   model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=num_hiddens, num_classes=10)\n","#   model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"metadata":{"id":"QjFzVrnKhYUZ","executionInfo":{"status":"ok","timestamp":1653264792336,"user_tz":-540,"elapsed":291,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC8f80a0w53m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653265577617,"user_tz":-540,"elapsed":66307,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"2f9f1b8f-46df-4028-8fd1-0c48ffbb8d0b"},"source":["# the best combination of hyperparameters from cross-validation\n","model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=128, num_classes=10)\n","lr, n_epochs, batch_size = 0.264, 25, 128\n","history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.220 0.938, valid loss/acc: 0.229 0.936\n","epoch 01 - train loss/acc: 0.158 0.955, valid loss/acc: 0.174 0.950\n","epoch 02 - train loss/acc: 0.121 0.966, valid loss/acc: 0.140 0.960\n","epoch 03 - train loss/acc: 0.096 0.974, valid loss/acc: 0.121 0.964\n","epoch 04 - train loss/acc: 0.082 0.978, valid loss/acc: 0.112 0.966\n","epoch 05 - train loss/acc: 0.072 0.980, valid loss/acc: 0.102 0.970\n","epoch 06 - train loss/acc: 0.059 0.984, valid loss/acc: 0.093 0.972\n","epoch 07 - train loss/acc: 0.054 0.986, valid loss/acc: 0.092 0.971\n","epoch 08 - train loss/acc: 0.049 0.987, valid loss/acc: 0.091 0.972\n","epoch 09 - train loss/acc: 0.046 0.987, valid loss/acc: 0.089 0.973\n","epoch 10 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.974\n","epoch 11 - train loss/acc: 0.036 0.991, valid loss/acc: 0.082 0.975\n","epoch 12 - train loss/acc: 0.031 0.993, valid loss/acc: 0.080 0.975\n","epoch 13 - train loss/acc: 0.029 0.994, valid loss/acc: 0.081 0.976\n","epoch 14 - train loss/acc: 0.024 0.995, valid loss/acc: 0.077 0.977\n","epoch 15 - train loss/acc: 0.022 0.996, valid loss/acc: 0.077 0.976\n","epoch 16 - train loss/acc: 0.022 0.996, valid loss/acc: 0.080 0.975\n","epoch 17 - train loss/acc: 0.019 0.997, valid loss/acc: 0.077 0.976\n","epoch 18 - train loss/acc: 0.017 0.998, valid loss/acc: 0.077 0.977\n","epoch 19 - train loss/acc: 0.015 0.998, valid loss/acc: 0.075 0.978\n","epoch 20 - train loss/acc: 0.014 0.998, valid loss/acc: 0.077 0.977\n","epoch 21 - train loss/acc: 0.014 0.998, valid loss/acc: 0.079 0.976\n","epoch 22 - train loss/acc: 0.014 0.998, valid loss/acc: 0.079 0.977\n","epoch 23 - train loss/acc: 0.011 0.999, valid loss/acc: 0.076 0.978\n","epoch 24 - train loss/acc: 0.010 0.999, valid loss/acc: 0.076 0.978\n"]}]},{"cell_type":"code","metadata":{"id":"4i__6TfpCqOc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653265580086,"user_tz":-540,"elapsed":303,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"e50bbf06-33cb-4db0-aa5e-1b1481ce897b"},"source":["# evalute the model on test data\n","Y_hat, _ = model_relu.forward(X_test)\n","test_loss = model_relu.compute_loss(Y_test, Y_hat)\n","test_acc = model_relu.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Final test loss = 0.070, acc = 0.979\n"]}]}]}