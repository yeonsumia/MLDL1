{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_2nn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"er0RD438gRLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652284939127,"user_tz":-540,"elapsed":15296,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"90617449-3e59-46f6-921c-159d61a6daf4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Jfeql_8sgnKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652284940800,"user_tz":-540,"elapsed":628,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"5bb486bb-e28e-428c-c369-b5da01e9a4b9"},"source":["%cd /content/drive/MyDrive/study/Github/MLDL1/HW3"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/study/Github/MLDL1/HW3\n"]}]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","metadata":{"id":"OyammZP8hI7P"},"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["#Utils"]},{"cell_type":"code","metadata":{"id":"xuQB6W2U5ZE2"},"source":["def sigmoid(z):\n","    \"\"\"\n","    Do NOT modify this function\n","    \"\"\"\n","    return 1/(1+np.exp(-z))\n","\n","def softmax(X):\n","    \"\"\"\n","    Do NOT modify this function\n","    \"\"\"\n","    logit = np.exp(X-np.amax(X, axis=1, keepdims=True))\n","    numer = logit\n","    denom = np.sum(logit, axis=1, keepdims=True)\n","    return numer/denom\n","\n","def relu(X):\n","    return np.where(X>0,X,0)\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["#2-Layer Neural Network"]},{"cell_type":"code","metadata":{"id":"mA5udiGmhRb5","executionInfo":{"status":"ok","timestamp":1652292023083,"user_tz":-540,"elapsed":305,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (a)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n","        \n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","\n","        params = {}\n","        bound = 1/np.sqrt(input_dim)\n","        params[\"W1\"] = np.random.uniform(-bound, bound, size=(input_dim,num_hiddens))\n","        params[\"b1\"] = np.zeros((num_hiddens,))\n","        params[\"W2\"] = np.random.uniform(-bound, bound, size=(num_hiddens,num_classes))\n","        params[\"b2\"] = np.zeros((num_classes,))\n","\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structure is given by\n","\n","          y = softmax(sigmoid(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (b)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","\n","        ff_dict = {}\n","        ff_dict[\"s1\"] = sigmoid(X.dot(self.params[\"W1\"]) + self.params[\"b1\"])\n","        ff_dict[\"s2\"] = softmax(ff_dict[\"s1\"].dot(self.params[\"W2\"]) + self.params[\"b2\"])\n","        y = ff_dict[\"s2\"]\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (c)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        # loss function: cross entropy\n","        grads_softmax = ff_dict[\"s2\"] - np.where(Y == 1,1,0)\n","        grads[\"db2\"] = grads_softmax.sum(axis=0)\n","        grads[\"dW2\"] = ff_dict[\"s1\"].T.dot(grads_softmax)\n","\n","        grads_s1 = grads_softmax.dot(self.params[\"W2\"].T)\n","        grads_sigmoid = grads_s1*(ff_dict[\"s1\"]*(1-ff_dict[\"s1\"]))\n","        grads[\"db1\"] = grads_sigmoid.sum(axis=0)\n","        grads[\"dW1\"] = X.T.dot(grads_sigmoid)\n","\n","        return grads\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","        \n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"        \n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["#Load MNIST"]},{"cell_type":"code","metadata":{"id":"48ooR6YIxYhC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652284957954,"user_tz":-540,"elapsed":4613,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"98fa1fff-0cfc-4018-adaf-402ec69e7af9"},"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 10)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 10)\n","\n","Set validation data aside\n","Training data shape:  (48000, 784)\n","Training labels shape:  (48000, 10)\n","Validation data shape:  (12000, 784)\n","Validation labels shape:  (12000, 10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["#Training & Evaluation"]},{"cell_type":"code","metadata":{"id":"IlnC_rerHPaN"},"source":["### \n","# Question (d)\n","# Tune the hyperparameters with validation data, \n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cWb6xg0NxOs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652290902415,"user_tz":-540,"elapsed":4693232,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"61c17d02-ec3b-4a8a-f9ea-0f4aed9c4010"},"source":["# train the model\n","lr_lst = np.random.uniform(1,3,size=5)\n","n_epochs_lst = [15, 20, 25]\n","batch_size_lst = [128, 256, 512]\n","num_hiddens_lst = [32, 64, 128]\n","combinations = [(lr, n_epochs, batch_size, num_hiddens) \n","                  for lr in lr_lst for n_epochs in n_epochs_lst \n","                    for batch_size in batch_size_lst for num_hiddens in num_hiddens_lst]\n","for i, (lr, n_epochs, batch_size, num_hiddens) in enumerate(combinations):\n","  print('Combination {:02} - lr: {:.3f}, n_epochs: {}, batch_size: {}, num_hiddens: {}'.\\\n","                      format(i, lr, n_epochs, batch_size, num_hiddens))\n","  model = TwoLayerNN(input_dim=784, num_hiddens=num_hiddens, num_classes=10)\n","  model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Combination 00 - lr: 2.189, n_epochs: 15, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.218 0.936, valid loss/acc: 0.230 0.929\n","epoch 01 - train loss/acc: 0.158 0.955, valid loss/acc: 0.180 0.945\n","epoch 02 - train loss/acc: 0.130 0.962, valid loss/acc: 0.158 0.953\n","epoch 03 - train loss/acc: 0.115 0.967, valid loss/acc: 0.150 0.956\n","epoch 04 - train loss/acc: 0.100 0.971, valid loss/acc: 0.141 0.959\n","epoch 05 - train loss/acc: 0.092 0.974, valid loss/acc: 0.138 0.960\n","epoch 06 - train loss/acc: 0.087 0.974, valid loss/acc: 0.137 0.959\n","epoch 07 - train loss/acc: 0.081 0.975, valid loss/acc: 0.137 0.960\n","epoch 08 - train loss/acc: 0.075 0.977, valid loss/acc: 0.134 0.961\n","epoch 09 - train loss/acc: 0.068 0.980, valid loss/acc: 0.130 0.961\n","epoch 10 - train loss/acc: 0.062 0.982, valid loss/acc: 0.127 0.963\n","epoch 11 - train loss/acc: 0.057 0.983, valid loss/acc: 0.128 0.964\n","epoch 12 - train loss/acc: 0.055 0.985, valid loss/acc: 0.124 0.963\n","epoch 13 - train loss/acc: 0.051 0.986, valid loss/acc: 0.127 0.963\n","epoch 14 - train loss/acc: 0.049 0.986, valid loss/acc: 0.132 0.964\n","Combination 01 - lr: 2.189, n_epochs: 15, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.209 0.939, valid loss/acc: 0.222 0.935\n","epoch 01 - train loss/acc: 0.144 0.959, valid loss/acc: 0.163 0.953\n","epoch 02 - train loss/acc: 0.108 0.968, valid loss/acc: 0.133 0.960\n","epoch 03 - train loss/acc: 0.090 0.974, valid loss/acc: 0.120 0.965\n","epoch 04 - train loss/acc: 0.074 0.979, valid loss/acc: 0.107 0.969\n","epoch 05 - train loss/acc: 0.067 0.981, valid loss/acc: 0.105 0.968\n","epoch 06 - train loss/acc: 0.059 0.983, valid loss/acc: 0.102 0.970\n","epoch 07 - train loss/acc: 0.052 0.986, valid loss/acc: 0.098 0.971\n","epoch 08 - train loss/acc: 0.042 0.989, valid loss/acc: 0.090 0.973\n","epoch 09 - train loss/acc: 0.037 0.991, valid loss/acc: 0.087 0.974\n","epoch 10 - train loss/acc: 0.034 0.992, valid loss/acc: 0.090 0.974\n","epoch 11 - train loss/acc: 0.032 0.992, valid loss/acc: 0.091 0.974\n","epoch 12 - train loss/acc: 0.026 0.995, valid loss/acc: 0.085 0.974\n","epoch 13 - train loss/acc: 0.025 0.995, valid loss/acc: 0.088 0.974\n","epoch 14 - train loss/acc: 0.024 0.995, valid loss/acc: 0.089 0.973\n","Combination 02 - lr: 2.189, n_epochs: 15, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.230 0.933, valid loss/acc: 0.242 0.928\n","epoch 01 - train loss/acc: 0.179 0.946, valid loss/acc: 0.194 0.942\n","epoch 02 - train loss/acc: 0.139 0.959, valid loss/acc: 0.168 0.950\n","epoch 03 - train loss/acc: 0.122 0.963, valid loss/acc: 0.156 0.953\n","epoch 04 - train loss/acc: 0.090 0.974, valid loss/acc: 0.128 0.962\n","epoch 05 - train loss/acc: 0.084 0.976, valid loss/acc: 0.127 0.962\n","epoch 06 - train loss/acc: 0.072 0.979, valid loss/acc: 0.119 0.964\n","epoch 07 - train loss/acc: 0.063 0.982, valid loss/acc: 0.112 0.967\n","epoch 08 - train loss/acc: 0.059 0.984, valid loss/acc: 0.110 0.968\n","epoch 09 - train loss/acc: 0.059 0.983, valid loss/acc: 0.116 0.966\n","epoch 10 - train loss/acc: 0.049 0.986, valid loss/acc: 0.111 0.969\n","epoch 11 - train loss/acc: 0.044 0.988, valid loss/acc: 0.107 0.968\n","epoch 12 - train loss/acc: 0.040 0.990, valid loss/acc: 0.108 0.968\n","epoch 13 - train loss/acc: 0.040 0.989, valid loss/acc: 0.110 0.967\n","epoch 14 - train loss/acc: 0.035 0.991, valid loss/acc: 0.107 0.970\n","Combination 03 - lr: 2.189, n_epochs: 15, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.289 0.913, valid loss/acc: 0.293 0.913\n","epoch 01 - train loss/acc: 0.207 0.941, valid loss/acc: 0.220 0.936\n","epoch 02 - train loss/acc: 0.173 0.950, valid loss/acc: 0.193 0.945\n","epoch 03 - train loss/acc: 0.150 0.958, valid loss/acc: 0.173 0.950\n","epoch 04 - train loss/acc: 0.134 0.962, valid loss/acc: 0.158 0.955\n","epoch 05 - train loss/acc: 0.128 0.962, valid loss/acc: 0.159 0.952\n","epoch 06 - train loss/acc: 0.110 0.969, valid loss/acc: 0.142 0.957\n","epoch 07 - train loss/acc: 0.102 0.971, valid loss/acc: 0.138 0.958\n","epoch 08 - train loss/acc: 0.097 0.973, valid loss/acc: 0.136 0.959\n","epoch 09 - train loss/acc: 0.087 0.976, valid loss/acc: 0.128 0.961\n","epoch 10 - train loss/acc: 0.086 0.975, valid loss/acc: 0.132 0.960\n","epoch 11 - train loss/acc: 0.081 0.977, valid loss/acc: 0.129 0.961\n","epoch 12 - train loss/acc: 0.076 0.979, valid loss/acc: 0.125 0.964\n","epoch 13 - train loss/acc: 0.073 0.980, valid loss/acc: 0.124 0.963\n","epoch 14 - train loss/acc: 0.070 0.981, valid loss/acc: 0.124 0.962\n","Combination 04 - lr: 2.189, n_epochs: 15, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.269 0.922, valid loss/acc: 0.278 0.918\n","epoch 01 - train loss/acc: 0.205 0.941, valid loss/acc: 0.218 0.936\n","epoch 02 - train loss/acc: 0.175 0.948, valid loss/acc: 0.193 0.942\n","epoch 03 - train loss/acc: 0.138 0.960, valid loss/acc: 0.159 0.953\n","epoch 04 - train loss/acc: 0.123 0.965, valid loss/acc: 0.150 0.956\n","epoch 05 - train loss/acc: 0.107 0.969, valid loss/acc: 0.135 0.959\n","epoch 06 - train loss/acc: 0.093 0.974, valid loss/acc: 0.123 0.963\n","epoch 07 - train loss/acc: 0.087 0.975, valid loss/acc: 0.118 0.964\n","epoch 08 - train loss/acc: 0.081 0.977, valid loss/acc: 0.117 0.964\n","epoch 09 - train loss/acc: 0.071 0.981, valid loss/acc: 0.109 0.966\n","epoch 10 - train loss/acc: 0.065 0.982, valid loss/acc: 0.104 0.969\n","epoch 11 - train loss/acc: 0.059 0.984, valid loss/acc: 0.100 0.969\n","epoch 12 - train loss/acc: 0.054 0.985, valid loss/acc: 0.098 0.970\n","epoch 13 - train loss/acc: 0.053 0.986, valid loss/acc: 0.100 0.970\n","epoch 14 - train loss/acc: 0.049 0.987, valid loss/acc: 0.098 0.970\n","Combination 05 - lr: 2.189, n_epochs: 15, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.281 0.918, valid loss/acc: 0.287 0.914\n","epoch 01 - train loss/acc: 0.207 0.940, valid loss/acc: 0.216 0.937\n","epoch 02 - train loss/acc: 0.162 0.952, valid loss/acc: 0.175 0.948\n","epoch 03 - train loss/acc: 0.139 0.959, valid loss/acc: 0.155 0.953\n","epoch 04 - train loss/acc: 0.115 0.967, valid loss/acc: 0.138 0.960\n","epoch 05 - train loss/acc: 0.100 0.971, valid loss/acc: 0.125 0.964\n","epoch 06 - train loss/acc: 0.087 0.976, valid loss/acc: 0.116 0.965\n","epoch 07 - train loss/acc: 0.082 0.977, valid loss/acc: 0.115 0.966\n","epoch 08 - train loss/acc: 0.071 0.980, valid loss/acc: 0.106 0.969\n","epoch 09 - train loss/acc: 0.066 0.982, valid loss/acc: 0.105 0.969\n","epoch 10 - train loss/acc: 0.059 0.984, valid loss/acc: 0.101 0.971\n","epoch 11 - train loss/acc: 0.052 0.987, valid loss/acc: 0.095 0.972\n","epoch 12 - train loss/acc: 0.049 0.987, valid loss/acc: 0.093 0.971\n","epoch 13 - train loss/acc: 0.047 0.987, valid loss/acc: 0.094 0.973\n","epoch 14 - train loss/acc: 0.043 0.989, valid loss/acc: 0.091 0.973\n","Combination 06 - lr: 2.189, n_epochs: 15, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.368 0.891, valid loss/acc: 0.376 0.888\n","epoch 01 - train loss/acc: 0.267 0.923, valid loss/acc: 0.275 0.920\n","epoch 02 - train loss/acc: 0.234 0.932, valid loss/acc: 0.244 0.929\n","epoch 03 - train loss/acc: 0.203 0.942, valid loss/acc: 0.216 0.937\n","epoch 04 - train loss/acc: 0.191 0.945, valid loss/acc: 0.207 0.939\n","epoch 05 - train loss/acc: 0.174 0.950, valid loss/acc: 0.192 0.944\n","epoch 06 - train loss/acc: 0.164 0.953, valid loss/acc: 0.184 0.946\n","epoch 07 - train loss/acc: 0.151 0.957, valid loss/acc: 0.176 0.949\n","epoch 08 - train loss/acc: 0.143 0.960, valid loss/acc: 0.168 0.951\n","epoch 09 - train loss/acc: 0.136 0.962, valid loss/acc: 0.164 0.952\n","epoch 10 - train loss/acc: 0.134 0.962, valid loss/acc: 0.163 0.951\n","epoch 11 - train loss/acc: 0.124 0.966, valid loss/acc: 0.155 0.953\n","epoch 12 - train loss/acc: 0.122 0.966, valid loss/acc: 0.156 0.954\n","epoch 13 - train loss/acc: 0.117 0.968, valid loss/acc: 0.151 0.954\n","epoch 14 - train loss/acc: 0.115 0.968, valid loss/acc: 0.152 0.955\n","Combination 07 - lr: 2.189, n_epochs: 15, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.334 0.904, valid loss/acc: 0.339 0.902\n","epoch 01 - train loss/acc: 0.268 0.923, valid loss/acc: 0.277 0.919\n","epoch 02 - train loss/acc: 0.220 0.937, valid loss/acc: 0.231 0.932\n","epoch 03 - train loss/acc: 0.195 0.944, valid loss/acc: 0.208 0.939\n","epoch 04 - train loss/acc: 0.174 0.949, valid loss/acc: 0.191 0.944\n","epoch 05 - train loss/acc: 0.155 0.955, valid loss/acc: 0.173 0.950\n","epoch 06 - train loss/acc: 0.146 0.958, valid loss/acc: 0.166 0.952\n","epoch 07 - train loss/acc: 0.133 0.962, valid loss/acc: 0.154 0.954\n","epoch 08 - train loss/acc: 0.122 0.965, valid loss/acc: 0.145 0.957\n","epoch 09 - train loss/acc: 0.114 0.968, valid loss/acc: 0.138 0.960\n","epoch 10 - train loss/acc: 0.110 0.969, valid loss/acc: 0.138 0.958\n","epoch 11 - train loss/acc: 0.105 0.970, valid loss/acc: 0.133 0.962\n","epoch 12 - train loss/acc: 0.094 0.973, valid loss/acc: 0.124 0.962\n","epoch 13 - train loss/acc: 0.087 0.976, valid loss/acc: 0.119 0.964\n","epoch 14 - train loss/acc: 0.082 0.977, valid loss/acc: 0.115 0.966\n","Combination 08 - lr: 2.189, n_epochs: 15, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.415 0.891, valid loss/acc: 0.418 0.892\n","epoch 01 - train loss/acc: 0.291 0.916, valid loss/acc: 0.297 0.914\n","epoch 02 - train loss/acc: 0.244 0.929, valid loss/acc: 0.253 0.925\n","epoch 03 - train loss/acc: 0.214 0.938, valid loss/acc: 0.228 0.933\n","epoch 04 - train loss/acc: 0.195 0.943, valid loss/acc: 0.211 0.938\n","epoch 05 - train loss/acc: 0.179 0.947, valid loss/acc: 0.198 0.941\n","epoch 06 - train loss/acc: 0.163 0.953, valid loss/acc: 0.182 0.945\n","epoch 07 - train loss/acc: 0.160 0.953, valid loss/acc: 0.184 0.945\n","epoch 08 - train loss/acc: 0.145 0.958, valid loss/acc: 0.169 0.950\n","epoch 09 - train loss/acc: 0.132 0.963, valid loss/acc: 0.159 0.951\n","epoch 10 - train loss/acc: 0.126 0.965, valid loss/acc: 0.154 0.954\n","epoch 11 - train loss/acc: 0.118 0.967, valid loss/acc: 0.148 0.955\n","epoch 12 - train loss/acc: 0.112 0.968, valid loss/acc: 0.145 0.956\n","epoch 13 - train loss/acc: 0.109 0.970, valid loss/acc: 0.144 0.957\n","epoch 14 - train loss/acc: 0.101 0.972, valid loss/acc: 0.138 0.958\n","Combination 09 - lr: 2.189, n_epochs: 20, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.222 0.936, valid loss/acc: 0.236 0.930\n","epoch 01 - train loss/acc: 0.166 0.951, valid loss/acc: 0.185 0.946\n","epoch 02 - train loss/acc: 0.138 0.960, valid loss/acc: 0.163 0.951\n","epoch 03 - train loss/acc: 0.115 0.966, valid loss/acc: 0.149 0.955\n","epoch 04 - train loss/acc: 0.098 0.972, valid loss/acc: 0.136 0.960\n","epoch 05 - train loss/acc: 0.091 0.974, valid loss/acc: 0.136 0.959\n","epoch 06 - train loss/acc: 0.076 0.979, valid loss/acc: 0.124 0.963\n","epoch 07 - train loss/acc: 0.074 0.979, valid loss/acc: 0.125 0.964\n","epoch 08 - train loss/acc: 0.067 0.981, valid loss/acc: 0.125 0.962\n","epoch 09 - train loss/acc: 0.061 0.983, valid loss/acc: 0.126 0.963\n","epoch 10 - train loss/acc: 0.060 0.983, valid loss/acc: 0.124 0.962\n","epoch 11 - train loss/acc: 0.054 0.985, valid loss/acc: 0.122 0.963\n","epoch 12 - train loss/acc: 0.056 0.983, valid loss/acc: 0.129 0.960\n","epoch 13 - train loss/acc: 0.047 0.987, valid loss/acc: 0.123 0.963\n","epoch 14 - train loss/acc: 0.047 0.987, valid loss/acc: 0.125 0.965\n","epoch 15 - train loss/acc: 0.041 0.989, valid loss/acc: 0.121 0.963\n","epoch 16 - train loss/acc: 0.040 0.989, valid loss/acc: 0.124 0.964\n","epoch 17 - train loss/acc: 0.039 0.990, valid loss/acc: 0.124 0.964\n","epoch 18 - train loss/acc: 0.036 0.991, valid loss/acc: 0.128 0.963\n","epoch 19 - train loss/acc: 0.036 0.990, valid loss/acc: 0.126 0.964\n","Combination 10 - lr: 2.189, n_epochs: 20, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.211 0.939, valid loss/acc: 0.222 0.933\n","epoch 01 - train loss/acc: 0.146 0.957, valid loss/acc: 0.167 0.950\n","epoch 02 - train loss/acc: 0.115 0.967, valid loss/acc: 0.143 0.957\n","epoch 03 - train loss/acc: 0.099 0.971, valid loss/acc: 0.133 0.961\n","epoch 04 - train loss/acc: 0.085 0.975, valid loss/acc: 0.124 0.963\n","epoch 05 - train loss/acc: 0.072 0.979, valid loss/acc: 0.112 0.965\n","epoch 06 - train loss/acc: 0.065 0.981, valid loss/acc: 0.112 0.966\n","epoch 07 - train loss/acc: 0.058 0.984, valid loss/acc: 0.104 0.968\n","epoch 08 - train loss/acc: 0.048 0.987, valid loss/acc: 0.098 0.970\n","epoch 09 - train loss/acc: 0.051 0.986, valid loss/acc: 0.105 0.967\n","epoch 10 - train loss/acc: 0.040 0.989, valid loss/acc: 0.097 0.970\n","epoch 11 - train loss/acc: 0.041 0.989, valid loss/acc: 0.104 0.968\n","epoch 12 - train loss/acc: 0.043 0.988, valid loss/acc: 0.102 0.968\n","epoch 13 - train loss/acc: 0.029 0.993, valid loss/acc: 0.093 0.972\n","epoch 14 - train loss/acc: 0.028 0.993, valid loss/acc: 0.098 0.970\n","epoch 15 - train loss/acc: 0.027 0.994, valid loss/acc: 0.098 0.970\n","epoch 16 - train loss/acc: 0.023 0.995, valid loss/acc: 0.094 0.971\n","epoch 17 - train loss/acc: 0.025 0.994, valid loss/acc: 0.105 0.969\n","epoch 18 - train loss/acc: 0.019 0.997, valid loss/acc: 0.096 0.970\n","epoch 19 - train loss/acc: 0.017 0.997, valid loss/acc: 0.093 0.972\n","Combination 11 - lr: 2.189, n_epochs: 20, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.226 0.935, valid loss/acc: 0.234 0.932\n","epoch 01 - train loss/acc: 0.156 0.955, valid loss/acc: 0.173 0.949\n","epoch 02 - train loss/acc: 0.116 0.966, valid loss/acc: 0.138 0.958\n","epoch 03 - train loss/acc: 0.102 0.970, valid loss/acc: 0.131 0.959\n","epoch 04 - train loss/acc: 0.080 0.977, valid loss/acc: 0.117 0.964\n","epoch 05 - train loss/acc: 0.071 0.979, valid loss/acc: 0.110 0.967\n","epoch 06 - train loss/acc: 0.058 0.984, valid loss/acc: 0.102 0.968\n","epoch 07 - train loss/acc: 0.055 0.985, valid loss/acc: 0.104 0.969\n","epoch 08 - train loss/acc: 0.046 0.988, valid loss/acc: 0.098 0.970\n","epoch 09 - train loss/acc: 0.042 0.989, valid loss/acc: 0.098 0.970\n","epoch 10 - train loss/acc: 0.036 0.992, valid loss/acc: 0.093 0.973\n","epoch 11 - train loss/acc: 0.032 0.992, valid loss/acc: 0.094 0.971\n","epoch 12 - train loss/acc: 0.028 0.994, valid loss/acc: 0.091 0.973\n","epoch 13 - train loss/acc: 0.026 0.994, valid loss/acc: 0.092 0.973\n","epoch 14 - train loss/acc: 0.022 0.996, valid loss/acc: 0.092 0.973\n","epoch 15 - train loss/acc: 0.021 0.996, valid loss/acc: 0.091 0.974\n","epoch 16 - train loss/acc: 0.020 0.997, valid loss/acc: 0.093 0.972\n","epoch 17 - train loss/acc: 0.019 0.997, valid loss/acc: 0.097 0.973\n","epoch 18 - train loss/acc: 0.015 0.998, valid loss/acc: 0.093 0.974\n","epoch 19 - train loss/acc: 0.014 0.998, valid loss/acc: 0.094 0.974\n","Combination 12 - lr: 2.189, n_epochs: 20, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.272 0.923, valid loss/acc: 0.281 0.920\n","epoch 01 - train loss/acc: 0.210 0.941, valid loss/acc: 0.222 0.936\n","epoch 02 - train loss/acc: 0.175 0.950, valid loss/acc: 0.195 0.944\n","epoch 03 - train loss/acc: 0.157 0.956, valid loss/acc: 0.181 0.948\n","epoch 04 - train loss/acc: 0.142 0.960, valid loss/acc: 0.168 0.951\n","epoch 05 - train loss/acc: 0.125 0.965, valid loss/acc: 0.153 0.954\n","epoch 06 - train loss/acc: 0.116 0.967, valid loss/acc: 0.147 0.957\n","epoch 07 - train loss/acc: 0.106 0.970, valid loss/acc: 0.142 0.958\n","epoch 08 - train loss/acc: 0.103 0.971, valid loss/acc: 0.141 0.958\n","epoch 09 - train loss/acc: 0.100 0.972, valid loss/acc: 0.139 0.957\n","epoch 10 - train loss/acc: 0.089 0.976, valid loss/acc: 0.131 0.961\n","epoch 11 - train loss/acc: 0.082 0.978, valid loss/acc: 0.130 0.962\n","epoch 12 - train loss/acc: 0.079 0.978, valid loss/acc: 0.127 0.962\n","epoch 13 - train loss/acc: 0.077 0.979, valid loss/acc: 0.129 0.961\n","epoch 14 - train loss/acc: 0.072 0.981, valid loss/acc: 0.123 0.963\n","epoch 15 - train loss/acc: 0.078 0.977, valid loss/acc: 0.130 0.960\n","epoch 16 - train loss/acc: 0.066 0.982, valid loss/acc: 0.123 0.963\n","epoch 17 - train loss/acc: 0.064 0.982, valid loss/acc: 0.122 0.964\n","epoch 18 - train loss/acc: 0.060 0.984, valid loss/acc: 0.122 0.963\n","epoch 19 - train loss/acc: 0.059 0.985, valid loss/acc: 0.124 0.963\n","Combination 13 - lr: 2.189, n_epochs: 20, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.282 0.914, valid loss/acc: 0.287 0.912\n","epoch 01 - train loss/acc: 0.197 0.941, valid loss/acc: 0.207 0.938\n","epoch 02 - train loss/acc: 0.161 0.954, valid loss/acc: 0.178 0.947\n","epoch 03 - train loss/acc: 0.134 0.961, valid loss/acc: 0.152 0.956\n","epoch 04 - train loss/acc: 0.115 0.967, valid loss/acc: 0.139 0.961\n","epoch 05 - train loss/acc: 0.105 0.970, valid loss/acc: 0.131 0.961\n","epoch 06 - train loss/acc: 0.094 0.973, valid loss/acc: 0.124 0.964\n","epoch 07 - train loss/acc: 0.082 0.977, valid loss/acc: 0.114 0.967\n","epoch 08 - train loss/acc: 0.078 0.978, valid loss/acc: 0.111 0.966\n","epoch 09 - train loss/acc: 0.067 0.982, valid loss/acc: 0.105 0.969\n","epoch 10 - train loss/acc: 0.063 0.983, valid loss/acc: 0.105 0.968\n","epoch 11 - train loss/acc: 0.057 0.985, valid loss/acc: 0.101 0.969\n","epoch 12 - train loss/acc: 0.053 0.986, valid loss/acc: 0.098 0.970\n","epoch 13 - train loss/acc: 0.049 0.988, valid loss/acc: 0.096 0.971\n","epoch 14 - train loss/acc: 0.046 0.988, valid loss/acc: 0.096 0.970\n","epoch 15 - train loss/acc: 0.046 0.989, valid loss/acc: 0.097 0.971\n","epoch 16 - train loss/acc: 0.040 0.991, valid loss/acc: 0.092 0.971\n","epoch 17 - train loss/acc: 0.039 0.991, valid loss/acc: 0.093 0.973\n","epoch 18 - train loss/acc: 0.037 0.991, valid loss/acc: 0.093 0.971\n","epoch 19 - train loss/acc: 0.034 0.993, valid loss/acc: 0.091 0.972\n","Combination 14 - lr: 2.189, n_epochs: 20, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.293 0.913, valid loss/acc: 0.297 0.912\n","epoch 01 - train loss/acc: 0.210 0.938, valid loss/acc: 0.218 0.937\n","epoch 02 - train loss/acc: 0.160 0.954, valid loss/acc: 0.173 0.948\n","epoch 03 - train loss/acc: 0.137 0.960, valid loss/acc: 0.153 0.955\n","epoch 04 - train loss/acc: 0.120 0.965, valid loss/acc: 0.142 0.958\n","epoch 05 - train loss/acc: 0.099 0.972, valid loss/acc: 0.124 0.963\n","epoch 06 - train loss/acc: 0.091 0.974, valid loss/acc: 0.121 0.963\n","epoch 07 - train loss/acc: 0.079 0.977, valid loss/acc: 0.110 0.967\n","epoch 08 - train loss/acc: 0.076 0.979, valid loss/acc: 0.110 0.967\n","epoch 09 - train loss/acc: 0.064 0.982, valid loss/acc: 0.101 0.968\n","epoch 10 - train loss/acc: 0.059 0.984, valid loss/acc: 0.095 0.971\n","epoch 11 - train loss/acc: 0.052 0.986, valid loss/acc: 0.092 0.971\n","epoch 12 - train loss/acc: 0.049 0.987, valid loss/acc: 0.090 0.972\n","epoch 13 - train loss/acc: 0.045 0.988, valid loss/acc: 0.090 0.973\n","epoch 14 - train loss/acc: 0.041 0.990, valid loss/acc: 0.086 0.973\n","epoch 15 - train loss/acc: 0.037 0.991, valid loss/acc: 0.084 0.974\n","epoch 16 - train loss/acc: 0.036 0.992, valid loss/acc: 0.084 0.974\n","epoch 17 - train loss/acc: 0.033 0.993, valid loss/acc: 0.083 0.974\n","epoch 18 - train loss/acc: 0.032 0.993, valid loss/acc: 0.083 0.974\n","epoch 19 - train loss/acc: 0.028 0.995, valid loss/acc: 0.081 0.974\n","Combination 15 - lr: 2.189, n_epochs: 20, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.346 0.904, valid loss/acc: 0.351 0.900\n","epoch 01 - train loss/acc: 0.270 0.923, valid loss/acc: 0.279 0.919\n","epoch 02 - train loss/acc: 0.232 0.933, valid loss/acc: 0.241 0.929\n","epoch 03 - train loss/acc: 0.201 0.943, valid loss/acc: 0.213 0.938\n","epoch 04 - train loss/acc: 0.189 0.946, valid loss/acc: 0.201 0.940\n","epoch 05 - train loss/acc: 0.170 0.952, valid loss/acc: 0.187 0.946\n","epoch 06 - train loss/acc: 0.156 0.955, valid loss/acc: 0.174 0.948\n","epoch 07 - train loss/acc: 0.145 0.959, valid loss/acc: 0.166 0.952\n","epoch 08 - train loss/acc: 0.138 0.961, valid loss/acc: 0.161 0.952\n","epoch 09 - train loss/acc: 0.126 0.965, valid loss/acc: 0.151 0.955\n","epoch 10 - train loss/acc: 0.124 0.966, valid loss/acc: 0.150 0.956\n","epoch 11 - train loss/acc: 0.116 0.968, valid loss/acc: 0.143 0.958\n","epoch 12 - train loss/acc: 0.113 0.969, valid loss/acc: 0.143 0.958\n","epoch 13 - train loss/acc: 0.107 0.970, valid loss/acc: 0.141 0.958\n","epoch 14 - train loss/acc: 0.104 0.971, valid loss/acc: 0.139 0.959\n","epoch 15 - train loss/acc: 0.103 0.970, valid loss/acc: 0.139 0.960\n","epoch 16 - train loss/acc: 0.095 0.974, valid loss/acc: 0.134 0.961\n","epoch 17 - train loss/acc: 0.091 0.975, valid loss/acc: 0.130 0.961\n","epoch 18 - train loss/acc: 0.088 0.976, valid loss/acc: 0.130 0.961\n","epoch 19 - train loss/acc: 0.086 0.977, valid loss/acc: 0.128 0.962\n","Combination 16 - lr: 2.189, n_epochs: 20, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.356 0.894, valid loss/acc: 0.360 0.891\n","epoch 01 - train loss/acc: 0.266 0.924, valid loss/acc: 0.272 0.920\n","epoch 02 - train loss/acc: 0.229 0.933, valid loss/acc: 0.238 0.931\n","epoch 03 - train loss/acc: 0.193 0.944, valid loss/acc: 0.205 0.938\n","epoch 04 - train loss/acc: 0.182 0.945, valid loss/acc: 0.195 0.941\n","epoch 05 - train loss/acc: 0.158 0.955, valid loss/acc: 0.174 0.948\n","epoch 06 - train loss/acc: 0.143 0.959, valid loss/acc: 0.161 0.952\n","epoch 07 - train loss/acc: 0.132 0.962, valid loss/acc: 0.153 0.954\n","epoch 08 - train loss/acc: 0.122 0.965, valid loss/acc: 0.145 0.957\n","epoch 09 - train loss/acc: 0.115 0.967, valid loss/acc: 0.140 0.957\n","epoch 10 - train loss/acc: 0.105 0.970, valid loss/acc: 0.132 0.960\n","epoch 11 - train loss/acc: 0.100 0.972, valid loss/acc: 0.128 0.961\n","epoch 12 - train loss/acc: 0.094 0.973, valid loss/acc: 0.125 0.961\n","epoch 13 - train loss/acc: 0.089 0.976, valid loss/acc: 0.120 0.964\n","epoch 14 - train loss/acc: 0.084 0.977, valid loss/acc: 0.117 0.965\n","epoch 15 - train loss/acc: 0.079 0.978, valid loss/acc: 0.114 0.965\n","epoch 16 - train loss/acc: 0.077 0.979, valid loss/acc: 0.113 0.965\n","epoch 17 - train loss/acc: 0.072 0.981, valid loss/acc: 0.109 0.966\n","epoch 18 - train loss/acc: 0.070 0.981, valid loss/acc: 0.109 0.967\n","epoch 19 - train loss/acc: 0.066 0.983, valid loss/acc: 0.108 0.966\n","Combination 17 - lr: 2.189, n_epochs: 20, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.387 0.886, valid loss/acc: 0.392 0.883\n","epoch 01 - train loss/acc: 0.275 0.919, valid loss/acc: 0.282 0.915\n","epoch 02 - train loss/acc: 0.228 0.934, valid loss/acc: 0.238 0.931\n","epoch 03 - train loss/acc: 0.203 0.940, valid loss/acc: 0.214 0.937\n","epoch 04 - train loss/acc: 0.179 0.947, valid loss/acc: 0.195 0.942\n","epoch 05 - train loss/acc: 0.163 0.952, valid loss/acc: 0.181 0.947\n","epoch 06 - train loss/acc: 0.152 0.956, valid loss/acc: 0.171 0.950\n","epoch 07 - train loss/acc: 0.138 0.960, valid loss/acc: 0.158 0.953\n","epoch 08 - train loss/acc: 0.128 0.963, valid loss/acc: 0.152 0.956\n","epoch 09 - train loss/acc: 0.114 0.968, valid loss/acc: 0.139 0.959\n","epoch 10 - train loss/acc: 0.106 0.970, valid loss/acc: 0.133 0.961\n","epoch 11 - train loss/acc: 0.098 0.973, valid loss/acc: 0.126 0.963\n","epoch 12 - train loss/acc: 0.093 0.974, valid loss/acc: 0.123 0.963\n","epoch 13 - train loss/acc: 0.088 0.976, valid loss/acc: 0.120 0.965\n","epoch 14 - train loss/acc: 0.082 0.977, valid loss/acc: 0.114 0.965\n","epoch 15 - train loss/acc: 0.078 0.979, valid loss/acc: 0.112 0.966\n","epoch 16 - train loss/acc: 0.074 0.980, valid loss/acc: 0.110 0.968\n","epoch 17 - train loss/acc: 0.072 0.981, valid loss/acc: 0.110 0.967\n","epoch 18 - train loss/acc: 0.067 0.982, valid loss/acc: 0.104 0.969\n","epoch 19 - train loss/acc: 0.063 0.983, valid loss/acc: 0.101 0.970\n","Combination 18 - lr: 2.189, n_epochs: 25, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.209 0.939, valid loss/acc: 0.218 0.935\n","epoch 01 - train loss/acc: 0.160 0.954, valid loss/acc: 0.175 0.947\n","epoch 02 - train loss/acc: 0.126 0.965, valid loss/acc: 0.148 0.954\n","epoch 03 - train loss/acc: 0.111 0.968, valid loss/acc: 0.139 0.957\n","epoch 04 - train loss/acc: 0.100 0.971, valid loss/acc: 0.132 0.960\n","epoch 05 - train loss/acc: 0.096 0.973, valid loss/acc: 0.133 0.960\n","epoch 06 - train loss/acc: 0.088 0.974, valid loss/acc: 0.128 0.961\n","epoch 07 - train loss/acc: 0.078 0.977, valid loss/acc: 0.128 0.960\n","epoch 08 - train loss/acc: 0.074 0.978, valid loss/acc: 0.127 0.961\n","epoch 09 - train loss/acc: 0.066 0.981, valid loss/acc: 0.123 0.962\n","epoch 10 - train loss/acc: 0.069 0.980, valid loss/acc: 0.125 0.961\n","epoch 11 - train loss/acc: 0.060 0.983, valid loss/acc: 0.121 0.964\n","epoch 12 - train loss/acc: 0.057 0.984, valid loss/acc: 0.125 0.962\n","epoch 13 - train loss/acc: 0.051 0.986, valid loss/acc: 0.120 0.963\n","epoch 14 - train loss/acc: 0.051 0.985, valid loss/acc: 0.125 0.963\n","epoch 15 - train loss/acc: 0.044 0.988, valid loss/acc: 0.119 0.965\n","epoch 16 - train loss/acc: 0.040 0.990, valid loss/acc: 0.117 0.965\n","epoch 17 - train loss/acc: 0.039 0.990, valid loss/acc: 0.119 0.965\n","epoch 18 - train loss/acc: 0.040 0.989, valid loss/acc: 0.122 0.965\n","epoch 19 - train loss/acc: 0.035 0.992, valid loss/acc: 0.119 0.965\n","epoch 20 - train loss/acc: 0.033 0.993, valid loss/acc: 0.124 0.965\n","epoch 21 - train loss/acc: 0.038 0.989, valid loss/acc: 0.127 0.963\n","epoch 22 - train loss/acc: 0.035 0.990, valid loss/acc: 0.125 0.965\n","epoch 23 - train loss/acc: 0.031 0.993, valid loss/acc: 0.131 0.963\n","epoch 24 - train loss/acc: 0.033 0.992, valid loss/acc: 0.131 0.964\n","Combination 19 - lr: 2.189, n_epochs: 25, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.214 0.936, valid loss/acc: 0.225 0.933\n","epoch 01 - train loss/acc: 0.142 0.958, valid loss/acc: 0.160 0.953\n","epoch 02 - train loss/acc: 0.107 0.970, valid loss/acc: 0.130 0.960\n","epoch 03 - train loss/acc: 0.092 0.973, valid loss/acc: 0.119 0.964\n","epoch 04 - train loss/acc: 0.077 0.978, valid loss/acc: 0.111 0.966\n","epoch 05 - train loss/acc: 0.064 0.982, valid loss/acc: 0.101 0.969\n","epoch 06 - train loss/acc: 0.058 0.985, valid loss/acc: 0.098 0.969\n","epoch 07 - train loss/acc: 0.049 0.987, valid loss/acc: 0.094 0.971\n","epoch 08 - train loss/acc: 0.043 0.989, valid loss/acc: 0.089 0.974\n","epoch 09 - train loss/acc: 0.040 0.989, valid loss/acc: 0.093 0.971\n","epoch 10 - train loss/acc: 0.036 0.992, valid loss/acc: 0.090 0.973\n","epoch 11 - train loss/acc: 0.032 0.993, valid loss/acc: 0.090 0.973\n","epoch 12 - train loss/acc: 0.029 0.993, valid loss/acc: 0.088 0.974\n","epoch 13 - train loss/acc: 0.026 0.995, valid loss/acc: 0.088 0.973\n","epoch 14 - train loss/acc: 0.024 0.995, valid loss/acc: 0.087 0.974\n","epoch 15 - train loss/acc: 0.021 0.996, valid loss/acc: 0.088 0.974\n","epoch 16 - train loss/acc: 0.019 0.997, valid loss/acc: 0.086 0.974\n","epoch 17 - train loss/acc: 0.018 0.997, valid loss/acc: 0.091 0.973\n","epoch 18 - train loss/acc: 0.016 0.998, valid loss/acc: 0.090 0.974\n","epoch 19 - train loss/acc: 0.015 0.998, valid loss/acc: 0.090 0.974\n","epoch 20 - train loss/acc: 0.013 0.998, valid loss/acc: 0.090 0.975\n","epoch 21 - train loss/acc: 0.013 0.999, valid loss/acc: 0.092 0.973\n","epoch 22 - train loss/acc: 0.011 0.999, valid loss/acc: 0.090 0.975\n","epoch 23 - train loss/acc: 0.010 0.999, valid loss/acc: 0.091 0.975\n","epoch 24 - train loss/acc: 0.010 0.999, valid loss/acc: 0.092 0.974\n","Combination 20 - lr: 2.189, n_epochs: 25, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.225 0.933, valid loss/acc: 0.235 0.929\n","epoch 01 - train loss/acc: 0.147 0.956, valid loss/acc: 0.168 0.952\n","epoch 02 - train loss/acc: 0.120 0.965, valid loss/acc: 0.148 0.956\n","epoch 03 - train loss/acc: 0.102 0.970, valid loss/acc: 0.133 0.960\n","epoch 04 - train loss/acc: 0.087 0.973, valid loss/acc: 0.125 0.963\n","epoch 05 - train loss/acc: 0.069 0.980, valid loss/acc: 0.113 0.966\n","epoch 06 - train loss/acc: 0.066 0.981, valid loss/acc: 0.112 0.965\n","epoch 07 - train loss/acc: 0.055 0.985, valid loss/acc: 0.104 0.969\n","epoch 08 - train loss/acc: 0.052 0.984, valid loss/acc: 0.107 0.969\n","epoch 09 - train loss/acc: 0.048 0.985, valid loss/acc: 0.103 0.970\n","epoch 10 - train loss/acc: 0.039 0.990, valid loss/acc: 0.101 0.971\n","epoch 11 - train loss/acc: 0.036 0.990, valid loss/acc: 0.097 0.971\n","epoch 12 - train loss/acc: 0.032 0.992, valid loss/acc: 0.096 0.971\n","epoch 13 - train loss/acc: 0.031 0.992, valid loss/acc: 0.098 0.970\n","epoch 14 - train loss/acc: 0.025 0.994, valid loss/acc: 0.097 0.971\n","epoch 15 - train loss/acc: 0.022 0.996, valid loss/acc: 0.092 0.974\n","epoch 16 - train loss/acc: 0.021 0.996, valid loss/acc: 0.097 0.971\n","epoch 17 - train loss/acc: 0.020 0.996, valid loss/acc: 0.097 0.971\n","epoch 18 - train loss/acc: 0.017 0.997, valid loss/acc: 0.098 0.972\n","epoch 19 - train loss/acc: 0.017 0.997, valid loss/acc: 0.096 0.973\n","epoch 20 - train loss/acc: 0.015 0.998, valid loss/acc: 0.096 0.972\n","epoch 21 - train loss/acc: 0.013 0.999, valid loss/acc: 0.098 0.973\n","epoch 22 - train loss/acc: 0.012 0.999, valid loss/acc: 0.099 0.972\n","epoch 23 - train loss/acc: 0.011 0.999, valid loss/acc: 0.097 0.972\n","epoch 24 - train loss/acc: 0.010 0.999, valid loss/acc: 0.098 0.972\n","Combination 21 - lr: 2.189, n_epochs: 25, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.270 0.922, valid loss/acc: 0.277 0.919\n","epoch 01 - train loss/acc: 0.209 0.940, valid loss/acc: 0.217 0.937\n","epoch 02 - train loss/acc: 0.176 0.949, valid loss/acc: 0.189 0.944\n","epoch 03 - train loss/acc: 0.153 0.956, valid loss/acc: 0.170 0.951\n","epoch 04 - train loss/acc: 0.140 0.960, valid loss/acc: 0.161 0.951\n","epoch 05 - train loss/acc: 0.130 0.963, valid loss/acc: 0.159 0.952\n","epoch 06 - train loss/acc: 0.115 0.967, valid loss/acc: 0.143 0.957\n","epoch 07 - train loss/acc: 0.106 0.970, valid loss/acc: 0.139 0.960\n","epoch 08 - train loss/acc: 0.098 0.973, valid loss/acc: 0.133 0.960\n","epoch 09 - train loss/acc: 0.096 0.973, valid loss/acc: 0.132 0.962\n","epoch 10 - train loss/acc: 0.089 0.975, valid loss/acc: 0.131 0.962\n","epoch 11 - train loss/acc: 0.083 0.977, valid loss/acc: 0.127 0.962\n","epoch 12 - train loss/acc: 0.081 0.977, valid loss/acc: 0.125 0.963\n","epoch 13 - train loss/acc: 0.074 0.980, valid loss/acc: 0.124 0.961\n","epoch 14 - train loss/acc: 0.086 0.974, valid loss/acc: 0.139 0.956\n","epoch 15 - train loss/acc: 0.072 0.980, valid loss/acc: 0.126 0.962\n","epoch 16 - train loss/acc: 0.066 0.982, valid loss/acc: 0.120 0.964\n","epoch 17 - train loss/acc: 0.062 0.984, valid loss/acc: 0.120 0.963\n","epoch 18 - train loss/acc: 0.063 0.983, valid loss/acc: 0.123 0.963\n","epoch 19 - train loss/acc: 0.057 0.985, valid loss/acc: 0.118 0.964\n","epoch 20 - train loss/acc: 0.055 0.986, valid loss/acc: 0.117 0.964\n","epoch 21 - train loss/acc: 0.055 0.985, valid loss/acc: 0.120 0.964\n","epoch 22 - train loss/acc: 0.053 0.986, valid loss/acc: 0.120 0.965\n","epoch 23 - train loss/acc: 0.052 0.986, valid loss/acc: 0.121 0.964\n","epoch 24 - train loss/acc: 0.050 0.987, valid loss/acc: 0.120 0.965\n","Combination 22 - lr: 2.189, n_epochs: 25, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.271 0.920, valid loss/acc: 0.278 0.918\n","epoch 01 - train loss/acc: 0.203 0.940, valid loss/acc: 0.214 0.936\n","epoch 02 - train loss/acc: 0.162 0.953, valid loss/acc: 0.179 0.948\n","epoch 03 - train loss/acc: 0.141 0.958, valid loss/acc: 0.160 0.952\n","epoch 04 - train loss/acc: 0.117 0.967, valid loss/acc: 0.143 0.959\n","epoch 05 - train loss/acc: 0.102 0.971, valid loss/acc: 0.129 0.962\n","epoch 06 - train loss/acc: 0.092 0.974, valid loss/acc: 0.123 0.964\n","epoch 07 - train loss/acc: 0.083 0.978, valid loss/acc: 0.117 0.966\n","epoch 08 - train loss/acc: 0.076 0.979, valid loss/acc: 0.112 0.968\n","epoch 09 - train loss/acc: 0.068 0.982, valid loss/acc: 0.109 0.969\n","epoch 10 - train loss/acc: 0.063 0.983, valid loss/acc: 0.105 0.969\n","epoch 11 - train loss/acc: 0.057 0.985, valid loss/acc: 0.103 0.969\n","epoch 12 - train loss/acc: 0.055 0.985, valid loss/acc: 0.100 0.969\n","epoch 13 - train loss/acc: 0.051 0.987, valid loss/acc: 0.101 0.971\n","epoch 14 - train loss/acc: 0.046 0.989, valid loss/acc: 0.097 0.971\n","epoch 15 - train loss/acc: 0.041 0.990, valid loss/acc: 0.096 0.972\n","epoch 16 - train loss/acc: 0.042 0.990, valid loss/acc: 0.097 0.971\n","epoch 17 - train loss/acc: 0.037 0.992, valid loss/acc: 0.095 0.972\n","epoch 18 - train loss/acc: 0.035 0.992, valid loss/acc: 0.094 0.972\n","epoch 19 - train loss/acc: 0.033 0.993, valid loss/acc: 0.094 0.972\n","epoch 20 - train loss/acc: 0.032 0.993, valid loss/acc: 0.094 0.972\n","epoch 21 - train loss/acc: 0.029 0.995, valid loss/acc: 0.091 0.973\n","epoch 22 - train loss/acc: 0.029 0.995, valid loss/acc: 0.093 0.973\n","epoch 23 - train loss/acc: 0.025 0.996, valid loss/acc: 0.092 0.972\n","epoch 24 - train loss/acc: 0.025 0.996, valid loss/acc: 0.092 0.972\n","Combination 23 - lr: 2.189, n_epochs: 25, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.295 0.915, valid loss/acc: 0.306 0.912\n","epoch 01 - train loss/acc: 0.219 0.938, valid loss/acc: 0.233 0.933\n","epoch 02 - train loss/acc: 0.174 0.949, valid loss/acc: 0.190 0.943\n","epoch 03 - train loss/acc: 0.145 0.959, valid loss/acc: 0.164 0.953\n","epoch 04 - train loss/acc: 0.129 0.963, valid loss/acc: 0.151 0.955\n","epoch 05 - train loss/acc: 0.112 0.969, valid loss/acc: 0.137 0.959\n","epoch 06 - train loss/acc: 0.102 0.972, valid loss/acc: 0.131 0.960\n","epoch 07 - train loss/acc: 0.094 0.974, valid loss/acc: 0.127 0.962\n","epoch 08 - train loss/acc: 0.089 0.975, valid loss/acc: 0.122 0.963\n","epoch 09 - train loss/acc: 0.081 0.977, valid loss/acc: 0.120 0.963\n","epoch 10 - train loss/acc: 0.073 0.980, valid loss/acc: 0.113 0.966\n","epoch 11 - train loss/acc: 0.067 0.982, valid loss/acc: 0.110 0.968\n","epoch 12 - train loss/acc: 0.066 0.982, valid loss/acc: 0.111 0.967\n","epoch 13 - train loss/acc: 0.058 0.985, valid loss/acc: 0.105 0.969\n","epoch 14 - train loss/acc: 0.054 0.986, valid loss/acc: 0.104 0.969\n","epoch 15 - train loss/acc: 0.053 0.986, valid loss/acc: 0.105 0.969\n","epoch 16 - train loss/acc: 0.047 0.989, valid loss/acc: 0.102 0.970\n","epoch 17 - train loss/acc: 0.045 0.989, valid loss/acc: 0.101 0.969\n","epoch 18 - train loss/acc: 0.044 0.989, valid loss/acc: 0.102 0.969\n","epoch 19 - train loss/acc: 0.039 0.991, valid loss/acc: 0.098 0.970\n","epoch 20 - train loss/acc: 0.038 0.991, valid loss/acc: 0.099 0.970\n","epoch 21 - train loss/acc: 0.035 0.992, valid loss/acc: 0.099 0.970\n","epoch 22 - train loss/acc: 0.034 0.993, valid loss/acc: 0.098 0.972\n","epoch 23 - train loss/acc: 0.032 0.993, valid loss/acc: 0.099 0.969\n","epoch 24 - train loss/acc: 0.033 0.992, valid loss/acc: 0.101 0.971\n","Combination 24 - lr: 2.189, n_epochs: 25, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.347 0.903, valid loss/acc: 0.353 0.900\n","epoch 01 - train loss/acc: 0.273 0.922, valid loss/acc: 0.283 0.919\n","epoch 02 - train loss/acc: 0.232 0.933, valid loss/acc: 0.246 0.929\n","epoch 03 - train loss/acc: 0.209 0.940, valid loss/acc: 0.227 0.936\n","epoch 04 - train loss/acc: 0.193 0.946, valid loss/acc: 0.212 0.939\n","epoch 05 - train loss/acc: 0.174 0.950, valid loss/acc: 0.196 0.944\n","epoch 06 - train loss/acc: 0.163 0.952, valid loss/acc: 0.188 0.946\n","epoch 07 - train loss/acc: 0.149 0.959, valid loss/acc: 0.175 0.951\n","epoch 08 - train loss/acc: 0.142 0.960, valid loss/acc: 0.171 0.951\n","epoch 09 - train loss/acc: 0.134 0.962, valid loss/acc: 0.163 0.953\n","epoch 10 - train loss/acc: 0.128 0.964, valid loss/acc: 0.159 0.955\n","epoch 11 - train loss/acc: 0.119 0.967, valid loss/acc: 0.153 0.956\n","epoch 12 - train loss/acc: 0.115 0.968, valid loss/acc: 0.149 0.957\n","epoch 13 - train loss/acc: 0.113 0.969, valid loss/acc: 0.149 0.957\n","epoch 14 - train loss/acc: 0.112 0.969, valid loss/acc: 0.151 0.956\n","epoch 15 - train loss/acc: 0.101 0.972, valid loss/acc: 0.141 0.959\n","epoch 16 - train loss/acc: 0.098 0.973, valid loss/acc: 0.141 0.960\n","epoch 17 - train loss/acc: 0.095 0.974, valid loss/acc: 0.137 0.960\n","epoch 18 - train loss/acc: 0.096 0.973, valid loss/acc: 0.141 0.960\n","epoch 19 - train loss/acc: 0.090 0.976, valid loss/acc: 0.136 0.960\n","epoch 20 - train loss/acc: 0.087 0.977, valid loss/acc: 0.134 0.961\n","epoch 21 - train loss/acc: 0.086 0.977, valid loss/acc: 0.135 0.961\n","epoch 22 - train loss/acc: 0.085 0.976, valid loss/acc: 0.135 0.961\n","epoch 23 - train loss/acc: 0.079 0.979, valid loss/acc: 0.131 0.962\n","epoch 24 - train loss/acc: 0.078 0.979, valid loss/acc: 0.131 0.963\n","Combination 25 - lr: 2.189, n_epochs: 25, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.346 0.901, valid loss/acc: 0.351 0.897\n","epoch 01 - train loss/acc: 0.278 0.919, valid loss/acc: 0.284 0.916\n","epoch 02 - train loss/acc: 0.224 0.934, valid loss/acc: 0.233 0.931\n","epoch 03 - train loss/acc: 0.197 0.943, valid loss/acc: 0.209 0.940\n","epoch 04 - train loss/acc: 0.180 0.947, valid loss/acc: 0.194 0.943\n","epoch 05 - train loss/acc: 0.157 0.955, valid loss/acc: 0.174 0.949\n","epoch 06 - train loss/acc: 0.145 0.959, valid loss/acc: 0.163 0.952\n","epoch 07 - train loss/acc: 0.135 0.961, valid loss/acc: 0.155 0.953\n","epoch 08 - train loss/acc: 0.126 0.965, valid loss/acc: 0.147 0.957\n","epoch 09 - train loss/acc: 0.117 0.966, valid loss/acc: 0.141 0.958\n","epoch 10 - train loss/acc: 0.107 0.970, valid loss/acc: 0.132 0.961\n","epoch 11 - train loss/acc: 0.100 0.972, valid loss/acc: 0.127 0.962\n","epoch 12 - train loss/acc: 0.093 0.974, valid loss/acc: 0.123 0.963\n","epoch 13 - train loss/acc: 0.088 0.977, valid loss/acc: 0.118 0.964\n","epoch 14 - train loss/acc: 0.086 0.976, valid loss/acc: 0.120 0.964\n","epoch 15 - train loss/acc: 0.080 0.979, valid loss/acc: 0.114 0.966\n","epoch 16 - train loss/acc: 0.076 0.980, valid loss/acc: 0.110 0.968\n","epoch 17 - train loss/acc: 0.072 0.981, valid loss/acc: 0.108 0.966\n","epoch 18 - train loss/acc: 0.069 0.982, valid loss/acc: 0.106 0.967\n","epoch 19 - train loss/acc: 0.067 0.982, valid loss/acc: 0.105 0.967\n","epoch 20 - train loss/acc: 0.063 0.983, valid loss/acc: 0.104 0.969\n","epoch 21 - train loss/acc: 0.062 0.984, valid loss/acc: 0.103 0.969\n","epoch 22 - train loss/acc: 0.060 0.984, valid loss/acc: 0.102 0.969\n","epoch 23 - train loss/acc: 0.056 0.986, valid loss/acc: 0.098 0.970\n","epoch 24 - train loss/acc: 0.054 0.987, valid loss/acc: 0.098 0.971\n","Combination 26 - lr: 2.189, n_epochs: 25, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.372 0.897, valid loss/acc: 0.377 0.896\n","epoch 01 - train loss/acc: 0.282 0.918, valid loss/acc: 0.288 0.915\n","epoch 02 - train loss/acc: 0.241 0.931, valid loss/acc: 0.251 0.927\n","epoch 03 - train loss/acc: 0.203 0.942, valid loss/acc: 0.216 0.937\n","epoch 04 - train loss/acc: 0.183 0.947, valid loss/acc: 0.197 0.942\n","epoch 05 - train loss/acc: 0.165 0.953, valid loss/acc: 0.180 0.946\n","epoch 06 - train loss/acc: 0.153 0.956, valid loss/acc: 0.171 0.953\n","epoch 07 - train loss/acc: 0.138 0.961, valid loss/acc: 0.158 0.954\n","epoch 08 - train loss/acc: 0.128 0.963, valid loss/acc: 0.150 0.957\n","epoch 09 - train loss/acc: 0.120 0.966, valid loss/acc: 0.143 0.959\n","epoch 10 - train loss/acc: 0.112 0.968, valid loss/acc: 0.135 0.960\n","epoch 11 - train loss/acc: 0.104 0.971, valid loss/acc: 0.130 0.961\n","epoch 12 - train loss/acc: 0.098 0.973, valid loss/acc: 0.126 0.963\n","epoch 13 - train loss/acc: 0.092 0.975, valid loss/acc: 0.120 0.964\n","epoch 14 - train loss/acc: 0.088 0.976, valid loss/acc: 0.118 0.964\n","epoch 15 - train loss/acc: 0.083 0.977, valid loss/acc: 0.114 0.966\n","epoch 16 - train loss/acc: 0.080 0.978, valid loss/acc: 0.113 0.965\n","epoch 17 - train loss/acc: 0.075 0.980, valid loss/acc: 0.111 0.967\n","epoch 18 - train loss/acc: 0.071 0.981, valid loss/acc: 0.108 0.968\n","epoch 19 - train loss/acc: 0.067 0.982, valid loss/acc: 0.103 0.968\n","epoch 20 - train loss/acc: 0.065 0.983, valid loss/acc: 0.103 0.968\n","epoch 21 - train loss/acc: 0.062 0.984, valid loss/acc: 0.101 0.970\n","epoch 22 - train loss/acc: 0.061 0.984, valid loss/acc: 0.102 0.968\n","epoch 23 - train loss/acc: 0.056 0.986, valid loss/acc: 0.098 0.971\n","epoch 24 - train loss/acc: 0.054 0.986, valid loss/acc: 0.097 0.970\n","Combination 27 - lr: 1.064, n_epochs: 15, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.290 0.916, valid loss/acc: 0.294 0.913\n","epoch 01 - train loss/acc: 0.222 0.935, valid loss/acc: 0.234 0.931\n","epoch 02 - train loss/acc: 0.185 0.947, valid loss/acc: 0.204 0.941\n","epoch 03 - train loss/acc: 0.159 0.955, valid loss/acc: 0.183 0.947\n","epoch 04 - train loss/acc: 0.144 0.959, valid loss/acc: 0.170 0.951\n","epoch 05 - train loss/acc: 0.133 0.962, valid loss/acc: 0.166 0.950\n","epoch 06 - train loss/acc: 0.122 0.965, valid loss/acc: 0.156 0.954\n","epoch 07 - train loss/acc: 0.105 0.971, valid loss/acc: 0.142 0.958\n","epoch 08 - train loss/acc: 0.099 0.972, valid loss/acc: 0.139 0.959\n","epoch 09 - train loss/acc: 0.095 0.974, valid loss/acc: 0.138 0.959\n","epoch 10 - train loss/acc: 0.089 0.975, valid loss/acc: 0.134 0.961\n","epoch 11 - train loss/acc: 0.083 0.977, valid loss/acc: 0.134 0.961\n","epoch 12 - train loss/acc: 0.078 0.978, valid loss/acc: 0.127 0.962\n","epoch 13 - train loss/acc: 0.073 0.979, valid loss/acc: 0.126 0.963\n","epoch 14 - train loss/acc: 0.072 0.980, valid loss/acc: 0.128 0.961\n","Combination 28 - lr: 1.064, n_epochs: 15, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.264 0.922, valid loss/acc: 0.270 0.920\n","epoch 01 - train loss/acc: 0.215 0.938, valid loss/acc: 0.229 0.933\n","epoch 02 - train loss/acc: 0.166 0.952, valid loss/acc: 0.182 0.947\n","epoch 03 - train loss/acc: 0.139 0.960, valid loss/acc: 0.157 0.954\n","epoch 04 - train loss/acc: 0.120 0.966, valid loss/acc: 0.144 0.958\n","epoch 05 - train loss/acc: 0.104 0.971, valid loss/acc: 0.129 0.961\n","epoch 06 - train loss/acc: 0.096 0.972, valid loss/acc: 0.126 0.964\n","epoch 07 - train loss/acc: 0.087 0.976, valid loss/acc: 0.117 0.965\n","epoch 08 - train loss/acc: 0.076 0.979, valid loss/acc: 0.110 0.967\n","epoch 09 - train loss/acc: 0.070 0.981, valid loss/acc: 0.105 0.969\n","epoch 10 - train loss/acc: 0.065 0.982, valid loss/acc: 0.105 0.967\n","epoch 11 - train loss/acc: 0.059 0.984, valid loss/acc: 0.098 0.971\n","epoch 12 - train loss/acc: 0.057 0.985, valid loss/acc: 0.100 0.971\n","epoch 13 - train loss/acc: 0.050 0.988, valid loss/acc: 0.096 0.971\n","epoch 14 - train loss/acc: 0.047 0.988, valid loss/acc: 0.094 0.971\n","Combination 29 - lr: 1.064, n_epochs: 15, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.280 0.916, valid loss/acc: 0.289 0.912\n","epoch 01 - train loss/acc: 0.206 0.940, valid loss/acc: 0.218 0.938\n","epoch 02 - train loss/acc: 0.164 0.952, valid loss/acc: 0.180 0.947\n","epoch 03 - train loss/acc: 0.136 0.961, valid loss/acc: 0.156 0.953\n","epoch 04 - train loss/acc: 0.119 0.966, valid loss/acc: 0.142 0.958\n","epoch 05 - train loss/acc: 0.105 0.970, valid loss/acc: 0.133 0.961\n","epoch 06 - train loss/acc: 0.094 0.973, valid loss/acc: 0.123 0.964\n","epoch 07 - train loss/acc: 0.081 0.977, valid loss/acc: 0.113 0.967\n","epoch 08 - train loss/acc: 0.072 0.980, valid loss/acc: 0.106 0.969\n","epoch 09 - train loss/acc: 0.065 0.982, valid loss/acc: 0.102 0.969\n","epoch 10 - train loss/acc: 0.061 0.983, valid loss/acc: 0.100 0.970\n","epoch 11 - train loss/acc: 0.053 0.986, valid loss/acc: 0.095 0.971\n","epoch 12 - train loss/acc: 0.049 0.988, valid loss/acc: 0.092 0.972\n","epoch 13 - train loss/acc: 0.046 0.988, valid loss/acc: 0.090 0.972\n","epoch 14 - train loss/acc: 0.043 0.989, valid loss/acc: 0.090 0.974\n","Combination 30 - lr: 1.064, n_epochs: 15, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.344 0.905, valid loss/acc: 0.350 0.899\n","epoch 01 - train loss/acc: 0.276 0.922, valid loss/acc: 0.283 0.917\n","epoch 02 - train loss/acc: 0.235 0.932, valid loss/acc: 0.244 0.928\n","epoch 03 - train loss/acc: 0.208 0.941, valid loss/acc: 0.220 0.936\n","epoch 04 - train loss/acc: 0.188 0.946, valid loss/acc: 0.204 0.941\n","epoch 05 - train loss/acc: 0.173 0.951, valid loss/acc: 0.190 0.945\n","epoch 06 - train loss/acc: 0.161 0.955, valid loss/acc: 0.179 0.948\n","epoch 07 - train loss/acc: 0.150 0.958, valid loss/acc: 0.169 0.950\n","epoch 08 - train loss/acc: 0.141 0.960, valid loss/acc: 0.163 0.952\n","epoch 09 - train loss/acc: 0.134 0.962, valid loss/acc: 0.160 0.952\n","epoch 10 - train loss/acc: 0.126 0.964, valid loss/acc: 0.152 0.955\n","epoch 11 - train loss/acc: 0.122 0.965, valid loss/acc: 0.150 0.954\n","epoch 12 - train loss/acc: 0.116 0.967, valid loss/acc: 0.147 0.956\n","epoch 13 - train loss/acc: 0.112 0.968, valid loss/acc: 0.143 0.956\n","epoch 14 - train loss/acc: 0.107 0.970, valid loss/acc: 0.139 0.957\n","Combination 31 - lr: 1.064, n_epochs: 15, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.334 0.904, valid loss/acc: 0.341 0.900\n","epoch 01 - train loss/acc: 0.269 0.923, valid loss/acc: 0.274 0.919\n","epoch 02 - train loss/acc: 0.229 0.934, valid loss/acc: 0.240 0.931\n","epoch 03 - train loss/acc: 0.202 0.941, valid loss/acc: 0.216 0.938\n","epoch 04 - train loss/acc: 0.189 0.944, valid loss/acc: 0.206 0.939\n","epoch 05 - train loss/acc: 0.166 0.952, valid loss/acc: 0.182 0.947\n","epoch 06 - train loss/acc: 0.148 0.958, valid loss/acc: 0.168 0.952\n","epoch 07 - train loss/acc: 0.136 0.962, valid loss/acc: 0.157 0.954\n","epoch 08 - train loss/acc: 0.128 0.964, valid loss/acc: 0.153 0.955\n","epoch 09 - train loss/acc: 0.119 0.966, valid loss/acc: 0.145 0.958\n","epoch 10 - train loss/acc: 0.111 0.969, valid loss/acc: 0.139 0.960\n","epoch 11 - train loss/acc: 0.103 0.972, valid loss/acc: 0.132 0.961\n","epoch 12 - train loss/acc: 0.097 0.973, valid loss/acc: 0.128 0.962\n","epoch 13 - train loss/acc: 0.091 0.975, valid loss/acc: 0.124 0.964\n","epoch 14 - train loss/acc: 0.090 0.975, valid loss/acc: 0.126 0.963\n","Combination 32 - lr: 1.064, n_epochs: 15, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.333 0.905, valid loss/acc: 0.338 0.902\n","epoch 01 - train loss/acc: 0.272 0.922, valid loss/acc: 0.281 0.919\n","epoch 02 - train loss/acc: 0.241 0.929, valid loss/acc: 0.251 0.925\n","epoch 03 - train loss/acc: 0.207 0.940, valid loss/acc: 0.219 0.935\n","epoch 04 - train loss/acc: 0.180 0.948, valid loss/acc: 0.195 0.944\n","epoch 05 - train loss/acc: 0.162 0.954, valid loss/acc: 0.177 0.949\n","epoch 06 - train loss/acc: 0.149 0.958, valid loss/acc: 0.167 0.952\n","epoch 07 - train loss/acc: 0.134 0.962, valid loss/acc: 0.154 0.955\n","epoch 08 - train loss/acc: 0.122 0.966, valid loss/acc: 0.144 0.958\n","epoch 09 - train loss/acc: 0.115 0.968, valid loss/acc: 0.140 0.960\n","epoch 10 - train loss/acc: 0.104 0.971, valid loss/acc: 0.130 0.962\n","epoch 11 - train loss/acc: 0.098 0.973, valid loss/acc: 0.126 0.962\n","epoch 12 - train loss/acc: 0.091 0.975, valid loss/acc: 0.120 0.965\n","epoch 13 - train loss/acc: 0.086 0.976, valid loss/acc: 0.118 0.965\n","epoch 14 - train loss/acc: 0.080 0.978, valid loss/acc: 0.110 0.968\n","Combination 33 - lr: 1.064, n_epochs: 15, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.467 0.884, valid loss/acc: 0.473 0.878\n","epoch 01 - train loss/acc: 0.339 0.907, valid loss/acc: 0.346 0.904\n","epoch 02 - train loss/acc: 0.294 0.917, valid loss/acc: 0.301 0.914\n","epoch 03 - train loss/acc: 0.269 0.923, valid loss/acc: 0.277 0.920\n","epoch 04 - train loss/acc: 0.244 0.930, valid loss/acc: 0.254 0.927\n","epoch 05 - train loss/acc: 0.229 0.935, valid loss/acc: 0.240 0.932\n","epoch 06 - train loss/acc: 0.215 0.939, valid loss/acc: 0.227 0.935\n","epoch 07 - train loss/acc: 0.204 0.942, valid loss/acc: 0.218 0.938\n","epoch 08 - train loss/acc: 0.193 0.946, valid loss/acc: 0.209 0.942\n","epoch 09 - train loss/acc: 0.185 0.948, valid loss/acc: 0.201 0.943\n","epoch 10 - train loss/acc: 0.178 0.950, valid loss/acc: 0.196 0.944\n","epoch 11 - train loss/acc: 0.170 0.952, valid loss/acc: 0.190 0.946\n","epoch 12 - train loss/acc: 0.164 0.954, valid loss/acc: 0.185 0.947\n","epoch 13 - train loss/acc: 0.159 0.956, valid loss/acc: 0.182 0.948\n","epoch 14 - train loss/acc: 0.155 0.957, valid loss/acc: 0.178 0.949\n","Combination 34 - lr: 1.064, n_epochs: 15, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.440 0.883, valid loss/acc: 0.445 0.880\n","epoch 01 - train loss/acc: 0.338 0.906, valid loss/acc: 0.343 0.903\n","epoch 02 - train loss/acc: 0.291 0.917, valid loss/acc: 0.298 0.914\n","epoch 03 - train loss/acc: 0.260 0.925, valid loss/acc: 0.269 0.923\n","epoch 04 - train loss/acc: 0.242 0.930, valid loss/acc: 0.252 0.928\n","epoch 05 - train loss/acc: 0.222 0.937, valid loss/acc: 0.232 0.933\n","epoch 06 - train loss/acc: 0.211 0.939, valid loss/acc: 0.223 0.934\n","epoch 07 - train loss/acc: 0.194 0.945, valid loss/acc: 0.207 0.940\n","epoch 08 - train loss/acc: 0.183 0.948, valid loss/acc: 0.197 0.943\n","epoch 09 - train loss/acc: 0.173 0.951, valid loss/acc: 0.189 0.946\n","epoch 10 - train loss/acc: 0.164 0.953, valid loss/acc: 0.181 0.947\n","epoch 11 - train loss/acc: 0.157 0.956, valid loss/acc: 0.175 0.950\n","epoch 12 - train loss/acc: 0.150 0.958, valid loss/acc: 0.169 0.951\n","epoch 13 - train loss/acc: 0.144 0.959, valid loss/acc: 0.163 0.952\n","epoch 14 - train loss/acc: 0.139 0.961, valid loss/acc: 0.160 0.954\n","Combination 35 - lr: 1.064, n_epochs: 15, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.445 0.870, valid loss/acc: 0.449 0.868\n","epoch 01 - train loss/acc: 0.329 0.906, valid loss/acc: 0.335 0.902\n","epoch 02 - train loss/acc: 0.297 0.914, valid loss/acc: 0.305 0.910\n","epoch 03 - train loss/acc: 0.271 0.921, valid loss/acc: 0.279 0.919\n","epoch 04 - train loss/acc: 0.252 0.928, valid loss/acc: 0.261 0.925\n","epoch 05 - train loss/acc: 0.233 0.933, valid loss/acc: 0.242 0.931\n","epoch 06 - train loss/acc: 0.219 0.937, valid loss/acc: 0.229 0.934\n","epoch 07 - train loss/acc: 0.205 0.941, valid loss/acc: 0.216 0.938\n","epoch 08 - train loss/acc: 0.199 0.943, valid loss/acc: 0.210 0.939\n","epoch 09 - train loss/acc: 0.186 0.947, valid loss/acc: 0.201 0.942\n","epoch 10 - train loss/acc: 0.174 0.951, valid loss/acc: 0.188 0.945\n","epoch 11 - train loss/acc: 0.164 0.952, valid loss/acc: 0.180 0.947\n","epoch 12 - train loss/acc: 0.157 0.956, valid loss/acc: 0.174 0.950\n","epoch 13 - train loss/acc: 0.148 0.957, valid loss/acc: 0.165 0.952\n","epoch 14 - train loss/acc: 0.144 0.959, valid loss/acc: 0.161 0.954\n","Combination 36 - lr: 1.064, n_epochs: 20, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.281 0.919, valid loss/acc: 0.287 0.917\n","epoch 01 - train loss/acc: 0.208 0.940, valid loss/acc: 0.223 0.936\n","epoch 02 - train loss/acc: 0.182 0.947, valid loss/acc: 0.201 0.942\n","epoch 03 - train loss/acc: 0.156 0.956, valid loss/acc: 0.176 0.949\n","epoch 04 - train loss/acc: 0.137 0.961, valid loss/acc: 0.162 0.951\n","epoch 05 - train loss/acc: 0.126 0.964, valid loss/acc: 0.152 0.954\n","epoch 06 - train loss/acc: 0.115 0.967, valid loss/acc: 0.144 0.956\n","epoch 07 - train loss/acc: 0.107 0.970, valid loss/acc: 0.141 0.959\n","epoch 08 - train loss/acc: 0.100 0.972, valid loss/acc: 0.137 0.960\n","epoch 09 - train loss/acc: 0.094 0.973, valid loss/acc: 0.133 0.958\n","epoch 10 - train loss/acc: 0.086 0.976, valid loss/acc: 0.128 0.962\n","epoch 11 - train loss/acc: 0.083 0.977, valid loss/acc: 0.128 0.962\n","epoch 12 - train loss/acc: 0.077 0.979, valid loss/acc: 0.123 0.963\n","epoch 13 - train loss/acc: 0.080 0.977, valid loss/acc: 0.129 0.960\n","epoch 14 - train loss/acc: 0.074 0.979, valid loss/acc: 0.125 0.962\n","epoch 15 - train loss/acc: 0.067 0.982, valid loss/acc: 0.121 0.964\n","epoch 16 - train loss/acc: 0.068 0.981, valid loss/acc: 0.123 0.963\n","epoch 17 - train loss/acc: 0.061 0.984, valid loss/acc: 0.119 0.963\n","epoch 18 - train loss/acc: 0.059 0.985, valid loss/acc: 0.119 0.963\n","epoch 19 - train loss/acc: 0.058 0.984, valid loss/acc: 0.121 0.964\n","Combination 37 - lr: 1.064, n_epochs: 20, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.260 0.925, valid loss/acc: 0.267 0.924\n","epoch 01 - train loss/acc: 0.206 0.938, valid loss/acc: 0.218 0.934\n","epoch 02 - train loss/acc: 0.160 0.954, valid loss/acc: 0.177 0.948\n","epoch 03 - train loss/acc: 0.138 0.961, valid loss/acc: 0.157 0.955\n","epoch 04 - train loss/acc: 0.121 0.966, valid loss/acc: 0.145 0.957\n","epoch 05 - train loss/acc: 0.104 0.971, valid loss/acc: 0.130 0.959\n","epoch 06 - train loss/acc: 0.095 0.973, valid loss/acc: 0.123 0.962\n","epoch 07 - train loss/acc: 0.087 0.976, valid loss/acc: 0.119 0.964\n","epoch 08 - train loss/acc: 0.078 0.979, valid loss/acc: 0.113 0.965\n","epoch 09 - train loss/acc: 0.071 0.981, valid loss/acc: 0.106 0.967\n","epoch 10 - train loss/acc: 0.064 0.984, valid loss/acc: 0.102 0.969\n","epoch 11 - train loss/acc: 0.060 0.984, valid loss/acc: 0.101 0.969\n","epoch 12 - train loss/acc: 0.056 0.985, valid loss/acc: 0.099 0.969\n","epoch 13 - train loss/acc: 0.051 0.988, valid loss/acc: 0.096 0.970\n","epoch 14 - train loss/acc: 0.049 0.988, valid loss/acc: 0.099 0.969\n","epoch 15 - train loss/acc: 0.046 0.989, valid loss/acc: 0.097 0.969\n","epoch 16 - train loss/acc: 0.042 0.990, valid loss/acc: 0.094 0.970\n","epoch 17 - train loss/acc: 0.041 0.990, valid loss/acc: 0.096 0.970\n","epoch 18 - train loss/acc: 0.036 0.992, valid loss/acc: 0.092 0.971\n","epoch 19 - train loss/acc: 0.035 0.992, valid loss/acc: 0.094 0.970\n","Combination 38 - lr: 1.064, n_epochs: 20, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.299 0.912, valid loss/acc: 0.309 0.909\n","epoch 01 - train loss/acc: 0.208 0.939, valid loss/acc: 0.216 0.937\n","epoch 02 - train loss/acc: 0.163 0.954, valid loss/acc: 0.176 0.949\n","epoch 03 - train loss/acc: 0.135 0.962, valid loss/acc: 0.152 0.956\n","epoch 04 - train loss/acc: 0.114 0.967, valid loss/acc: 0.135 0.959\n","epoch 05 - train loss/acc: 0.101 0.971, valid loss/acc: 0.124 0.963\n","epoch 06 - train loss/acc: 0.093 0.974, valid loss/acc: 0.120 0.965\n","epoch 07 - train loss/acc: 0.082 0.977, valid loss/acc: 0.111 0.967\n","epoch 08 - train loss/acc: 0.072 0.980, valid loss/acc: 0.102 0.969\n","epoch 09 - train loss/acc: 0.064 0.982, valid loss/acc: 0.098 0.971\n","epoch 10 - train loss/acc: 0.057 0.985, valid loss/acc: 0.093 0.972\n","epoch 11 - train loss/acc: 0.053 0.986, valid loss/acc: 0.091 0.973\n","epoch 12 - train loss/acc: 0.048 0.987, valid loss/acc: 0.089 0.973\n","epoch 13 - train loss/acc: 0.044 0.989, valid loss/acc: 0.087 0.973\n","epoch 14 - train loss/acc: 0.042 0.989, valid loss/acc: 0.088 0.973\n","epoch 15 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.975\n","epoch 16 - train loss/acc: 0.035 0.992, valid loss/acc: 0.083 0.975\n","epoch 17 - train loss/acc: 0.033 0.992, valid loss/acc: 0.081 0.975\n","epoch 18 - train loss/acc: 0.029 0.994, valid loss/acc: 0.080 0.976\n","epoch 19 - train loss/acc: 0.027 0.995, valid loss/acc: 0.080 0.976\n","Combination 39 - lr: 1.064, n_epochs: 20, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.359 0.901, valid loss/acc: 0.366 0.894\n","epoch 01 - train loss/acc: 0.275 0.923, valid loss/acc: 0.284 0.919\n","epoch 02 - train loss/acc: 0.234 0.933, valid loss/acc: 0.248 0.928\n","epoch 03 - train loss/acc: 0.209 0.940, valid loss/acc: 0.225 0.935\n","epoch 04 - train loss/acc: 0.191 0.946, valid loss/acc: 0.211 0.940\n","epoch 05 - train loss/acc: 0.176 0.951, valid loss/acc: 0.198 0.944\n","epoch 06 - train loss/acc: 0.164 0.953, valid loss/acc: 0.188 0.947\n","epoch 07 - train loss/acc: 0.154 0.957, valid loss/acc: 0.179 0.949\n","epoch 08 - train loss/acc: 0.142 0.960, valid loss/acc: 0.170 0.951\n","epoch 09 - train loss/acc: 0.136 0.962, valid loss/acc: 0.167 0.952\n","epoch 10 - train loss/acc: 0.129 0.964, valid loss/acc: 0.162 0.953\n","epoch 11 - train loss/acc: 0.123 0.966, valid loss/acc: 0.157 0.954\n","epoch 12 - train loss/acc: 0.117 0.967, valid loss/acc: 0.152 0.955\n","epoch 13 - train loss/acc: 0.112 0.968, valid loss/acc: 0.149 0.956\n","epoch 14 - train loss/acc: 0.110 0.969, valid loss/acc: 0.148 0.956\n","epoch 15 - train loss/acc: 0.104 0.971, valid loss/acc: 0.144 0.958\n","epoch 16 - train loss/acc: 0.100 0.973, valid loss/acc: 0.142 0.960\n","epoch 17 - train loss/acc: 0.096 0.973, valid loss/acc: 0.140 0.959\n","epoch 18 - train loss/acc: 0.094 0.974, valid loss/acc: 0.139 0.961\n","epoch 19 - train loss/acc: 0.093 0.974, valid loss/acc: 0.139 0.961\n","Combination 40 - lr: 1.064, n_epochs: 20, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.344 0.899, valid loss/acc: 0.350 0.895\n","epoch 01 - train loss/acc: 0.270 0.921, valid loss/acc: 0.278 0.918\n","epoch 02 - train loss/acc: 0.228 0.933, valid loss/acc: 0.237 0.931\n","epoch 03 - train loss/acc: 0.203 0.940, valid loss/acc: 0.213 0.939\n","epoch 04 - train loss/acc: 0.177 0.949, valid loss/acc: 0.189 0.945\n","epoch 05 - train loss/acc: 0.157 0.955, valid loss/acc: 0.170 0.950\n","epoch 06 - train loss/acc: 0.145 0.959, valid loss/acc: 0.161 0.954\n","epoch 07 - train loss/acc: 0.133 0.962, valid loss/acc: 0.151 0.956\n","epoch 08 - train loss/acc: 0.127 0.964, valid loss/acc: 0.147 0.957\n","epoch 09 - train loss/acc: 0.116 0.968, valid loss/acc: 0.138 0.959\n","epoch 10 - train loss/acc: 0.107 0.970, valid loss/acc: 0.130 0.963\n","epoch 11 - train loss/acc: 0.101 0.972, valid loss/acc: 0.127 0.962\n","epoch 12 - train loss/acc: 0.096 0.973, valid loss/acc: 0.124 0.963\n","epoch 13 - train loss/acc: 0.091 0.975, valid loss/acc: 0.120 0.964\n","epoch 14 - train loss/acc: 0.087 0.976, valid loss/acc: 0.117 0.964\n","epoch 15 - train loss/acc: 0.080 0.978, valid loss/acc: 0.112 0.967\n","epoch 16 - train loss/acc: 0.076 0.979, valid loss/acc: 0.108 0.967\n","epoch 17 - train loss/acc: 0.072 0.981, valid loss/acc: 0.107 0.967\n","epoch 18 - train loss/acc: 0.069 0.982, valid loss/acc: 0.105 0.967\n","epoch 19 - train loss/acc: 0.066 0.983, valid loss/acc: 0.103 0.968\n","Combination 41 - lr: 1.064, n_epochs: 20, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.335 0.902, valid loss/acc: 0.342 0.898\n","epoch 01 - train loss/acc: 0.273 0.919, valid loss/acc: 0.280 0.917\n","epoch 02 - train loss/acc: 0.229 0.933, valid loss/acc: 0.238 0.930\n","epoch 03 - train loss/acc: 0.202 0.943, valid loss/acc: 0.213 0.939\n","epoch 04 - train loss/acc: 0.179 0.949, valid loss/acc: 0.192 0.945\n","epoch 05 - train loss/acc: 0.161 0.954, valid loss/acc: 0.176 0.949\n","epoch 06 - train loss/acc: 0.148 0.957, valid loss/acc: 0.166 0.951\n","epoch 07 - train loss/acc: 0.136 0.961, valid loss/acc: 0.159 0.953\n","epoch 08 - train loss/acc: 0.123 0.965, valid loss/acc: 0.145 0.957\n","epoch 09 - train loss/acc: 0.113 0.968, valid loss/acc: 0.138 0.959\n","epoch 10 - train loss/acc: 0.104 0.971, valid loss/acc: 0.130 0.960\n","epoch 11 - train loss/acc: 0.097 0.973, valid loss/acc: 0.125 0.963\n","epoch 12 - train loss/acc: 0.091 0.974, valid loss/acc: 0.119 0.964\n","epoch 13 - train loss/acc: 0.089 0.974, valid loss/acc: 0.120 0.963\n","epoch 14 - train loss/acc: 0.079 0.978, valid loss/acc: 0.112 0.967\n","epoch 15 - train loss/acc: 0.076 0.979, valid loss/acc: 0.110 0.968\n","epoch 16 - train loss/acc: 0.070 0.981, valid loss/acc: 0.105 0.969\n","epoch 17 - train loss/acc: 0.068 0.981, valid loss/acc: 0.104 0.968\n","epoch 18 - train loss/acc: 0.064 0.983, valid loss/acc: 0.102 0.968\n","epoch 19 - train loss/acc: 0.060 0.984, valid loss/acc: 0.097 0.970\n","Combination 42 - lr: 1.064, n_epochs: 20, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.473 0.879, valid loss/acc: 0.476 0.874\n","epoch 01 - train loss/acc: 0.342 0.906, valid loss/acc: 0.348 0.902\n","epoch 02 - train loss/acc: 0.292 0.918, valid loss/acc: 0.299 0.914\n","epoch 03 - train loss/acc: 0.264 0.925, valid loss/acc: 0.273 0.920\n","epoch 04 - train loss/acc: 0.244 0.931, valid loss/acc: 0.255 0.926\n","epoch 05 - train loss/acc: 0.227 0.936, valid loss/acc: 0.240 0.930\n","epoch 06 - train loss/acc: 0.218 0.938, valid loss/acc: 0.230 0.933\n","epoch 07 - train loss/acc: 0.203 0.942, valid loss/acc: 0.219 0.937\n","epoch 08 - train loss/acc: 0.196 0.945, valid loss/acc: 0.211 0.938\n","epoch 09 - train loss/acc: 0.186 0.947, valid loss/acc: 0.204 0.942\n","epoch 10 - train loss/acc: 0.176 0.950, valid loss/acc: 0.196 0.945\n","epoch 11 - train loss/acc: 0.170 0.952, valid loss/acc: 0.190 0.946\n","epoch 12 - train loss/acc: 0.165 0.953, valid loss/acc: 0.185 0.947\n","epoch 13 - train loss/acc: 0.159 0.955, valid loss/acc: 0.182 0.947\n","epoch 14 - train loss/acc: 0.153 0.956, valid loss/acc: 0.176 0.950\n","epoch 15 - train loss/acc: 0.150 0.958, valid loss/acc: 0.173 0.951\n","epoch 16 - train loss/acc: 0.145 0.959, valid loss/acc: 0.170 0.951\n","epoch 17 - train loss/acc: 0.141 0.961, valid loss/acc: 0.166 0.953\n","epoch 18 - train loss/acc: 0.137 0.961, valid loss/acc: 0.164 0.953\n","epoch 19 - train loss/acc: 0.135 0.963, valid loss/acc: 0.162 0.954\n","Combination 43 - lr: 1.064, n_epochs: 20, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.441 0.886, valid loss/acc: 0.445 0.882\n","epoch 01 - train loss/acc: 0.333 0.905, valid loss/acc: 0.337 0.902\n","epoch 02 - train loss/acc: 0.290 0.917, valid loss/acc: 0.296 0.915\n","epoch 03 - train loss/acc: 0.259 0.926, valid loss/acc: 0.267 0.923\n","epoch 04 - train loss/acc: 0.243 0.930, valid loss/acc: 0.252 0.927\n","epoch 05 - train loss/acc: 0.226 0.935, valid loss/acc: 0.237 0.932\n","epoch 06 - train loss/acc: 0.211 0.940, valid loss/acc: 0.222 0.934\n","epoch 07 - train loss/acc: 0.196 0.943, valid loss/acc: 0.208 0.940\n","epoch 08 - train loss/acc: 0.186 0.947, valid loss/acc: 0.201 0.941\n","epoch 09 - train loss/acc: 0.177 0.950, valid loss/acc: 0.192 0.945\n","epoch 10 - train loss/acc: 0.167 0.952, valid loss/acc: 0.184 0.947\n","epoch 11 - train loss/acc: 0.160 0.955, valid loss/acc: 0.178 0.949\n","epoch 12 - train loss/acc: 0.153 0.957, valid loss/acc: 0.171 0.951\n","epoch 13 - train loss/acc: 0.145 0.959, valid loss/acc: 0.165 0.952\n","epoch 14 - train loss/acc: 0.138 0.961, valid loss/acc: 0.159 0.953\n","epoch 15 - train loss/acc: 0.135 0.962, valid loss/acc: 0.157 0.954\n","epoch 16 - train loss/acc: 0.128 0.964, valid loss/acc: 0.151 0.956\n","epoch 17 - train loss/acc: 0.124 0.965, valid loss/acc: 0.148 0.957\n","epoch 18 - train loss/acc: 0.119 0.967, valid loss/acc: 0.144 0.958\n","epoch 19 - train loss/acc: 0.115 0.968, valid loss/acc: 0.140 0.958\n","Combination 44 - lr: 1.064, n_epochs: 20, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.432 0.882, valid loss/acc: 0.436 0.880\n","epoch 01 - train loss/acc: 0.330 0.906, valid loss/acc: 0.335 0.904\n","epoch 02 - train loss/acc: 0.293 0.915, valid loss/acc: 0.300 0.913\n","epoch 03 - train loss/acc: 0.267 0.922, valid loss/acc: 0.274 0.920\n","epoch 04 - train loss/acc: 0.248 0.929, valid loss/acc: 0.258 0.926\n","epoch 05 - train loss/acc: 0.231 0.934, valid loss/acc: 0.241 0.931\n","epoch 06 - train loss/acc: 0.218 0.938, valid loss/acc: 0.228 0.935\n","epoch 07 - train loss/acc: 0.201 0.943, valid loss/acc: 0.213 0.939\n","epoch 08 - train loss/acc: 0.189 0.947, valid loss/acc: 0.202 0.941\n","epoch 09 - train loss/acc: 0.178 0.949, valid loss/acc: 0.192 0.944\n","epoch 10 - train loss/acc: 0.169 0.951, valid loss/acc: 0.185 0.947\n","epoch 11 - train loss/acc: 0.162 0.955, valid loss/acc: 0.178 0.947\n","epoch 12 - train loss/acc: 0.152 0.957, valid loss/acc: 0.170 0.950\n","epoch 13 - train loss/acc: 0.145 0.959, valid loss/acc: 0.163 0.953\n","epoch 14 - train loss/acc: 0.142 0.959, valid loss/acc: 0.160 0.953\n","epoch 15 - train loss/acc: 0.134 0.962, valid loss/acc: 0.153 0.955\n","epoch 16 - train loss/acc: 0.129 0.963, valid loss/acc: 0.150 0.956\n","epoch 17 - train loss/acc: 0.125 0.964, valid loss/acc: 0.147 0.956\n","epoch 18 - train loss/acc: 0.120 0.966, valid loss/acc: 0.144 0.958\n","epoch 19 - train loss/acc: 0.113 0.968, valid loss/acc: 0.138 0.960\n","Combination 45 - lr: 1.064, n_epochs: 25, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.274 0.921, valid loss/acc: 0.279 0.917\n","epoch 01 - train loss/acc: 0.211 0.939, valid loss/acc: 0.223 0.937\n","epoch 02 - train loss/acc: 0.182 0.947, valid loss/acc: 0.198 0.943\n","epoch 03 - train loss/acc: 0.155 0.955, valid loss/acc: 0.176 0.949\n","epoch 04 - train loss/acc: 0.147 0.958, valid loss/acc: 0.171 0.951\n","epoch 05 - train loss/acc: 0.128 0.964, valid loss/acc: 0.159 0.954\n","epoch 06 - train loss/acc: 0.116 0.967, valid loss/acc: 0.150 0.955\n","epoch 07 - train loss/acc: 0.107 0.969, valid loss/acc: 0.143 0.958\n","epoch 08 - train loss/acc: 0.107 0.969, valid loss/acc: 0.147 0.956\n","epoch 09 - train loss/acc: 0.099 0.971, valid loss/acc: 0.142 0.957\n","epoch 10 - train loss/acc: 0.096 0.973, valid loss/acc: 0.142 0.958\n","epoch 11 - train loss/acc: 0.083 0.977, valid loss/acc: 0.131 0.961\n","epoch 12 - train loss/acc: 0.081 0.978, valid loss/acc: 0.132 0.960\n","epoch 13 - train loss/acc: 0.081 0.977, valid loss/acc: 0.133 0.961\n","epoch 14 - train loss/acc: 0.074 0.980, valid loss/acc: 0.131 0.960\n","epoch 15 - train loss/acc: 0.070 0.982, valid loss/acc: 0.130 0.963\n","epoch 16 - train loss/acc: 0.067 0.982, valid loss/acc: 0.128 0.961\n","epoch 17 - train loss/acc: 0.062 0.984, valid loss/acc: 0.125 0.963\n","epoch 18 - train loss/acc: 0.061 0.983, valid loss/acc: 0.126 0.963\n","epoch 19 - train loss/acc: 0.058 0.985, valid loss/acc: 0.123 0.965\n","epoch 20 - train loss/acc: 0.057 0.985, valid loss/acc: 0.126 0.963\n","epoch 21 - train loss/acc: 0.053 0.986, valid loss/acc: 0.124 0.964\n","epoch 22 - train loss/acc: 0.054 0.985, valid loss/acc: 0.126 0.963\n","epoch 23 - train loss/acc: 0.055 0.985, valid loss/acc: 0.129 0.961\n","epoch 24 - train loss/acc: 0.051 0.986, valid loss/acc: 0.128 0.962\n","Combination 46 - lr: 1.064, n_epochs: 25, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.278 0.919, valid loss/acc: 0.285 0.915\n","epoch 01 - train loss/acc: 0.204 0.940, valid loss/acc: 0.215 0.937\n","epoch 02 - train loss/acc: 0.164 0.952, valid loss/acc: 0.179 0.948\n","epoch 03 - train loss/acc: 0.140 0.959, valid loss/acc: 0.158 0.954\n","epoch 04 - train loss/acc: 0.119 0.966, valid loss/acc: 0.143 0.957\n","epoch 05 - train loss/acc: 0.107 0.969, valid loss/acc: 0.133 0.960\n","epoch 06 - train loss/acc: 0.095 0.973, valid loss/acc: 0.125 0.963\n","epoch 07 - train loss/acc: 0.085 0.976, valid loss/acc: 0.117 0.965\n","epoch 08 - train loss/acc: 0.077 0.979, valid loss/acc: 0.110 0.967\n","epoch 09 - train loss/acc: 0.074 0.979, valid loss/acc: 0.111 0.966\n","epoch 10 - train loss/acc: 0.066 0.982, valid loss/acc: 0.106 0.969\n","epoch 11 - train loss/acc: 0.059 0.985, valid loss/acc: 0.101 0.970\n","epoch 12 - train loss/acc: 0.054 0.986, valid loss/acc: 0.098 0.970\n","epoch 13 - train loss/acc: 0.051 0.987, valid loss/acc: 0.097 0.970\n","epoch 14 - train loss/acc: 0.049 0.987, valid loss/acc: 0.096 0.972\n","epoch 15 - train loss/acc: 0.044 0.989, valid loss/acc: 0.095 0.971\n","epoch 16 - train loss/acc: 0.043 0.989, valid loss/acc: 0.095 0.971\n","epoch 17 - train loss/acc: 0.038 0.991, valid loss/acc: 0.094 0.971\n","epoch 18 - train loss/acc: 0.037 0.992, valid loss/acc: 0.092 0.972\n","epoch 19 - train loss/acc: 0.033 0.993, valid loss/acc: 0.091 0.972\n","epoch 20 - train loss/acc: 0.033 0.992, valid loss/acc: 0.093 0.971\n","epoch 21 - train loss/acc: 0.030 0.994, valid loss/acc: 0.091 0.972\n","epoch 22 - train loss/acc: 0.029 0.994, valid loss/acc: 0.092 0.972\n","epoch 23 - train loss/acc: 0.026 0.995, valid loss/acc: 0.090 0.973\n","epoch 24 - train loss/acc: 0.025 0.995, valid loss/acc: 0.091 0.973\n","Combination 47 - lr: 1.064, n_epochs: 25, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.284 0.916, valid loss/acc: 0.289 0.913\n","epoch 01 - train loss/acc: 0.202 0.941, valid loss/acc: 0.213 0.937\n","epoch 02 - train loss/acc: 0.163 0.952, valid loss/acc: 0.178 0.947\n","epoch 03 - train loss/acc: 0.139 0.961, valid loss/acc: 0.156 0.955\n","epoch 04 - train loss/acc: 0.119 0.967, valid loss/acc: 0.140 0.959\n","epoch 05 - train loss/acc: 0.101 0.972, valid loss/acc: 0.124 0.963\n","epoch 06 - train loss/acc: 0.090 0.975, valid loss/acc: 0.116 0.965\n","epoch 07 - train loss/acc: 0.080 0.977, valid loss/acc: 0.110 0.967\n","epoch 08 - train loss/acc: 0.073 0.980, valid loss/acc: 0.103 0.969\n","epoch 09 - train loss/acc: 0.066 0.982, valid loss/acc: 0.099 0.970\n","epoch 10 - train loss/acc: 0.061 0.983, valid loss/acc: 0.096 0.972\n","epoch 11 - train loss/acc: 0.055 0.986, valid loss/acc: 0.095 0.971\n","epoch 12 - train loss/acc: 0.050 0.987, valid loss/acc: 0.091 0.971\n","epoch 13 - train loss/acc: 0.045 0.989, valid loss/acc: 0.087 0.974\n","epoch 14 - train loss/acc: 0.041 0.990, valid loss/acc: 0.084 0.973\n","epoch 15 - train loss/acc: 0.040 0.990, valid loss/acc: 0.083 0.974\n","epoch 16 - train loss/acc: 0.037 0.991, valid loss/acc: 0.084 0.973\n","epoch 17 - train loss/acc: 0.034 0.992, valid loss/acc: 0.082 0.974\n","epoch 18 - train loss/acc: 0.031 0.994, valid loss/acc: 0.081 0.974\n","epoch 19 - train loss/acc: 0.029 0.994, valid loss/acc: 0.079 0.976\n","epoch 20 - train loss/acc: 0.027 0.994, valid loss/acc: 0.080 0.975\n","epoch 21 - train loss/acc: 0.025 0.996, valid loss/acc: 0.079 0.975\n","epoch 22 - train loss/acc: 0.023 0.996, valid loss/acc: 0.079 0.974\n","epoch 23 - train loss/acc: 0.023 0.996, valid loss/acc: 0.078 0.975\n","epoch 24 - train loss/acc: 0.021 0.997, valid loss/acc: 0.077 0.976\n","Combination 48 - lr: 1.064, n_epochs: 25, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.343 0.906, valid loss/acc: 0.348 0.903\n","epoch 01 - train loss/acc: 0.268 0.924, valid loss/acc: 0.276 0.921\n","epoch 02 - train loss/acc: 0.234 0.934, valid loss/acc: 0.243 0.929\n","epoch 03 - train loss/acc: 0.207 0.942, valid loss/acc: 0.218 0.938\n","epoch 04 - train loss/acc: 0.185 0.947, valid loss/acc: 0.202 0.942\n","epoch 05 - train loss/acc: 0.172 0.950, valid loss/acc: 0.190 0.944\n","epoch 06 - train loss/acc: 0.158 0.956, valid loss/acc: 0.177 0.948\n","epoch 07 - train loss/acc: 0.150 0.957, valid loss/acc: 0.172 0.950\n","epoch 08 - train loss/acc: 0.139 0.961, valid loss/acc: 0.161 0.953\n","epoch 09 - train loss/acc: 0.137 0.961, valid loss/acc: 0.162 0.952\n","epoch 10 - train loss/acc: 0.128 0.963, valid loss/acc: 0.156 0.953\n","epoch 11 - train loss/acc: 0.118 0.967, valid loss/acc: 0.148 0.956\n","epoch 12 - train loss/acc: 0.114 0.968, valid loss/acc: 0.145 0.957\n","epoch 13 - train loss/acc: 0.110 0.969, valid loss/acc: 0.144 0.957\n","epoch 14 - train loss/acc: 0.105 0.971, valid loss/acc: 0.139 0.959\n","epoch 15 - train loss/acc: 0.103 0.971, valid loss/acc: 0.141 0.958\n","epoch 16 - train loss/acc: 0.097 0.973, valid loss/acc: 0.134 0.962\n","epoch 17 - train loss/acc: 0.094 0.974, valid loss/acc: 0.132 0.961\n","epoch 18 - train loss/acc: 0.090 0.976, valid loss/acc: 0.130 0.962\n","epoch 19 - train loss/acc: 0.088 0.976, valid loss/acc: 0.130 0.961\n","epoch 20 - train loss/acc: 0.086 0.977, valid loss/acc: 0.129 0.962\n","epoch 21 - train loss/acc: 0.082 0.978, valid loss/acc: 0.127 0.964\n","epoch 22 - train loss/acc: 0.080 0.979, valid loss/acc: 0.128 0.962\n","epoch 23 - train loss/acc: 0.077 0.980, valid loss/acc: 0.124 0.964\n","epoch 24 - train loss/acc: 0.077 0.979, valid loss/acc: 0.127 0.963\n","Combination 49 - lr: 1.064, n_epochs: 25, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.333 0.906, valid loss/acc: 0.337 0.904\n","epoch 01 - train loss/acc: 0.265 0.923, valid loss/acc: 0.272 0.920\n","epoch 02 - train loss/acc: 0.223 0.936, valid loss/acc: 0.232 0.931\n","epoch 03 - train loss/acc: 0.201 0.942, valid loss/acc: 0.212 0.937\n","epoch 04 - train loss/acc: 0.179 0.949, valid loss/acc: 0.193 0.943\n","epoch 05 - train loss/acc: 0.163 0.952, valid loss/acc: 0.178 0.948\n","epoch 06 - train loss/acc: 0.149 0.958, valid loss/acc: 0.165 0.951\n","epoch 07 - train loss/acc: 0.136 0.961, valid loss/acc: 0.155 0.954\n","epoch 08 - train loss/acc: 0.128 0.963, valid loss/acc: 0.150 0.957\n","epoch 09 - train loss/acc: 0.119 0.966, valid loss/acc: 0.142 0.958\n","epoch 10 - train loss/acc: 0.112 0.968, valid loss/acc: 0.135 0.960\n","epoch 11 - train loss/acc: 0.104 0.970, valid loss/acc: 0.130 0.961\n","epoch 12 - train loss/acc: 0.098 0.973, valid loss/acc: 0.127 0.962\n","epoch 13 - train loss/acc: 0.096 0.973, valid loss/acc: 0.125 0.963\n","epoch 14 - train loss/acc: 0.089 0.976, valid loss/acc: 0.119 0.965\n","epoch 15 - train loss/acc: 0.084 0.977, valid loss/acc: 0.115 0.966\n","epoch 16 - train loss/acc: 0.080 0.978, valid loss/acc: 0.114 0.966\n","epoch 17 - train loss/acc: 0.078 0.978, valid loss/acc: 0.114 0.965\n","epoch 18 - train loss/acc: 0.076 0.980, valid loss/acc: 0.112 0.967\n","epoch 19 - train loss/acc: 0.071 0.981, valid loss/acc: 0.110 0.967\n","epoch 20 - train loss/acc: 0.066 0.983, valid loss/acc: 0.105 0.968\n","epoch 21 - train loss/acc: 0.064 0.983, valid loss/acc: 0.105 0.969\n","epoch 22 - train loss/acc: 0.061 0.984, valid loss/acc: 0.101 0.969\n","epoch 23 - train loss/acc: 0.059 0.985, valid loss/acc: 0.102 0.969\n","epoch 24 - train loss/acc: 0.056 0.986, valid loss/acc: 0.099 0.971\n","Combination 50 - lr: 1.064, n_epochs: 25, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.334 0.902, valid loss/acc: 0.340 0.901\n","epoch 01 - train loss/acc: 0.272 0.920, valid loss/acc: 0.278 0.918\n","epoch 02 - train loss/acc: 0.231 0.933, valid loss/acc: 0.238 0.931\n","epoch 03 - train loss/acc: 0.202 0.942, valid loss/acc: 0.213 0.940\n","epoch 04 - train loss/acc: 0.178 0.949, valid loss/acc: 0.191 0.945\n","epoch 05 - train loss/acc: 0.161 0.954, valid loss/acc: 0.176 0.948\n","epoch 06 - train loss/acc: 0.145 0.959, valid loss/acc: 0.163 0.952\n","epoch 07 - train loss/acc: 0.133 0.962, valid loss/acc: 0.151 0.955\n","epoch 08 - train loss/acc: 0.122 0.966, valid loss/acc: 0.144 0.957\n","epoch 09 - train loss/acc: 0.114 0.968, valid loss/acc: 0.136 0.961\n","epoch 10 - train loss/acc: 0.106 0.971, valid loss/acc: 0.131 0.961\n","epoch 11 - train loss/acc: 0.098 0.973, valid loss/acc: 0.124 0.963\n","epoch 12 - train loss/acc: 0.091 0.975, valid loss/acc: 0.118 0.964\n","epoch 13 - train loss/acc: 0.086 0.977, valid loss/acc: 0.115 0.966\n","epoch 14 - train loss/acc: 0.080 0.978, valid loss/acc: 0.110 0.967\n","epoch 15 - train loss/acc: 0.079 0.979, valid loss/acc: 0.112 0.966\n","epoch 16 - train loss/acc: 0.074 0.980, valid loss/acc: 0.109 0.966\n","epoch 17 - train loss/acc: 0.066 0.983, valid loss/acc: 0.100 0.969\n","epoch 18 - train loss/acc: 0.064 0.983, valid loss/acc: 0.100 0.970\n","epoch 19 - train loss/acc: 0.062 0.983, valid loss/acc: 0.099 0.970\n","epoch 20 - train loss/acc: 0.058 0.985, valid loss/acc: 0.096 0.970\n","epoch 21 - train loss/acc: 0.055 0.986, valid loss/acc: 0.093 0.972\n","epoch 22 - train loss/acc: 0.052 0.987, valid loss/acc: 0.093 0.972\n","epoch 23 - train loss/acc: 0.050 0.988, valid loss/acc: 0.091 0.973\n","epoch 24 - train loss/acc: 0.046 0.989, valid loss/acc: 0.089 0.974\n","Combination 51 - lr: 1.064, n_epochs: 25, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.466 0.884, valid loss/acc: 0.469 0.882\n","epoch 01 - train loss/acc: 0.340 0.906, valid loss/acc: 0.346 0.904\n","epoch 02 - train loss/acc: 0.301 0.916, valid loss/acc: 0.306 0.914\n","epoch 03 - train loss/acc: 0.268 0.924, valid loss/acc: 0.275 0.921\n","epoch 04 - train loss/acc: 0.245 0.930, valid loss/acc: 0.255 0.927\n","epoch 05 - train loss/acc: 0.229 0.935, valid loss/acc: 0.241 0.931\n","epoch 06 - train loss/acc: 0.215 0.939, valid loss/acc: 0.227 0.935\n","epoch 07 - train loss/acc: 0.204 0.943, valid loss/acc: 0.216 0.938\n","epoch 08 - train loss/acc: 0.193 0.946, valid loss/acc: 0.208 0.939\n","epoch 09 - train loss/acc: 0.184 0.948, valid loss/acc: 0.199 0.944\n","epoch 10 - train loss/acc: 0.177 0.950, valid loss/acc: 0.193 0.944\n","epoch 11 - train loss/acc: 0.171 0.952, valid loss/acc: 0.190 0.945\n","epoch 12 - train loss/acc: 0.162 0.954, valid loss/acc: 0.180 0.949\n","epoch 13 - train loss/acc: 0.156 0.955, valid loss/acc: 0.175 0.950\n","epoch 14 - train loss/acc: 0.151 0.957, valid loss/acc: 0.172 0.950\n","epoch 15 - train loss/acc: 0.145 0.959, valid loss/acc: 0.167 0.951\n","epoch 16 - train loss/acc: 0.141 0.960, valid loss/acc: 0.164 0.952\n","epoch 17 - train loss/acc: 0.138 0.961, valid loss/acc: 0.161 0.953\n","epoch 18 - train loss/acc: 0.132 0.963, valid loss/acc: 0.156 0.954\n","epoch 19 - train loss/acc: 0.129 0.963, valid loss/acc: 0.154 0.954\n","epoch 20 - train loss/acc: 0.126 0.964, valid loss/acc: 0.153 0.956\n","epoch 21 - train loss/acc: 0.123 0.965, valid loss/acc: 0.151 0.955\n","epoch 22 - train loss/acc: 0.120 0.966, valid loss/acc: 0.148 0.957\n","epoch 23 - train loss/acc: 0.117 0.967, valid loss/acc: 0.148 0.957\n","epoch 24 - train loss/acc: 0.114 0.968, valid loss/acc: 0.145 0.957\n","Combination 52 - lr: 1.064, n_epochs: 25, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.449 0.873, valid loss/acc: 0.454 0.870\n","epoch 01 - train loss/acc: 0.331 0.907, valid loss/acc: 0.336 0.906\n","epoch 02 - train loss/acc: 0.290 0.917, valid loss/acc: 0.296 0.913\n","epoch 03 - train loss/acc: 0.267 0.923, valid loss/acc: 0.272 0.921\n","epoch 04 - train loss/acc: 0.240 0.932, valid loss/acc: 0.249 0.929\n","epoch 05 - train loss/acc: 0.226 0.935, valid loss/acc: 0.236 0.933\n","epoch 06 - train loss/acc: 0.212 0.940, valid loss/acc: 0.223 0.936\n","epoch 07 - train loss/acc: 0.199 0.942, valid loss/acc: 0.211 0.940\n","epoch 08 - train loss/acc: 0.187 0.946, valid loss/acc: 0.201 0.943\n","epoch 09 - train loss/acc: 0.176 0.950, valid loss/acc: 0.191 0.946\n","epoch 10 - train loss/acc: 0.168 0.952, valid loss/acc: 0.183 0.947\n","epoch 11 - train loss/acc: 0.158 0.955, valid loss/acc: 0.176 0.949\n","epoch 12 - train loss/acc: 0.153 0.957, valid loss/acc: 0.171 0.950\n","epoch 13 - train loss/acc: 0.146 0.959, valid loss/acc: 0.165 0.952\n","epoch 14 - train loss/acc: 0.142 0.960, valid loss/acc: 0.162 0.953\n","epoch 15 - train loss/acc: 0.135 0.962, valid loss/acc: 0.157 0.954\n","epoch 16 - train loss/acc: 0.131 0.963, valid loss/acc: 0.154 0.955\n","epoch 17 - train loss/acc: 0.126 0.965, valid loss/acc: 0.148 0.957\n","epoch 18 - train loss/acc: 0.120 0.966, valid loss/acc: 0.145 0.959\n","epoch 19 - train loss/acc: 0.115 0.968, valid loss/acc: 0.141 0.959\n","epoch 20 - train loss/acc: 0.114 0.969, valid loss/acc: 0.140 0.960\n","epoch 21 - train loss/acc: 0.109 0.969, valid loss/acc: 0.136 0.960\n","epoch 22 - train loss/acc: 0.105 0.971, valid loss/acc: 0.133 0.961\n","epoch 23 - train loss/acc: 0.104 0.972, valid loss/acc: 0.134 0.962\n","epoch 24 - train loss/acc: 0.100 0.972, valid loss/acc: 0.130 0.962\n","Combination 53 - lr: 1.064, n_epochs: 25, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.432 0.881, valid loss/acc: 0.435 0.879\n","epoch 01 - train loss/acc: 0.332 0.903, valid loss/acc: 0.337 0.900\n","epoch 02 - train loss/acc: 0.297 0.914, valid loss/acc: 0.302 0.913\n","epoch 03 - train loss/acc: 0.266 0.923, valid loss/acc: 0.272 0.920\n","epoch 04 - train loss/acc: 0.247 0.929, valid loss/acc: 0.255 0.926\n","epoch 05 - train loss/acc: 0.231 0.933, valid loss/acc: 0.239 0.931\n","epoch 06 - train loss/acc: 0.219 0.938, valid loss/acc: 0.229 0.932\n","epoch 07 - train loss/acc: 0.205 0.940, valid loss/acc: 0.217 0.936\n","epoch 08 - train loss/acc: 0.194 0.944, valid loss/acc: 0.205 0.939\n","epoch 09 - train loss/acc: 0.181 0.949, valid loss/acc: 0.195 0.943\n","epoch 10 - train loss/acc: 0.170 0.951, valid loss/acc: 0.185 0.946\n","epoch 11 - train loss/acc: 0.161 0.954, valid loss/acc: 0.177 0.947\n","epoch 12 - train loss/acc: 0.153 0.957, valid loss/acc: 0.170 0.949\n","epoch 13 - train loss/acc: 0.146 0.958, valid loss/acc: 0.164 0.953\n","epoch 14 - train loss/acc: 0.141 0.959, valid loss/acc: 0.160 0.952\n","epoch 15 - train loss/acc: 0.134 0.962, valid loss/acc: 0.154 0.954\n","epoch 16 - train loss/acc: 0.128 0.964, valid loss/acc: 0.148 0.956\n","epoch 17 - train loss/acc: 0.124 0.965, valid loss/acc: 0.145 0.956\n","epoch 18 - train loss/acc: 0.119 0.966, valid loss/acc: 0.140 0.959\n","epoch 19 - train loss/acc: 0.113 0.968, valid loss/acc: 0.137 0.960\n","epoch 20 - train loss/acc: 0.111 0.968, valid loss/acc: 0.135 0.960\n","epoch 21 - train loss/acc: 0.106 0.970, valid loss/acc: 0.130 0.961\n","epoch 22 - train loss/acc: 0.101 0.971, valid loss/acc: 0.127 0.962\n","epoch 23 - train loss/acc: 0.099 0.973, valid loss/acc: 0.126 0.962\n","epoch 24 - train loss/acc: 0.095 0.973, valid loss/acc: 0.122 0.963\n","Combination 54 - lr: 2.803, n_epochs: 15, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.210 0.939, valid loss/acc: 0.224 0.934\n","epoch 01 - train loss/acc: 0.209 0.935, valid loss/acc: 0.233 0.929\n","epoch 02 - train loss/acc: 0.142 0.957, valid loss/acc: 0.176 0.947\n","epoch 03 - train loss/acc: 0.117 0.965, valid loss/acc: 0.157 0.953\n","epoch 04 - train loss/acc: 0.105 0.968, valid loss/acc: 0.146 0.955\n","epoch 05 - train loss/acc: 0.088 0.974, valid loss/acc: 0.136 0.959\n","epoch 06 - train loss/acc: 0.080 0.976, valid loss/acc: 0.127 0.962\n","epoch 07 - train loss/acc: 0.067 0.980, valid loss/acc: 0.123 0.962\n","epoch 08 - train loss/acc: 0.070 0.979, valid loss/acc: 0.131 0.960\n","epoch 09 - train loss/acc: 0.062 0.982, valid loss/acc: 0.122 0.963\n","epoch 10 - train loss/acc: 0.060 0.982, valid loss/acc: 0.126 0.962\n","epoch 11 - train loss/acc: 0.057 0.983, valid loss/acc: 0.132 0.962\n","epoch 12 - train loss/acc: 0.051 0.985, valid loss/acc: 0.127 0.963\n","epoch 13 - train loss/acc: 0.049 0.985, valid loss/acc: 0.128 0.963\n","epoch 14 - train loss/acc: 0.046 0.987, valid loss/acc: 0.130 0.963\n","Combination 55 - lr: 2.803, n_epochs: 15, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.207 0.939, valid loss/acc: 0.222 0.935\n","epoch 01 - train loss/acc: 0.138 0.959, valid loss/acc: 0.157 0.953\n","epoch 02 - train loss/acc: 0.108 0.969, valid loss/acc: 0.134 0.960\n","epoch 03 - train loss/acc: 0.093 0.972, valid loss/acc: 0.125 0.962\n","epoch 04 - train loss/acc: 0.073 0.979, valid loss/acc: 0.111 0.967\n","epoch 05 - train loss/acc: 0.059 0.983, valid loss/acc: 0.102 0.969\n","epoch 06 - train loss/acc: 0.056 0.984, valid loss/acc: 0.106 0.968\n","epoch 07 - train loss/acc: 0.053 0.984, valid loss/acc: 0.109 0.968\n","epoch 08 - train loss/acc: 0.040 0.989, valid loss/acc: 0.095 0.973\n","epoch 09 - train loss/acc: 0.041 0.989, valid loss/acc: 0.101 0.969\n","epoch 10 - train loss/acc: 0.032 0.992, valid loss/acc: 0.097 0.970\n","epoch 11 - train loss/acc: 0.031 0.993, valid loss/acc: 0.098 0.972\n","epoch 12 - train loss/acc: 0.028 0.993, valid loss/acc: 0.099 0.971\n","epoch 13 - train loss/acc: 0.023 0.995, valid loss/acc: 0.097 0.972\n","epoch 14 - train loss/acc: 0.028 0.993, valid loss/acc: 0.106 0.969\n","Combination 56 - lr: 2.803, n_epochs: 15, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.251 0.927, valid loss/acc: 0.266 0.924\n","epoch 01 - train loss/acc: 0.179 0.947, valid loss/acc: 0.197 0.941\n","epoch 02 - train loss/acc: 0.148 0.956, valid loss/acc: 0.177 0.947\n","epoch 03 - train loss/acc: 0.131 0.962, valid loss/acc: 0.166 0.950\n","epoch 04 - train loss/acc: 0.115 0.965, valid loss/acc: 0.158 0.952\n","epoch 05 - train loss/acc: 0.104 0.968, valid loss/acc: 0.158 0.953\n","epoch 06 - train loss/acc: 0.104 0.968, valid loss/acc: 0.157 0.953\n","epoch 07 - train loss/acc: 0.095 0.971, valid loss/acc: 0.151 0.955\n","epoch 08 - train loss/acc: 0.089 0.973, valid loss/acc: 0.151 0.955\n","epoch 09 - train loss/acc: 0.090 0.971, valid loss/acc: 0.149 0.955\n","epoch 10 - train loss/acc: 0.075 0.977, valid loss/acc: 0.136 0.960\n","epoch 11 - train loss/acc: 0.064 0.981, valid loss/acc: 0.132 0.960\n","epoch 12 - train loss/acc: 0.075 0.976, valid loss/acc: 0.142 0.957\n","epoch 13 - train loss/acc: 0.055 0.984, valid loss/acc: 0.129 0.962\n","epoch 14 - train loss/acc: 0.060 0.981, valid loss/acc: 0.136 0.958\n","Combination 57 - lr: 2.803, n_epochs: 15, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.267 0.922, valid loss/acc: 0.273 0.921\n","epoch 01 - train loss/acc: 0.185 0.948, valid loss/acc: 0.199 0.943\n","epoch 02 - train loss/acc: 0.157 0.956, valid loss/acc: 0.177 0.949\n","epoch 03 - train loss/acc: 0.142 0.960, valid loss/acc: 0.166 0.951\n","epoch 04 - train loss/acc: 0.123 0.965, valid loss/acc: 0.150 0.955\n","epoch 05 - train loss/acc: 0.109 0.970, valid loss/acc: 0.141 0.958\n","epoch 06 - train loss/acc: 0.108 0.969, valid loss/acc: 0.140 0.959\n","epoch 07 - train loss/acc: 0.101 0.971, valid loss/acc: 0.136 0.959\n","epoch 08 - train loss/acc: 0.085 0.976, valid loss/acc: 0.126 0.963\n","epoch 09 - train loss/acc: 0.080 0.978, valid loss/acc: 0.125 0.963\n","epoch 10 - train loss/acc: 0.074 0.980, valid loss/acc: 0.123 0.964\n","epoch 11 - train loss/acc: 0.069 0.981, valid loss/acc: 0.121 0.964\n","epoch 12 - train loss/acc: 0.070 0.981, valid loss/acc: 0.124 0.963\n","epoch 13 - train loss/acc: 0.066 0.982, valid loss/acc: 0.122 0.965\n","epoch 14 - train loss/acc: 0.067 0.981, valid loss/acc: 0.128 0.962\n","Combination 58 - lr: 2.803, n_epochs: 15, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.288 0.916, valid loss/acc: 0.298 0.913\n","epoch 01 - train loss/acc: 0.201 0.941, valid loss/acc: 0.216 0.935\n","epoch 02 - train loss/acc: 0.166 0.951, valid loss/acc: 0.190 0.944\n","epoch 03 - train loss/acc: 0.146 0.958, valid loss/acc: 0.173 0.949\n","epoch 04 - train loss/acc: 0.140 0.959, valid loss/acc: 0.167 0.950\n","epoch 05 - train loss/acc: 0.116 0.966, valid loss/acc: 0.148 0.955\n","epoch 06 - train loss/acc: 0.106 0.970, valid loss/acc: 0.140 0.957\n","epoch 07 - train loss/acc: 0.098 0.973, valid loss/acc: 0.135 0.958\n","epoch 08 - train loss/acc: 0.086 0.977, valid loss/acc: 0.127 0.963\n","epoch 09 - train loss/acc: 0.089 0.975, valid loss/acc: 0.133 0.959\n","epoch 10 - train loss/acc: 0.078 0.978, valid loss/acc: 0.124 0.963\n","epoch 11 - train loss/acc: 0.076 0.978, valid loss/acc: 0.125 0.962\n","epoch 12 - train loss/acc: 0.065 0.982, valid loss/acc: 0.117 0.965\n","epoch 13 - train loss/acc: 0.063 0.983, valid loss/acc: 0.119 0.964\n","epoch 14 - train loss/acc: 0.061 0.983, valid loss/acc: 0.118 0.964\n","Combination 59 - lr: 2.803, n_epochs: 15, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.312 0.908, valid loss/acc: 0.319 0.904\n","epoch 01 - train loss/acc: 0.217 0.937, valid loss/acc: 0.232 0.932\n","epoch 02 - train loss/acc: 0.169 0.952, valid loss/acc: 0.188 0.945\n","epoch 03 - train loss/acc: 0.143 0.959, valid loss/acc: 0.167 0.953\n","epoch 04 - train loss/acc: 0.125 0.963, valid loss/acc: 0.153 0.955\n","epoch 05 - train loss/acc: 0.110 0.969, valid loss/acc: 0.141 0.959\n","epoch 06 - train loss/acc: 0.099 0.972, valid loss/acc: 0.134 0.961\n","epoch 07 - train loss/acc: 0.103 0.970, valid loss/acc: 0.141 0.959\n","epoch 08 - train loss/acc: 0.086 0.975, valid loss/acc: 0.131 0.963\n","epoch 09 - train loss/acc: 0.079 0.977, valid loss/acc: 0.126 0.964\n","epoch 10 - train loss/acc: 0.073 0.979, valid loss/acc: 0.119 0.966\n","epoch 11 - train loss/acc: 0.075 0.979, valid loss/acc: 0.125 0.964\n","epoch 12 - train loss/acc: 0.061 0.983, valid loss/acc: 0.114 0.967\n","epoch 13 - train loss/acc: 0.060 0.983, valid loss/acc: 0.115 0.966\n","epoch 14 - train loss/acc: 0.056 0.985, valid loss/acc: 0.112 0.968\n","Combination 60 - lr: 2.803, n_epochs: 15, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.353 0.896, valid loss/acc: 0.358 0.892\n","epoch 01 - train loss/acc: 0.255 0.928, valid loss/acc: 0.262 0.924\n","epoch 02 - train loss/acc: 0.211 0.938, valid loss/acc: 0.222 0.935\n","epoch 03 - train loss/acc: 0.187 0.947, valid loss/acc: 0.200 0.943\n","epoch 04 - train loss/acc: 0.166 0.952, valid loss/acc: 0.183 0.946\n","epoch 05 - train loss/acc: 0.151 0.957, valid loss/acc: 0.172 0.951\n","epoch 06 - train loss/acc: 0.143 0.959, valid loss/acc: 0.165 0.954\n","epoch 07 - train loss/acc: 0.136 0.960, valid loss/acc: 0.163 0.954\n","epoch 08 - train loss/acc: 0.126 0.964, valid loss/acc: 0.156 0.957\n","epoch 09 - train loss/acc: 0.116 0.967, valid loss/acc: 0.145 0.958\n","epoch 10 - train loss/acc: 0.110 0.968, valid loss/acc: 0.141 0.959\n","epoch 11 - train loss/acc: 0.105 0.970, valid loss/acc: 0.138 0.961\n","epoch 12 - train loss/acc: 0.100 0.971, valid loss/acc: 0.137 0.961\n","epoch 13 - train loss/acc: 0.095 0.973, valid loss/acc: 0.133 0.962\n","epoch 14 - train loss/acc: 0.089 0.975, valid loss/acc: 0.128 0.963\n","Combination 61 - lr: 2.803, n_epochs: 15, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.320 0.907, valid loss/acc: 0.326 0.905\n","epoch 01 - train loss/acc: 0.254 0.925, valid loss/acc: 0.262 0.923\n","epoch 02 - train loss/acc: 0.208 0.940, valid loss/acc: 0.220 0.937\n","epoch 03 - train loss/acc: 0.182 0.948, valid loss/acc: 0.197 0.944\n","epoch 04 - train loss/acc: 0.158 0.955, valid loss/acc: 0.175 0.950\n","epoch 05 - train loss/acc: 0.149 0.957, valid loss/acc: 0.169 0.949\n","epoch 06 - train loss/acc: 0.133 0.962, valid loss/acc: 0.154 0.956\n","epoch 07 - train loss/acc: 0.122 0.965, valid loss/acc: 0.147 0.956\n","epoch 08 - train loss/acc: 0.112 0.968, valid loss/acc: 0.138 0.960\n","epoch 09 - train loss/acc: 0.103 0.972, valid loss/acc: 0.133 0.961\n","epoch 10 - train loss/acc: 0.096 0.974, valid loss/acc: 0.127 0.963\n","epoch 11 - train loss/acc: 0.091 0.975, valid loss/acc: 0.126 0.962\n","epoch 12 - train loss/acc: 0.084 0.977, valid loss/acc: 0.119 0.965\n","epoch 13 - train loss/acc: 0.079 0.978, valid loss/acc: 0.117 0.964\n","epoch 14 - train loss/acc: 0.078 0.979, valid loss/acc: 0.115 0.966\n","Combination 62 - lr: 2.803, n_epochs: 15, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.405 0.888, valid loss/acc: 0.411 0.884\n","epoch 01 - train loss/acc: 0.277 0.919, valid loss/acc: 0.284 0.915\n","epoch 02 - train loss/acc: 0.233 0.933, valid loss/acc: 0.244 0.927\n","epoch 03 - train loss/acc: 0.205 0.940, valid loss/acc: 0.219 0.934\n","epoch 04 - train loss/acc: 0.187 0.945, valid loss/acc: 0.204 0.940\n","epoch 05 - train loss/acc: 0.169 0.950, valid loss/acc: 0.188 0.944\n","epoch 06 - train loss/acc: 0.154 0.956, valid loss/acc: 0.175 0.949\n","epoch 07 - train loss/acc: 0.141 0.959, valid loss/acc: 0.164 0.952\n","epoch 08 - train loss/acc: 0.128 0.964, valid loss/acc: 0.154 0.955\n","epoch 09 - train loss/acc: 0.120 0.967, valid loss/acc: 0.147 0.956\n","epoch 10 - train loss/acc: 0.115 0.967, valid loss/acc: 0.143 0.957\n","epoch 11 - train loss/acc: 0.108 0.970, valid loss/acc: 0.138 0.959\n","epoch 12 - train loss/acc: 0.105 0.970, valid loss/acc: 0.138 0.959\n","epoch 13 - train loss/acc: 0.099 0.972, valid loss/acc: 0.132 0.959\n","epoch 14 - train loss/acc: 0.094 0.974, valid loss/acc: 0.130 0.961\n","Combination 63 - lr: 2.803, n_epochs: 20, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.211 0.941, valid loss/acc: 0.224 0.937\n","epoch 01 - train loss/acc: 0.162 0.951, valid loss/acc: 0.190 0.944\n","epoch 02 - train loss/acc: 0.137 0.960, valid loss/acc: 0.167 0.951\n","epoch 03 - train loss/acc: 0.109 0.968, valid loss/acc: 0.144 0.957\n","epoch 04 - train loss/acc: 0.099 0.971, valid loss/acc: 0.138 0.959\n","epoch 05 - train loss/acc: 0.089 0.974, valid loss/acc: 0.129 0.964\n","epoch 06 - train loss/acc: 0.083 0.975, valid loss/acc: 0.131 0.962\n","epoch 07 - train loss/acc: 0.075 0.978, valid loss/acc: 0.130 0.962\n","epoch 08 - train loss/acc: 0.078 0.977, valid loss/acc: 0.132 0.960\n","epoch 09 - train loss/acc: 0.072 0.978, valid loss/acc: 0.132 0.963\n","epoch 10 - train loss/acc: 0.063 0.982, valid loss/acc: 0.126 0.963\n","epoch 11 - train loss/acc: 0.057 0.983, valid loss/acc: 0.129 0.964\n","epoch 12 - train loss/acc: 0.060 0.982, valid loss/acc: 0.134 0.962\n","epoch 13 - train loss/acc: 0.058 0.982, valid loss/acc: 0.132 0.962\n","epoch 14 - train loss/acc: 0.049 0.985, valid loss/acc: 0.128 0.965\n","epoch 15 - train loss/acc: 0.045 0.988, valid loss/acc: 0.125 0.963\n","epoch 16 - train loss/acc: 0.043 0.988, valid loss/acc: 0.128 0.965\n","epoch 17 - train loss/acc: 0.041 0.989, valid loss/acc: 0.133 0.964\n","epoch 18 - train loss/acc: 0.040 0.989, valid loss/acc: 0.128 0.964\n","epoch 19 - train loss/acc: 0.040 0.989, valid loss/acc: 0.137 0.964\n","Combination 64 - lr: 2.803, n_epochs: 20, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.192 0.943, valid loss/acc: 0.205 0.939\n","epoch 01 - train loss/acc: 0.142 0.958, valid loss/acc: 0.165 0.950\n","epoch 02 - train loss/acc: 0.101 0.970, valid loss/acc: 0.129 0.961\n","epoch 03 - train loss/acc: 0.085 0.975, valid loss/acc: 0.120 0.964\n","epoch 04 - train loss/acc: 0.066 0.982, valid loss/acc: 0.107 0.966\n","epoch 05 - train loss/acc: 0.062 0.983, valid loss/acc: 0.108 0.966\n","epoch 06 - train loss/acc: 0.055 0.984, valid loss/acc: 0.105 0.968\n","epoch 07 - train loss/acc: 0.047 0.987, valid loss/acc: 0.102 0.968\n","epoch 08 - train loss/acc: 0.045 0.988, valid loss/acc: 0.108 0.967\n","epoch 09 - train loss/acc: 0.034 0.992, valid loss/acc: 0.098 0.970\n","epoch 10 - train loss/acc: 0.030 0.993, valid loss/acc: 0.097 0.970\n","epoch 11 - train loss/acc: 0.031 0.992, valid loss/acc: 0.103 0.969\n","epoch 12 - train loss/acc: 0.023 0.996, valid loss/acc: 0.096 0.972\n","epoch 13 - train loss/acc: 0.022 0.996, valid loss/acc: 0.097 0.971\n","epoch 14 - train loss/acc: 0.019 0.997, valid loss/acc: 0.099 0.971\n","epoch 15 - train loss/acc: 0.019 0.996, valid loss/acc: 0.099 0.972\n","epoch 16 - train loss/acc: 0.016 0.998, valid loss/acc: 0.100 0.971\n","epoch 17 - train loss/acc: 0.016 0.997, valid loss/acc: 0.102 0.971\n","epoch 18 - train loss/acc: 0.013 0.998, valid loss/acc: 0.100 0.972\n","epoch 19 - train loss/acc: 0.011 0.999, valid loss/acc: 0.102 0.971\n","Combination 65 - lr: 2.803, n_epochs: 20, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.222 0.936, valid loss/acc: 0.234 0.930\n","epoch 01 - train loss/acc: 0.165 0.951, valid loss/acc: 0.178 0.945\n","epoch 02 - train loss/acc: 0.121 0.965, valid loss/acc: 0.147 0.956\n","epoch 03 - train loss/acc: 0.106 0.969, valid loss/acc: 0.136 0.960\n","epoch 04 - train loss/acc: 0.083 0.976, valid loss/acc: 0.117 0.965\n","epoch 05 - train loss/acc: 0.077 0.978, valid loss/acc: 0.115 0.964\n","epoch 06 - train loss/acc: 0.065 0.980, valid loss/acc: 0.107 0.968\n","epoch 07 - train loss/acc: 0.064 0.981, valid loss/acc: 0.112 0.966\n","epoch 08 - train loss/acc: 0.053 0.985, valid loss/acc: 0.106 0.969\n","epoch 09 - train loss/acc: 0.048 0.987, valid loss/acc: 0.107 0.968\n","epoch 10 - train loss/acc: 0.042 0.989, valid loss/acc: 0.103 0.970\n","epoch 11 - train loss/acc: 0.035 0.992, valid loss/acc: 0.099 0.971\n","epoch 12 - train loss/acc: 0.036 0.991, valid loss/acc: 0.100 0.970\n","epoch 13 - train loss/acc: 0.034 0.992, valid loss/acc: 0.106 0.969\n","epoch 14 - train loss/acc: 0.031 0.992, valid loss/acc: 0.103 0.971\n","epoch 15 - train loss/acc: 0.027 0.994, valid loss/acc: 0.104 0.970\n","epoch 16 - train loss/acc: 0.024 0.995, valid loss/acc: 0.100 0.971\n","epoch 17 - train loss/acc: 0.022 0.996, valid loss/acc: 0.103 0.970\n","epoch 18 - train loss/acc: 0.021 0.995, valid loss/acc: 0.103 0.970\n","epoch 19 - train loss/acc: 0.020 0.996, valid loss/acc: 0.105 0.970\n","Combination 66 - lr: 2.803, n_epochs: 20, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.256 0.926, valid loss/acc: 0.268 0.920\n","epoch 01 - train loss/acc: 0.190 0.945, valid loss/acc: 0.207 0.938\n","epoch 02 - train loss/acc: 0.161 0.952, valid loss/acc: 0.183 0.945\n","epoch 03 - train loss/acc: 0.148 0.956, valid loss/acc: 0.171 0.947\n","epoch 04 - train loss/acc: 0.126 0.964, valid loss/acc: 0.156 0.952\n","epoch 05 - train loss/acc: 0.117 0.966, valid loss/acc: 0.153 0.955\n","epoch 06 - train loss/acc: 0.108 0.968, valid loss/acc: 0.143 0.955\n","epoch 07 - train loss/acc: 0.098 0.972, valid loss/acc: 0.139 0.958\n","epoch 08 - train loss/acc: 0.093 0.973, valid loss/acc: 0.136 0.958\n","epoch 09 - train loss/acc: 0.087 0.976, valid loss/acc: 0.136 0.960\n","epoch 10 - train loss/acc: 0.088 0.975, valid loss/acc: 0.139 0.958\n","epoch 11 - train loss/acc: 0.080 0.976, valid loss/acc: 0.133 0.960\n","epoch 12 - train loss/acc: 0.075 0.979, valid loss/acc: 0.129 0.961\n","epoch 13 - train loss/acc: 0.067 0.981, valid loss/acc: 0.126 0.963\n","epoch 14 - train loss/acc: 0.064 0.982, valid loss/acc: 0.127 0.962\n","epoch 15 - train loss/acc: 0.061 0.983, valid loss/acc: 0.126 0.962\n","epoch 16 - train loss/acc: 0.062 0.983, valid loss/acc: 0.128 0.961\n","epoch 17 - train loss/acc: 0.056 0.985, valid loss/acc: 0.125 0.962\n","epoch 18 - train loss/acc: 0.053 0.986, valid loss/acc: 0.124 0.963\n","epoch 19 - train loss/acc: 0.052 0.985, valid loss/acc: 0.125 0.964\n","Combination 67 - lr: 2.803, n_epochs: 20, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.333 0.894, valid loss/acc: 0.345 0.891\n","epoch 01 - train loss/acc: 0.203 0.940, valid loss/acc: 0.218 0.934\n","epoch 02 - train loss/acc: 0.178 0.948, valid loss/acc: 0.201 0.941\n","epoch 03 - train loss/acc: 0.146 0.958, valid loss/acc: 0.174 0.948\n","epoch 04 - train loss/acc: 0.122 0.965, valid loss/acc: 0.151 0.956\n","epoch 05 - train loss/acc: 0.111 0.968, valid loss/acc: 0.144 0.958\n","epoch 06 - train loss/acc: 0.103 0.969, valid loss/acc: 0.138 0.958\n","epoch 07 - train loss/acc: 0.089 0.975, valid loss/acc: 0.131 0.962\n","epoch 08 - train loss/acc: 0.088 0.974, valid loss/acc: 0.132 0.961\n","epoch 09 - train loss/acc: 0.077 0.978, valid loss/acc: 0.123 0.964\n","epoch 10 - train loss/acc: 0.072 0.980, valid loss/acc: 0.123 0.964\n","epoch 11 - train loss/acc: 0.070 0.980, valid loss/acc: 0.122 0.964\n","epoch 12 - train loss/acc: 0.070 0.980, valid loss/acc: 0.126 0.964\n","epoch 13 - train loss/acc: 0.066 0.981, valid loss/acc: 0.122 0.965\n","epoch 14 - train loss/acc: 0.059 0.983, valid loss/acc: 0.116 0.966\n","epoch 15 - train loss/acc: 0.056 0.985, valid loss/acc: 0.119 0.966\n","epoch 16 - train loss/acc: 0.058 0.984, valid loss/acc: 0.121 0.965\n","epoch 17 - train loss/acc: 0.050 0.987, valid loss/acc: 0.116 0.966\n","epoch 18 - train loss/acc: 0.050 0.987, valid loss/acc: 0.119 0.965\n","epoch 19 - train loss/acc: 0.047 0.988, valid loss/acc: 0.116 0.967\n","Combination 68 - lr: 2.803, n_epochs: 20, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.278 0.919, valid loss/acc: 0.289 0.915\n","epoch 01 - train loss/acc: 0.194 0.944, valid loss/acc: 0.208 0.938\n","epoch 02 - train loss/acc: 0.154 0.956, valid loss/acc: 0.176 0.948\n","epoch 03 - train loss/acc: 0.136 0.961, valid loss/acc: 0.161 0.952\n","epoch 04 - train loss/acc: 0.118 0.967, valid loss/acc: 0.147 0.955\n","epoch 05 - train loss/acc: 0.101 0.971, valid loss/acc: 0.134 0.958\n","epoch 06 - train loss/acc: 0.103 0.970, valid loss/acc: 0.136 0.958\n","epoch 07 - train loss/acc: 0.084 0.977, valid loss/acc: 0.123 0.963\n","epoch 08 - train loss/acc: 0.078 0.979, valid loss/acc: 0.119 0.965\n","epoch 09 - train loss/acc: 0.072 0.980, valid loss/acc: 0.115 0.964\n","epoch 10 - train loss/acc: 0.066 0.981, valid loss/acc: 0.112 0.965\n","epoch 11 - train loss/acc: 0.061 0.983, valid loss/acc: 0.112 0.966\n","epoch 12 - train loss/acc: 0.058 0.984, valid loss/acc: 0.109 0.967\n","epoch 13 - train loss/acc: 0.055 0.986, valid loss/acc: 0.107 0.968\n","epoch 14 - train loss/acc: 0.049 0.988, valid loss/acc: 0.105 0.968\n","epoch 15 - train loss/acc: 0.047 0.987, valid loss/acc: 0.104 0.969\n","epoch 16 - train loss/acc: 0.044 0.989, valid loss/acc: 0.104 0.969\n","epoch 17 - train loss/acc: 0.038 0.991, valid loss/acc: 0.100 0.971\n","epoch 18 - train loss/acc: 0.038 0.991, valid loss/acc: 0.102 0.970\n","epoch 19 - train loss/acc: 0.035 0.992, valid loss/acc: 0.100 0.970\n","Combination 69 - lr: 2.803, n_epochs: 20, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.332 0.905, valid loss/acc: 0.336 0.905\n","epoch 01 - train loss/acc: 0.254 0.926, valid loss/acc: 0.263 0.924\n","epoch 02 - train loss/acc: 0.214 0.939, valid loss/acc: 0.224 0.935\n","epoch 03 - train loss/acc: 0.185 0.947, valid loss/acc: 0.197 0.942\n","epoch 04 - train loss/acc: 0.187 0.946, valid loss/acc: 0.203 0.938\n","epoch 05 - train loss/acc: 0.154 0.957, valid loss/acc: 0.172 0.949\n","epoch 06 - train loss/acc: 0.142 0.960, valid loss/acc: 0.161 0.952\n","epoch 07 - train loss/acc: 0.132 0.963, valid loss/acc: 0.154 0.954\n","epoch 08 - train loss/acc: 0.132 0.962, valid loss/acc: 0.157 0.954\n","epoch 09 - train loss/acc: 0.116 0.968, valid loss/acc: 0.144 0.957\n","epoch 10 - train loss/acc: 0.110 0.969, valid loss/acc: 0.140 0.960\n","epoch 11 - train loss/acc: 0.107 0.970, valid loss/acc: 0.139 0.961\n","epoch 12 - train loss/acc: 0.100 0.973, valid loss/acc: 0.134 0.960\n","epoch 13 - train loss/acc: 0.096 0.973, valid loss/acc: 0.130 0.961\n","epoch 14 - train loss/acc: 0.092 0.975, valid loss/acc: 0.129 0.962\n","epoch 15 - train loss/acc: 0.089 0.975, valid loss/acc: 0.128 0.961\n","epoch 16 - train loss/acc: 0.089 0.975, valid loss/acc: 0.130 0.961\n","epoch 17 - train loss/acc: 0.081 0.978, valid loss/acc: 0.125 0.963\n","epoch 18 - train loss/acc: 0.079 0.978, valid loss/acc: 0.125 0.964\n","epoch 19 - train loss/acc: 0.078 0.978, valid loss/acc: 0.126 0.963\n","Combination 70 - lr: 2.803, n_epochs: 20, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.339 0.900, valid loss/acc: 0.342 0.899\n","epoch 01 - train loss/acc: 0.246 0.928, valid loss/acc: 0.251 0.926\n","epoch 02 - train loss/acc: 0.203 0.941, valid loss/acc: 0.214 0.937\n","epoch 03 - train loss/acc: 0.179 0.949, valid loss/acc: 0.192 0.944\n","epoch 04 - train loss/acc: 0.160 0.954, valid loss/acc: 0.176 0.948\n","epoch 05 - train loss/acc: 0.145 0.959, valid loss/acc: 0.163 0.953\n","epoch 06 - train loss/acc: 0.126 0.964, valid loss/acc: 0.148 0.956\n","epoch 07 - train loss/acc: 0.116 0.967, valid loss/acc: 0.140 0.959\n","epoch 08 - train loss/acc: 0.106 0.971, valid loss/acc: 0.131 0.959\n","epoch 09 - train loss/acc: 0.101 0.972, valid loss/acc: 0.129 0.960\n","epoch 10 - train loss/acc: 0.093 0.974, valid loss/acc: 0.124 0.963\n","epoch 11 - train loss/acc: 0.085 0.976, valid loss/acc: 0.117 0.964\n","epoch 12 - train loss/acc: 0.082 0.977, valid loss/acc: 0.117 0.963\n","epoch 13 - train loss/acc: 0.078 0.978, valid loss/acc: 0.113 0.965\n","epoch 14 - train loss/acc: 0.074 0.980, valid loss/acc: 0.112 0.966\n","epoch 15 - train loss/acc: 0.070 0.981, valid loss/acc: 0.109 0.967\n","epoch 16 - train loss/acc: 0.065 0.983, valid loss/acc: 0.107 0.969\n","epoch 17 - train loss/acc: 0.062 0.984, valid loss/acc: 0.104 0.967\n","epoch 18 - train loss/acc: 0.060 0.984, valid loss/acc: 0.104 0.969\n","epoch 19 - train loss/acc: 0.056 0.985, valid loss/acc: 0.100 0.969\n","Combination 71 - lr: 2.803, n_epochs: 20, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.358 0.898, valid loss/acc: 0.364 0.891\n","epoch 01 - train loss/acc: 0.284 0.913, valid loss/acc: 0.291 0.910\n","epoch 02 - train loss/acc: 0.216 0.937, valid loss/acc: 0.227 0.932\n","epoch 03 - train loss/acc: 0.190 0.944, valid loss/acc: 0.206 0.939\n","epoch 04 - train loss/acc: 0.169 0.950, valid loss/acc: 0.186 0.944\n","epoch 05 - train loss/acc: 0.150 0.956, valid loss/acc: 0.171 0.949\n","epoch 06 - train loss/acc: 0.136 0.961, valid loss/acc: 0.160 0.952\n","epoch 07 - train loss/acc: 0.129 0.962, valid loss/acc: 0.155 0.954\n","epoch 08 - train loss/acc: 0.115 0.968, valid loss/acc: 0.143 0.957\n","epoch 09 - train loss/acc: 0.109 0.969, valid loss/acc: 0.139 0.959\n","epoch 10 - train loss/acc: 0.099 0.972, valid loss/acc: 0.131 0.962\n","epoch 11 - train loss/acc: 0.093 0.974, valid loss/acc: 0.127 0.962\n","epoch 12 - train loss/acc: 0.090 0.975, valid loss/acc: 0.124 0.963\n","epoch 13 - train loss/acc: 0.084 0.977, valid loss/acc: 0.122 0.963\n","epoch 14 - train loss/acc: 0.078 0.978, valid loss/acc: 0.116 0.965\n","epoch 15 - train loss/acc: 0.075 0.979, valid loss/acc: 0.114 0.966\n","epoch 16 - train loss/acc: 0.071 0.980, valid loss/acc: 0.114 0.965\n","epoch 17 - train loss/acc: 0.068 0.981, valid loss/acc: 0.110 0.968\n","epoch 18 - train loss/acc: 0.064 0.983, valid loss/acc: 0.106 0.968\n","epoch 19 - train loss/acc: 0.062 0.983, valid loss/acc: 0.107 0.967\n","Combination 72 - lr: 2.803, n_epochs: 25, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.211 0.939, valid loss/acc: 0.230 0.931\n","epoch 01 - train loss/acc: 0.152 0.957, valid loss/acc: 0.178 0.948\n","epoch 02 - train loss/acc: 0.130 0.961, valid loss/acc: 0.161 0.951\n","epoch 03 - train loss/acc: 0.117 0.965, valid loss/acc: 0.152 0.954\n","epoch 04 - train loss/acc: 0.092 0.973, valid loss/acc: 0.135 0.958\n","epoch 05 - train loss/acc: 0.092 0.973, valid loss/acc: 0.140 0.957\n","epoch 06 - train loss/acc: 0.080 0.976, valid loss/acc: 0.133 0.960\n","epoch 07 - train loss/acc: 0.072 0.979, valid loss/acc: 0.130 0.960\n","epoch 08 - train loss/acc: 0.066 0.981, valid loss/acc: 0.124 0.963\n","epoch 09 - train loss/acc: 0.064 0.980, valid loss/acc: 0.128 0.961\n","epoch 10 - train loss/acc: 0.056 0.984, valid loss/acc: 0.125 0.964\n","epoch 11 - train loss/acc: 0.055 0.984, valid loss/acc: 0.123 0.963\n","epoch 12 - train loss/acc: 0.051 0.985, valid loss/acc: 0.130 0.962\n","epoch 13 - train loss/acc: 0.049 0.986, valid loss/acc: 0.127 0.963\n","epoch 14 - train loss/acc: 0.048 0.986, valid loss/acc: 0.126 0.963\n","epoch 15 - train loss/acc: 0.050 0.984, valid loss/acc: 0.136 0.961\n","epoch 16 - train loss/acc: 0.041 0.988, valid loss/acc: 0.129 0.964\n","epoch 17 - train loss/acc: 0.039 0.989, valid loss/acc: 0.132 0.962\n","epoch 18 - train loss/acc: 0.038 0.989, valid loss/acc: 0.132 0.963\n","epoch 19 - train loss/acc: 0.033 0.991, valid loss/acc: 0.130 0.963\n","epoch 20 - train loss/acc: 0.034 0.991, valid loss/acc: 0.137 0.962\n","epoch 21 - train loss/acc: 0.031 0.992, valid loss/acc: 0.139 0.963\n","epoch 22 - train loss/acc: 0.031 0.992, valid loss/acc: 0.132 0.963\n","epoch 23 - train loss/acc: 0.036 0.989, valid loss/acc: 0.147 0.960\n","epoch 24 - train loss/acc: 0.026 0.994, valid loss/acc: 0.135 0.963\n","Combination 73 - lr: 2.803, n_epochs: 25, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.208 0.938, valid loss/acc: 0.219 0.933\n","epoch 01 - train loss/acc: 0.149 0.956, valid loss/acc: 0.174 0.947\n","epoch 02 - train loss/acc: 0.125 0.962, valid loss/acc: 0.154 0.954\n","epoch 03 - train loss/acc: 0.110 0.967, valid loss/acc: 0.143 0.959\n","epoch 04 - train loss/acc: 0.076 0.978, valid loss/acc: 0.112 0.966\n","epoch 05 - train loss/acc: 0.067 0.980, valid loss/acc: 0.115 0.966\n","epoch 06 - train loss/acc: 0.064 0.982, valid loss/acc: 0.115 0.966\n","epoch 07 - train loss/acc: 0.053 0.985, valid loss/acc: 0.106 0.967\n","epoch 08 - train loss/acc: 0.051 0.985, valid loss/acc: 0.114 0.965\n","epoch 09 - train loss/acc: 0.041 0.990, valid loss/acc: 0.104 0.969\n","epoch 10 - train loss/acc: 0.037 0.991, valid loss/acc: 0.104 0.969\n","epoch 11 - train loss/acc: 0.035 0.991, valid loss/acc: 0.104 0.969\n","epoch 12 - train loss/acc: 0.029 0.993, valid loss/acc: 0.102 0.970\n","epoch 13 - train loss/acc: 0.029 0.993, valid loss/acc: 0.105 0.969\n","epoch 14 - train loss/acc: 0.030 0.992, valid loss/acc: 0.107 0.969\n","epoch 15 - train loss/acc: 0.023 0.995, valid loss/acc: 0.102 0.969\n","epoch 16 - train loss/acc: 0.020 0.996, valid loss/acc: 0.103 0.969\n","epoch 17 - train loss/acc: 0.020 0.996, valid loss/acc: 0.105 0.970\n","epoch 18 - train loss/acc: 0.020 0.996, valid loss/acc: 0.110 0.968\n","epoch 19 - train loss/acc: 0.017 0.997, valid loss/acc: 0.107 0.970\n","epoch 20 - train loss/acc: 0.016 0.997, valid loss/acc: 0.105 0.971\n","epoch 21 - train loss/acc: 0.014 0.998, valid loss/acc: 0.105 0.970\n","epoch 22 - train loss/acc: 0.012 0.999, valid loss/acc: 0.107 0.970\n","epoch 23 - train loss/acc: 0.012 0.999, valid loss/acc: 0.108 0.970\n","epoch 24 - train loss/acc: 0.012 0.998, valid loss/acc: 0.109 0.969\n","Combination 74 - lr: 2.803, n_epochs: 25, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.208 0.937, valid loss/acc: 0.218 0.934\n","epoch 01 - train loss/acc: 0.143 0.958, valid loss/acc: 0.165 0.950\n","epoch 02 - train loss/acc: 0.108 0.968, valid loss/acc: 0.135 0.960\n","epoch 03 - train loss/acc: 0.091 0.974, valid loss/acc: 0.122 0.962\n","epoch 04 - train loss/acc: 0.081 0.976, valid loss/acc: 0.119 0.964\n","epoch 05 - train loss/acc: 0.064 0.982, valid loss/acc: 0.108 0.967\n","epoch 06 - train loss/acc: 0.061 0.983, valid loss/acc: 0.107 0.967\n","epoch 07 - train loss/acc: 0.058 0.983, valid loss/acc: 0.106 0.967\n","epoch 08 - train loss/acc: 0.048 0.987, valid loss/acc: 0.106 0.968\n","epoch 09 - train loss/acc: 0.040 0.990, valid loss/acc: 0.096 0.971\n","epoch 10 - train loss/acc: 0.035 0.991, valid loss/acc: 0.100 0.971\n","epoch 11 - train loss/acc: 0.031 0.993, valid loss/acc: 0.096 0.971\n","epoch 12 - train loss/acc: 0.032 0.992, valid loss/acc: 0.098 0.972\n","epoch 13 - train loss/acc: 0.023 0.995, valid loss/acc: 0.094 0.973\n","epoch 14 - train loss/acc: 0.021 0.996, valid loss/acc: 0.092 0.973\n","epoch 15 - train loss/acc: 0.020 0.996, valid loss/acc: 0.096 0.972\n","epoch 16 - train loss/acc: 0.018 0.997, valid loss/acc: 0.095 0.972\n","epoch 17 - train loss/acc: 0.015 0.998, valid loss/acc: 0.096 0.972\n","epoch 18 - train loss/acc: 0.014 0.998, valid loss/acc: 0.096 0.972\n","epoch 19 - train loss/acc: 0.013 0.998, valid loss/acc: 0.096 0.973\n","epoch 20 - train loss/acc: 0.011 0.999, valid loss/acc: 0.096 0.974\n","epoch 21 - train loss/acc: 0.011 0.999, valid loss/acc: 0.096 0.973\n","epoch 22 - train loss/acc: 0.009 0.999, valid loss/acc: 0.098 0.972\n","epoch 23 - train loss/acc: 0.008 0.999, valid loss/acc: 0.098 0.973\n","epoch 24 - train loss/acc: 0.008 1.000, valid loss/acc: 0.099 0.973\n","Combination 75 - lr: 2.803, n_epochs: 25, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.274 0.920, valid loss/acc: 0.280 0.917\n","epoch 01 - train loss/acc: 0.202 0.942, valid loss/acc: 0.216 0.936\n","epoch 02 - train loss/acc: 0.160 0.955, valid loss/acc: 0.178 0.947\n","epoch 03 - train loss/acc: 0.144 0.959, valid loss/acc: 0.164 0.952\n","epoch 04 - train loss/acc: 0.123 0.966, valid loss/acc: 0.148 0.956\n","epoch 05 - train loss/acc: 0.110 0.969, valid loss/acc: 0.141 0.959\n","epoch 06 - train loss/acc: 0.100 0.972, valid loss/acc: 0.132 0.962\n","epoch 07 - train loss/acc: 0.094 0.974, valid loss/acc: 0.130 0.961\n","epoch 08 - train loss/acc: 0.085 0.976, valid loss/acc: 0.123 0.963\n","epoch 09 - train loss/acc: 0.080 0.978, valid loss/acc: 0.120 0.966\n","epoch 10 - train loss/acc: 0.078 0.978, valid loss/acc: 0.125 0.963\n","epoch 11 - train loss/acc: 0.070 0.981, valid loss/acc: 0.117 0.967\n","epoch 12 - train loss/acc: 0.067 0.981, valid loss/acc: 0.119 0.965\n","epoch 13 - train loss/acc: 0.064 0.983, valid loss/acc: 0.118 0.965\n","epoch 14 - train loss/acc: 0.066 0.982, valid loss/acc: 0.123 0.965\n","epoch 15 - train loss/acc: 0.059 0.984, valid loss/acc: 0.120 0.966\n","epoch 16 - train loss/acc: 0.059 0.984, valid loss/acc: 0.121 0.965\n","epoch 17 - train loss/acc: 0.059 0.983, valid loss/acc: 0.125 0.964\n","epoch 18 - train loss/acc: 0.051 0.987, valid loss/acc: 0.119 0.965\n","epoch 19 - train loss/acc: 0.047 0.988, valid loss/acc: 0.117 0.966\n","epoch 20 - train loss/acc: 0.049 0.987, valid loss/acc: 0.123 0.964\n","epoch 21 - train loss/acc: 0.049 0.987, valid loss/acc: 0.124 0.964\n","epoch 22 - train loss/acc: 0.042 0.990, valid loss/acc: 0.120 0.966\n","epoch 23 - train loss/acc: 0.043 0.989, valid loss/acc: 0.121 0.966\n","epoch 24 - train loss/acc: 0.040 0.990, valid loss/acc: 0.120 0.965\n","Combination 76 - lr: 2.803, n_epochs: 25, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.266 0.919, valid loss/acc: 0.278 0.917\n","epoch 01 - train loss/acc: 0.189 0.945, valid loss/acc: 0.205 0.939\n","epoch 02 - train loss/acc: 0.154 0.955, valid loss/acc: 0.172 0.949\n","epoch 03 - train loss/acc: 0.128 0.963, valid loss/acc: 0.153 0.955\n","epoch 04 - train loss/acc: 0.107 0.970, valid loss/acc: 0.135 0.960\n","epoch 05 - train loss/acc: 0.093 0.973, valid loss/acc: 0.125 0.963\n","epoch 06 - train loss/acc: 0.083 0.977, valid loss/acc: 0.119 0.966\n","epoch 07 - train loss/acc: 0.073 0.980, valid loss/acc: 0.111 0.965\n","epoch 08 - train loss/acc: 0.068 0.981, valid loss/acc: 0.109 0.967\n","epoch 09 - train loss/acc: 0.070 0.980, valid loss/acc: 0.115 0.965\n","epoch 10 - train loss/acc: 0.058 0.984, valid loss/acc: 0.104 0.969\n","epoch 11 - train loss/acc: 0.055 0.985, valid loss/acc: 0.105 0.970\n","epoch 12 - train loss/acc: 0.053 0.985, valid loss/acc: 0.105 0.968\n","epoch 13 - train loss/acc: 0.044 0.989, valid loss/acc: 0.098 0.970\n","epoch 14 - train loss/acc: 0.040 0.990, valid loss/acc: 0.095 0.970\n","epoch 15 - train loss/acc: 0.039 0.990, valid loss/acc: 0.098 0.970\n","epoch 16 - train loss/acc: 0.034 0.992, valid loss/acc: 0.097 0.970\n","epoch 17 - train loss/acc: 0.034 0.992, valid loss/acc: 0.098 0.971\n","epoch 18 - train loss/acc: 0.029 0.994, valid loss/acc: 0.094 0.972\n","epoch 19 - train loss/acc: 0.029 0.994, valid loss/acc: 0.094 0.972\n","epoch 20 - train loss/acc: 0.025 0.996, valid loss/acc: 0.094 0.972\n","epoch 21 - train loss/acc: 0.024 0.996, valid loss/acc: 0.094 0.972\n","epoch 22 - train loss/acc: 0.022 0.997, valid loss/acc: 0.093 0.973\n","epoch 23 - train loss/acc: 0.020 0.997, valid loss/acc: 0.094 0.972\n","epoch 24 - train loss/acc: 0.019 0.997, valid loss/acc: 0.094 0.973\n","Combination 77 - lr: 2.803, n_epochs: 25, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.272 0.921, valid loss/acc: 0.283 0.916\n","epoch 01 - train loss/acc: 0.202 0.942, valid loss/acc: 0.220 0.935\n","epoch 02 - train loss/acc: 0.155 0.955, valid loss/acc: 0.176 0.949\n","epoch 03 - train loss/acc: 0.136 0.961, valid loss/acc: 0.161 0.952\n","epoch 04 - train loss/acc: 0.112 0.968, valid loss/acc: 0.138 0.959\n","epoch 05 - train loss/acc: 0.099 0.972, valid loss/acc: 0.131 0.961\n","epoch 06 - train loss/acc: 0.090 0.975, valid loss/acc: 0.125 0.962\n","epoch 07 - train loss/acc: 0.086 0.975, valid loss/acc: 0.123 0.964\n","epoch 08 - train loss/acc: 0.075 0.979, valid loss/acc: 0.116 0.964\n","epoch 09 - train loss/acc: 0.066 0.981, valid loss/acc: 0.113 0.966\n","epoch 10 - train loss/acc: 0.063 0.983, valid loss/acc: 0.108 0.968\n","epoch 11 - train loss/acc: 0.059 0.984, valid loss/acc: 0.107 0.968\n","epoch 12 - train loss/acc: 0.053 0.985, valid loss/acc: 0.104 0.969\n","epoch 13 - train loss/acc: 0.048 0.987, valid loss/acc: 0.102 0.970\n","epoch 14 - train loss/acc: 0.047 0.987, valid loss/acc: 0.102 0.970\n","epoch 15 - train loss/acc: 0.042 0.989, valid loss/acc: 0.100 0.970\n","epoch 16 - train loss/acc: 0.040 0.990, valid loss/acc: 0.100 0.970\n","epoch 17 - train loss/acc: 0.036 0.991, valid loss/acc: 0.097 0.971\n","epoch 18 - train loss/acc: 0.034 0.993, valid loss/acc: 0.097 0.971\n","epoch 19 - train loss/acc: 0.033 0.992, valid loss/acc: 0.100 0.970\n","epoch 20 - train loss/acc: 0.032 0.992, valid loss/acc: 0.099 0.971\n","epoch 21 - train loss/acc: 0.026 0.995, valid loss/acc: 0.095 0.972\n","epoch 22 - train loss/acc: 0.025 0.995, valid loss/acc: 0.097 0.972\n","epoch 23 - train loss/acc: 0.024 0.995, valid loss/acc: 0.095 0.972\n","epoch 24 - train loss/acc: 0.022 0.996, valid loss/acc: 0.094 0.972\n","Combination 78 - lr: 2.803, n_epochs: 25, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.330 0.903, valid loss/acc: 0.335 0.900\n","epoch 01 - train loss/acc: 0.244 0.931, valid loss/acc: 0.252 0.928\n","epoch 02 - train loss/acc: 0.207 0.941, valid loss/acc: 0.218 0.937\n","epoch 03 - train loss/acc: 0.183 0.948, valid loss/acc: 0.199 0.943\n","epoch 04 - train loss/acc: 0.170 0.951, valid loss/acc: 0.187 0.945\n","epoch 05 - train loss/acc: 0.150 0.957, valid loss/acc: 0.171 0.951\n","epoch 06 - train loss/acc: 0.138 0.961, valid loss/acc: 0.161 0.954\n","epoch 07 - train loss/acc: 0.130 0.963, valid loss/acc: 0.155 0.955\n","epoch 08 - train loss/acc: 0.123 0.965, valid loss/acc: 0.149 0.957\n","epoch 09 - train loss/acc: 0.119 0.966, valid loss/acc: 0.148 0.956\n","epoch 10 - train loss/acc: 0.109 0.970, valid loss/acc: 0.140 0.959\n","epoch 11 - train loss/acc: 0.105 0.970, valid loss/acc: 0.139 0.958\n","epoch 12 - train loss/acc: 0.101 0.971, valid loss/acc: 0.134 0.959\n","epoch 13 - train loss/acc: 0.095 0.973, valid loss/acc: 0.132 0.960\n","epoch 14 - train loss/acc: 0.092 0.974, valid loss/acc: 0.131 0.960\n","epoch 15 - train loss/acc: 0.088 0.976, valid loss/acc: 0.129 0.960\n","epoch 16 - train loss/acc: 0.085 0.977, valid loss/acc: 0.127 0.960\n","epoch 17 - train loss/acc: 0.082 0.977, valid loss/acc: 0.126 0.960\n","epoch 18 - train loss/acc: 0.079 0.978, valid loss/acc: 0.124 0.961\n","epoch 19 - train loss/acc: 0.075 0.979, valid loss/acc: 0.121 0.963\n","epoch 20 - train loss/acc: 0.073 0.980, valid loss/acc: 0.120 0.963\n","epoch 21 - train loss/acc: 0.072 0.980, valid loss/acc: 0.122 0.963\n","epoch 22 - train loss/acc: 0.069 0.981, valid loss/acc: 0.121 0.963\n","epoch 23 - train loss/acc: 0.070 0.980, valid loss/acc: 0.122 0.962\n","epoch 24 - train loss/acc: 0.064 0.983, valid loss/acc: 0.118 0.964\n","Combination 79 - lr: 2.803, n_epochs: 25, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.319 0.908, valid loss/acc: 0.327 0.903\n","epoch 01 - train loss/acc: 0.251 0.927, valid loss/acc: 0.259 0.924\n","epoch 02 - train loss/acc: 0.219 0.936, valid loss/acc: 0.230 0.930\n","epoch 03 - train loss/acc: 0.179 0.948, valid loss/acc: 0.193 0.942\n","epoch 04 - train loss/acc: 0.159 0.954, valid loss/acc: 0.176 0.948\n","epoch 05 - train loss/acc: 0.141 0.960, valid loss/acc: 0.160 0.953\n","epoch 06 - train loss/acc: 0.130 0.963, valid loss/acc: 0.150 0.956\n","epoch 07 - train loss/acc: 0.118 0.966, valid loss/acc: 0.141 0.959\n","epoch 08 - train loss/acc: 0.107 0.970, valid loss/acc: 0.133 0.961\n","epoch 09 - train loss/acc: 0.098 0.973, valid loss/acc: 0.125 0.963\n","epoch 10 - train loss/acc: 0.093 0.974, valid loss/acc: 0.123 0.964\n","epoch 11 - train loss/acc: 0.085 0.977, valid loss/acc: 0.117 0.965\n","epoch 12 - train loss/acc: 0.082 0.977, valid loss/acc: 0.118 0.965\n","epoch 13 - train loss/acc: 0.076 0.980, valid loss/acc: 0.111 0.967\n","epoch 14 - train loss/acc: 0.073 0.980, valid loss/acc: 0.110 0.967\n","epoch 15 - train loss/acc: 0.071 0.981, valid loss/acc: 0.108 0.967\n","epoch 16 - train loss/acc: 0.063 0.984, valid loss/acc: 0.103 0.970\n","epoch 17 - train loss/acc: 0.062 0.984, valid loss/acc: 0.104 0.969\n","epoch 18 - train loss/acc: 0.061 0.984, valid loss/acc: 0.103 0.970\n","epoch 19 - train loss/acc: 0.055 0.986, valid loss/acc: 0.099 0.971\n","epoch 20 - train loss/acc: 0.053 0.986, valid loss/acc: 0.099 0.970\n","epoch 21 - train loss/acc: 0.051 0.987, valid loss/acc: 0.098 0.971\n","epoch 22 - train loss/acc: 0.048 0.988, valid loss/acc: 0.096 0.971\n","epoch 23 - train loss/acc: 0.046 0.989, valid loss/acc: 0.095 0.972\n","epoch 24 - train loss/acc: 0.045 0.989, valid loss/acc: 0.096 0.971\n","Combination 80 - lr: 2.803, n_epochs: 25, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.372 0.900, valid loss/acc: 0.377 0.895\n","epoch 01 - train loss/acc: 0.259 0.925, valid loss/acc: 0.266 0.922\n","epoch 02 - train loss/acc: 0.221 0.935, valid loss/acc: 0.231 0.931\n","epoch 03 - train loss/acc: 0.191 0.944, valid loss/acc: 0.203 0.939\n","epoch 04 - train loss/acc: 0.170 0.949, valid loss/acc: 0.186 0.946\n","epoch 05 - train loss/acc: 0.152 0.956, valid loss/acc: 0.172 0.949\n","epoch 06 - train loss/acc: 0.141 0.960, valid loss/acc: 0.162 0.953\n","epoch 07 - train loss/acc: 0.130 0.963, valid loss/acc: 0.153 0.956\n","epoch 08 - train loss/acc: 0.122 0.965, valid loss/acc: 0.148 0.958\n","epoch 09 - train loss/acc: 0.113 0.967, valid loss/acc: 0.140 0.959\n","epoch 10 - train loss/acc: 0.104 0.971, valid loss/acc: 0.135 0.960\n","epoch 11 - train loss/acc: 0.099 0.972, valid loss/acc: 0.132 0.962\n","epoch 12 - train loss/acc: 0.091 0.974, valid loss/acc: 0.125 0.965\n","epoch 13 - train loss/acc: 0.086 0.976, valid loss/acc: 0.123 0.964\n","epoch 14 - train loss/acc: 0.083 0.977, valid loss/acc: 0.120 0.965\n","epoch 15 - train loss/acc: 0.079 0.978, valid loss/acc: 0.119 0.965\n","epoch 16 - train loss/acc: 0.075 0.979, valid loss/acc: 0.116 0.966\n","epoch 17 - train loss/acc: 0.072 0.980, valid loss/acc: 0.116 0.965\n","epoch 18 - train loss/acc: 0.067 0.982, valid loss/acc: 0.112 0.968\n","epoch 19 - train loss/acc: 0.065 0.982, valid loss/acc: 0.110 0.968\n","epoch 20 - train loss/acc: 0.061 0.984, valid loss/acc: 0.109 0.969\n","epoch 21 - train loss/acc: 0.061 0.984, valid loss/acc: 0.109 0.968\n","epoch 22 - train loss/acc: 0.058 0.984, valid loss/acc: 0.108 0.969\n","epoch 23 - train loss/acc: 0.055 0.985, valid loss/acc: 0.106 0.968\n","epoch 24 - train loss/acc: 0.052 0.987, valid loss/acc: 0.106 0.968\n","Combination 81 - lr: 1.590, n_epochs: 15, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.237 0.932, valid loss/acc: 0.250 0.926\n","epoch 01 - train loss/acc: 0.177 0.949, valid loss/acc: 0.195 0.944\n","epoch 02 - train loss/acc: 0.147 0.957, valid loss/acc: 0.174 0.950\n","epoch 03 - train loss/acc: 0.132 0.961, valid loss/acc: 0.163 0.952\n","epoch 04 - train loss/acc: 0.122 0.964, valid loss/acc: 0.157 0.953\n","epoch 05 - train loss/acc: 0.106 0.970, valid loss/acc: 0.149 0.956\n","epoch 06 - train loss/acc: 0.099 0.972, valid loss/acc: 0.142 0.958\n","epoch 07 - train loss/acc: 0.089 0.975, valid loss/acc: 0.140 0.958\n","epoch 08 - train loss/acc: 0.090 0.974, valid loss/acc: 0.142 0.958\n","epoch 09 - train loss/acc: 0.076 0.979, valid loss/acc: 0.133 0.962\n","epoch 10 - train loss/acc: 0.081 0.976, valid loss/acc: 0.140 0.959\n","epoch 11 - train loss/acc: 0.070 0.980, valid loss/acc: 0.133 0.961\n","epoch 12 - train loss/acc: 0.066 0.982, valid loss/acc: 0.132 0.961\n","epoch 13 - train loss/acc: 0.062 0.983, valid loss/acc: 0.129 0.962\n","epoch 14 - train loss/acc: 0.060 0.984, valid loss/acc: 0.131 0.962\n","Combination 82 - lr: 1.590, n_epochs: 15, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.238 0.928, valid loss/acc: 0.246 0.926\n","epoch 01 - train loss/acc: 0.173 0.948, valid loss/acc: 0.191 0.944\n","epoch 02 - train loss/acc: 0.131 0.963, valid loss/acc: 0.150 0.955\n","epoch 03 - train loss/acc: 0.107 0.970, valid loss/acc: 0.133 0.961\n","epoch 04 - train loss/acc: 0.096 0.973, valid loss/acc: 0.123 0.962\n","epoch 05 - train loss/acc: 0.078 0.978, valid loss/acc: 0.111 0.967\n","epoch 06 - train loss/acc: 0.072 0.980, valid loss/acc: 0.107 0.967\n","epoch 07 - train loss/acc: 0.065 0.982, valid loss/acc: 0.103 0.969\n","epoch 08 - train loss/acc: 0.058 0.984, valid loss/acc: 0.101 0.969\n","epoch 09 - train loss/acc: 0.051 0.986, valid loss/acc: 0.098 0.971\n","epoch 10 - train loss/acc: 0.047 0.988, valid loss/acc: 0.096 0.971\n","epoch 11 - train loss/acc: 0.040 0.990, valid loss/acc: 0.092 0.972\n","epoch 12 - train loss/acc: 0.038 0.990, valid loss/acc: 0.092 0.972\n","epoch 13 - train loss/acc: 0.034 0.993, valid loss/acc: 0.091 0.972\n","epoch 14 - train loss/acc: 0.032 0.992, valid loss/acc: 0.093 0.973\n","Combination 83 - lr: 1.590, n_epochs: 15, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.238 0.931, valid loss/acc: 0.245 0.927\n","epoch 01 - train loss/acc: 0.182 0.947, valid loss/acc: 0.197 0.941\n","epoch 02 - train loss/acc: 0.143 0.959, valid loss/acc: 0.161 0.951\n","epoch 03 - train loss/acc: 0.120 0.965, valid loss/acc: 0.144 0.958\n","epoch 04 - train loss/acc: 0.102 0.971, valid loss/acc: 0.129 0.962\n","epoch 05 - train loss/acc: 0.090 0.975, valid loss/acc: 0.122 0.965\n","epoch 06 - train loss/acc: 0.078 0.978, valid loss/acc: 0.116 0.965\n","epoch 07 - train loss/acc: 0.072 0.980, valid loss/acc: 0.113 0.966\n","epoch 08 - train loss/acc: 0.063 0.983, valid loss/acc: 0.105 0.968\n","epoch 09 - train loss/acc: 0.055 0.985, valid loss/acc: 0.100 0.972\n","epoch 10 - train loss/acc: 0.052 0.986, valid loss/acc: 0.102 0.969\n","epoch 11 - train loss/acc: 0.047 0.988, valid loss/acc: 0.096 0.971\n","epoch 12 - train loss/acc: 0.044 0.989, valid loss/acc: 0.096 0.971\n","epoch 13 - train loss/acc: 0.040 0.989, valid loss/acc: 0.095 0.971\n","epoch 14 - train loss/acc: 0.036 0.991, valid loss/acc: 0.096 0.971\n","Combination 84 - lr: 1.590, n_epochs: 15, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.304 0.912, valid loss/acc: 0.314 0.907\n","epoch 01 - train loss/acc: 0.230 0.934, valid loss/acc: 0.241 0.929\n","epoch 02 - train loss/acc: 0.195 0.944, valid loss/acc: 0.210 0.937\n","epoch 03 - train loss/acc: 0.177 0.950, valid loss/acc: 0.198 0.943\n","epoch 04 - train loss/acc: 0.157 0.955, valid loss/acc: 0.182 0.947\n","epoch 05 - train loss/acc: 0.139 0.960, valid loss/acc: 0.166 0.952\n","epoch 06 - train loss/acc: 0.131 0.963, valid loss/acc: 0.158 0.953\n","epoch 07 - train loss/acc: 0.125 0.965, valid loss/acc: 0.156 0.956\n","epoch 08 - train loss/acc: 0.113 0.968, valid loss/acc: 0.147 0.957\n","epoch 09 - train loss/acc: 0.108 0.969, valid loss/acc: 0.144 0.958\n","epoch 10 - train loss/acc: 0.100 0.972, valid loss/acc: 0.137 0.959\n","epoch 11 - train loss/acc: 0.098 0.973, valid loss/acc: 0.137 0.958\n","epoch 12 - train loss/acc: 0.091 0.975, valid loss/acc: 0.131 0.960\n","epoch 13 - train loss/acc: 0.086 0.977, valid loss/acc: 0.130 0.960\n","epoch 14 - train loss/acc: 0.085 0.977, valid loss/acc: 0.131 0.959\n","Combination 85 - lr: 1.590, n_epochs: 15, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.309 0.910, valid loss/acc: 0.318 0.906\n","epoch 01 - train loss/acc: 0.228 0.933, valid loss/acc: 0.238 0.931\n","epoch 02 - train loss/acc: 0.190 0.945, valid loss/acc: 0.201 0.941\n","epoch 03 - train loss/acc: 0.163 0.953, valid loss/acc: 0.179 0.947\n","epoch 04 - train loss/acc: 0.141 0.959, valid loss/acc: 0.159 0.953\n","epoch 05 - train loss/acc: 0.128 0.964, valid loss/acc: 0.151 0.956\n","epoch 06 - train loss/acc: 0.118 0.967, valid loss/acc: 0.142 0.958\n","epoch 07 - train loss/acc: 0.107 0.970, valid loss/acc: 0.135 0.958\n","epoch 08 - train loss/acc: 0.097 0.973, valid loss/acc: 0.128 0.961\n","epoch 09 - train loss/acc: 0.090 0.976, valid loss/acc: 0.121 0.964\n","epoch 10 - train loss/acc: 0.082 0.977, valid loss/acc: 0.116 0.965\n","epoch 11 - train loss/acc: 0.078 0.979, valid loss/acc: 0.112 0.967\n","epoch 12 - train loss/acc: 0.071 0.981, valid loss/acc: 0.107 0.968\n","epoch 13 - train loss/acc: 0.066 0.982, valid loss/acc: 0.105 0.968\n","epoch 14 - train loss/acc: 0.063 0.984, valid loss/acc: 0.103 0.969\n","Combination 86 - lr: 1.590, n_epochs: 15, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.309 0.909, valid loss/acc: 0.315 0.907\n","epoch 01 - train loss/acc: 0.233 0.932, valid loss/acc: 0.244 0.926\n","epoch 02 - train loss/acc: 0.191 0.946, valid loss/acc: 0.203 0.941\n","epoch 03 - train loss/acc: 0.162 0.954, valid loss/acc: 0.178 0.947\n","epoch 04 - train loss/acc: 0.143 0.958, valid loss/acc: 0.161 0.952\n","epoch 05 - train loss/acc: 0.131 0.962, valid loss/acc: 0.152 0.954\n","epoch 06 - train loss/acc: 0.113 0.968, valid loss/acc: 0.139 0.958\n","epoch 07 - train loss/acc: 0.102 0.972, valid loss/acc: 0.129 0.961\n","epoch 08 - train loss/acc: 0.092 0.974, valid loss/acc: 0.121 0.964\n","epoch 09 - train loss/acc: 0.087 0.975, valid loss/acc: 0.118 0.964\n","epoch 10 - train loss/acc: 0.079 0.978, valid loss/acc: 0.113 0.966\n","epoch 11 - train loss/acc: 0.073 0.979, valid loss/acc: 0.109 0.967\n","epoch 12 - train loss/acc: 0.068 0.981, valid loss/acc: 0.106 0.968\n","epoch 13 - train loss/acc: 0.066 0.983, valid loss/acc: 0.107 0.968\n","epoch 14 - train loss/acc: 0.060 0.983, valid loss/acc: 0.102 0.969\n","Combination 87 - lr: 1.590, n_epochs: 15, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.403 0.889, valid loss/acc: 0.407 0.887\n","epoch 01 - train loss/acc: 0.294 0.918, valid loss/acc: 0.300 0.917\n","epoch 02 - train loss/acc: 0.265 0.925, valid loss/acc: 0.273 0.919\n","epoch 03 - train loss/acc: 0.233 0.934, valid loss/acc: 0.243 0.930\n","epoch 04 - train loss/acc: 0.212 0.940, valid loss/acc: 0.223 0.936\n","epoch 05 - train loss/acc: 0.201 0.942, valid loss/acc: 0.213 0.939\n","epoch 06 - train loss/acc: 0.186 0.947, valid loss/acc: 0.200 0.941\n","epoch 07 - train loss/acc: 0.187 0.945, valid loss/acc: 0.202 0.940\n","epoch 08 - train loss/acc: 0.172 0.951, valid loss/acc: 0.188 0.945\n","epoch 09 - train loss/acc: 0.161 0.954, valid loss/acc: 0.180 0.946\n","epoch 10 - train loss/acc: 0.155 0.955, valid loss/acc: 0.175 0.949\n","epoch 11 - train loss/acc: 0.148 0.958, valid loss/acc: 0.170 0.949\n","epoch 12 - train loss/acc: 0.143 0.959, valid loss/acc: 0.166 0.951\n","epoch 13 - train loss/acc: 0.137 0.961, valid loss/acc: 0.161 0.952\n","epoch 14 - train loss/acc: 0.132 0.963, valid loss/acc: 0.159 0.952\n","Combination 88 - lr: 1.590, n_epochs: 15, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.375 0.897, valid loss/acc: 0.380 0.894\n","epoch 01 - train loss/acc: 0.304 0.911, valid loss/acc: 0.310 0.909\n","epoch 02 - train loss/acc: 0.264 0.923, valid loss/acc: 0.274 0.919\n","epoch 03 - train loss/acc: 0.230 0.934, valid loss/acc: 0.241 0.932\n","epoch 04 - train loss/acc: 0.211 0.940, valid loss/acc: 0.223 0.936\n","epoch 05 - train loss/acc: 0.194 0.944, valid loss/acc: 0.208 0.940\n","epoch 06 - train loss/acc: 0.181 0.948, valid loss/acc: 0.196 0.943\n","epoch 07 - train loss/acc: 0.169 0.951, valid loss/acc: 0.188 0.946\n","epoch 08 - train loss/acc: 0.156 0.955, valid loss/acc: 0.175 0.950\n","epoch 09 - train loss/acc: 0.147 0.958, valid loss/acc: 0.168 0.950\n","epoch 10 - train loss/acc: 0.137 0.961, valid loss/acc: 0.159 0.953\n","epoch 11 - train loss/acc: 0.132 0.963, valid loss/acc: 0.155 0.956\n","epoch 12 - train loss/acc: 0.124 0.965, valid loss/acc: 0.148 0.957\n","epoch 13 - train loss/acc: 0.117 0.968, valid loss/acc: 0.143 0.958\n","epoch 14 - train loss/acc: 0.112 0.969, valid loss/acc: 0.139 0.959\n","Combination 89 - lr: 1.590, n_epochs: 15, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.382 0.888, valid loss/acc: 0.386 0.887\n","epoch 01 - train loss/acc: 0.300 0.913, valid loss/acc: 0.306 0.911\n","epoch 02 - train loss/acc: 0.257 0.924, valid loss/acc: 0.264 0.920\n","epoch 03 - train loss/acc: 0.234 0.932, valid loss/acc: 0.243 0.929\n","epoch 04 - train loss/acc: 0.213 0.938, valid loss/acc: 0.224 0.933\n","epoch 05 - train loss/acc: 0.191 0.945, valid loss/acc: 0.203 0.940\n","epoch 06 - train loss/acc: 0.174 0.950, valid loss/acc: 0.188 0.945\n","epoch 07 - train loss/acc: 0.163 0.953, valid loss/acc: 0.179 0.948\n","epoch 08 - train loss/acc: 0.150 0.957, valid loss/acc: 0.167 0.951\n","epoch 09 - train loss/acc: 0.138 0.960, valid loss/acc: 0.156 0.954\n","epoch 10 - train loss/acc: 0.128 0.963, valid loss/acc: 0.147 0.957\n","epoch 11 - train loss/acc: 0.124 0.964, valid loss/acc: 0.144 0.957\n","epoch 12 - train loss/acc: 0.116 0.967, valid loss/acc: 0.137 0.960\n","epoch 13 - train loss/acc: 0.109 0.969, valid loss/acc: 0.132 0.960\n","epoch 14 - train loss/acc: 0.103 0.971, valid loss/acc: 0.128 0.962\n","Combination 90 - lr: 1.590, n_epochs: 20, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.236 0.932, valid loss/acc: 0.243 0.930\n","epoch 01 - train loss/acc: 0.188 0.945, valid loss/acc: 0.203 0.941\n","epoch 02 - train loss/acc: 0.146 0.959, valid loss/acc: 0.170 0.952\n","epoch 03 - train loss/acc: 0.122 0.965, valid loss/acc: 0.149 0.957\n","epoch 04 - train loss/acc: 0.108 0.969, valid loss/acc: 0.140 0.959\n","epoch 05 - train loss/acc: 0.102 0.971, valid loss/acc: 0.139 0.960\n","epoch 06 - train loss/acc: 0.092 0.974, valid loss/acc: 0.135 0.960\n","epoch 07 - train loss/acc: 0.085 0.976, valid loss/acc: 0.131 0.960\n","epoch 08 - train loss/acc: 0.081 0.977, valid loss/acc: 0.127 0.963\n","epoch 09 - train loss/acc: 0.075 0.978, valid loss/acc: 0.129 0.961\n","epoch 10 - train loss/acc: 0.075 0.978, valid loss/acc: 0.131 0.961\n","epoch 11 - train loss/acc: 0.066 0.981, valid loss/acc: 0.125 0.963\n","epoch 12 - train loss/acc: 0.059 0.984, valid loss/acc: 0.121 0.965\n","epoch 13 - train loss/acc: 0.059 0.984, valid loss/acc: 0.123 0.964\n","epoch 14 - train loss/acc: 0.055 0.985, valid loss/acc: 0.122 0.964\n","epoch 15 - train loss/acc: 0.056 0.984, valid loss/acc: 0.126 0.965\n","epoch 16 - train loss/acc: 0.052 0.986, valid loss/acc: 0.127 0.963\n","epoch 17 - train loss/acc: 0.047 0.988, valid loss/acc: 0.122 0.965\n","epoch 18 - train loss/acc: 0.048 0.987, valid loss/acc: 0.127 0.963\n","epoch 19 - train loss/acc: 0.052 0.985, valid loss/acc: 0.134 0.962\n","Combination 91 - lr: 1.590, n_epochs: 20, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.247 0.928, valid loss/acc: 0.260 0.926\n","epoch 01 - train loss/acc: 0.168 0.951, valid loss/acc: 0.184 0.945\n","epoch 02 - train loss/acc: 0.137 0.961, valid loss/acc: 0.159 0.953\n","epoch 03 - train loss/acc: 0.109 0.969, valid loss/acc: 0.136 0.961\n","epoch 04 - train loss/acc: 0.095 0.973, valid loss/acc: 0.126 0.963\n","epoch 05 - train loss/acc: 0.085 0.976, valid loss/acc: 0.121 0.964\n","epoch 06 - train loss/acc: 0.074 0.980, valid loss/acc: 0.112 0.966\n","epoch 07 - train loss/acc: 0.073 0.979, valid loss/acc: 0.114 0.966\n","epoch 08 - train loss/acc: 0.059 0.984, valid loss/acc: 0.104 0.969\n","epoch 09 - train loss/acc: 0.053 0.986, valid loss/acc: 0.103 0.969\n","epoch 10 - train loss/acc: 0.049 0.987, valid loss/acc: 0.103 0.969\n","epoch 11 - train loss/acc: 0.043 0.990, valid loss/acc: 0.098 0.970\n","epoch 12 - train loss/acc: 0.041 0.990, valid loss/acc: 0.103 0.969\n","epoch 13 - train loss/acc: 0.036 0.991, valid loss/acc: 0.099 0.972\n","epoch 14 - train loss/acc: 0.032 0.993, valid loss/acc: 0.096 0.971\n","epoch 15 - train loss/acc: 0.032 0.993, valid loss/acc: 0.098 0.971\n","epoch 16 - train loss/acc: 0.028 0.995, valid loss/acc: 0.098 0.970\n","epoch 17 - train loss/acc: 0.026 0.995, valid loss/acc: 0.096 0.972\n","epoch 18 - train loss/acc: 0.023 0.996, valid loss/acc: 0.095 0.972\n","epoch 19 - train loss/acc: 0.022 0.996, valid loss/acc: 0.096 0.973\n","Combination 92 - lr: 1.590, n_epochs: 20, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.243 0.928, valid loss/acc: 0.251 0.926\n","epoch 01 - train loss/acc: 0.175 0.946, valid loss/acc: 0.188 0.943\n","epoch 02 - train loss/acc: 0.130 0.962, valid loss/acc: 0.152 0.956\n","epoch 03 - train loss/acc: 0.104 0.970, valid loss/acc: 0.129 0.962\n","epoch 04 - train loss/acc: 0.087 0.975, valid loss/acc: 0.116 0.965\n","epoch 05 - train loss/acc: 0.075 0.979, valid loss/acc: 0.107 0.968\n","epoch 06 - train loss/acc: 0.067 0.981, valid loss/acc: 0.103 0.968\n","epoch 07 - train loss/acc: 0.060 0.983, valid loss/acc: 0.097 0.972\n","epoch 08 - train loss/acc: 0.050 0.987, valid loss/acc: 0.091 0.973\n","epoch 09 - train loss/acc: 0.046 0.988, valid loss/acc: 0.088 0.974\n","epoch 10 - train loss/acc: 0.040 0.990, valid loss/acc: 0.085 0.974\n","epoch 11 - train loss/acc: 0.040 0.990, valid loss/acc: 0.088 0.975\n","epoch 12 - train loss/acc: 0.034 0.992, valid loss/acc: 0.083 0.975\n","epoch 13 - train loss/acc: 0.030 0.994, valid loss/acc: 0.081 0.974\n","epoch 14 - train loss/acc: 0.026 0.994, valid loss/acc: 0.080 0.976\n","epoch 15 - train loss/acc: 0.026 0.994, valid loss/acc: 0.082 0.975\n","epoch 16 - train loss/acc: 0.023 0.996, valid loss/acc: 0.081 0.976\n","epoch 17 - train loss/acc: 0.020 0.997, valid loss/acc: 0.078 0.978\n","epoch 18 - train loss/acc: 0.018 0.997, valid loss/acc: 0.078 0.977\n","epoch 19 - train loss/acc: 0.017 0.998, valid loss/acc: 0.078 0.977\n","Combination 93 - lr: 1.590, n_epochs: 20, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.302 0.912, valid loss/acc: 0.308 0.910\n","epoch 01 - train loss/acc: 0.236 0.932, valid loss/acc: 0.245 0.931\n","epoch 02 - train loss/acc: 0.205 0.939, valid loss/acc: 0.219 0.936\n","epoch 03 - train loss/acc: 0.174 0.950, valid loss/acc: 0.193 0.946\n","epoch 04 - train loss/acc: 0.160 0.954, valid loss/acc: 0.181 0.949\n","epoch 05 - train loss/acc: 0.147 0.959, valid loss/acc: 0.172 0.951\n","epoch 06 - train loss/acc: 0.134 0.962, valid loss/acc: 0.161 0.953\n","epoch 07 - train loss/acc: 0.129 0.963, valid loss/acc: 0.160 0.952\n","epoch 08 - train loss/acc: 0.118 0.967, valid loss/acc: 0.150 0.955\n","epoch 09 - train loss/acc: 0.111 0.969, valid loss/acc: 0.146 0.958\n","epoch 10 - train loss/acc: 0.105 0.971, valid loss/acc: 0.142 0.957\n","epoch 11 - train loss/acc: 0.102 0.972, valid loss/acc: 0.140 0.958\n","epoch 12 - train loss/acc: 0.096 0.974, valid loss/acc: 0.136 0.960\n","epoch 13 - train loss/acc: 0.089 0.976, valid loss/acc: 0.134 0.960\n","epoch 14 - train loss/acc: 0.087 0.976, valid loss/acc: 0.132 0.960\n","epoch 15 - train loss/acc: 0.087 0.976, valid loss/acc: 0.132 0.961\n","epoch 16 - train loss/acc: 0.080 0.979, valid loss/acc: 0.128 0.962\n","epoch 17 - train loss/acc: 0.078 0.979, valid loss/acc: 0.131 0.961\n","epoch 18 - train loss/acc: 0.075 0.980, valid loss/acc: 0.128 0.962\n","epoch 19 - train loss/acc: 0.072 0.980, valid loss/acc: 0.127 0.962\n","Combination 94 - lr: 1.590, n_epochs: 20, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.300 0.913, valid loss/acc: 0.305 0.911\n","epoch 01 - train loss/acc: 0.231 0.933, valid loss/acc: 0.241 0.929\n","epoch 02 - train loss/acc: 0.193 0.944, valid loss/acc: 0.206 0.940\n","epoch 03 - train loss/acc: 0.168 0.953, valid loss/acc: 0.182 0.946\n","epoch 04 - train loss/acc: 0.145 0.958, valid loss/acc: 0.163 0.952\n","epoch 05 - train loss/acc: 0.128 0.964, valid loss/acc: 0.149 0.957\n","epoch 06 - train loss/acc: 0.119 0.966, valid loss/acc: 0.144 0.958\n","epoch 07 - train loss/acc: 0.107 0.970, valid loss/acc: 0.134 0.961\n","epoch 08 - train loss/acc: 0.099 0.973, valid loss/acc: 0.125 0.964\n","epoch 09 - train loss/acc: 0.091 0.975, valid loss/acc: 0.122 0.964\n","epoch 10 - train loss/acc: 0.089 0.975, valid loss/acc: 0.126 0.961\n","epoch 11 - train loss/acc: 0.078 0.979, valid loss/acc: 0.114 0.966\n","epoch 12 - train loss/acc: 0.074 0.980, valid loss/acc: 0.113 0.966\n","epoch 13 - train loss/acc: 0.071 0.980, valid loss/acc: 0.112 0.967\n","epoch 14 - train loss/acc: 0.063 0.983, valid loss/acc: 0.104 0.969\n","epoch 15 - train loss/acc: 0.059 0.985, valid loss/acc: 0.102 0.968\n","epoch 16 - train loss/acc: 0.056 0.986, valid loss/acc: 0.100 0.969\n","epoch 17 - train loss/acc: 0.055 0.986, valid loss/acc: 0.103 0.969\n","epoch 18 - train loss/acc: 0.051 0.987, valid loss/acc: 0.100 0.970\n","epoch 19 - train loss/acc: 0.048 0.988, valid loss/acc: 0.098 0.970\n","Combination 95 - lr: 1.590, n_epochs: 20, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.308 0.908, valid loss/acc: 0.312 0.904\n","epoch 01 - train loss/acc: 0.234 0.932, valid loss/acc: 0.243 0.930\n","epoch 02 - train loss/acc: 0.189 0.945, valid loss/acc: 0.202 0.940\n","epoch 03 - train loss/acc: 0.162 0.953, valid loss/acc: 0.177 0.948\n","epoch 04 - train loss/acc: 0.142 0.958, valid loss/acc: 0.160 0.953\n","epoch 05 - train loss/acc: 0.122 0.965, valid loss/acc: 0.143 0.957\n","epoch 06 - train loss/acc: 0.111 0.968, valid loss/acc: 0.134 0.960\n","epoch 07 - train loss/acc: 0.098 0.972, valid loss/acc: 0.124 0.961\n","epoch 08 - train loss/acc: 0.092 0.974, valid loss/acc: 0.119 0.964\n","epoch 09 - train loss/acc: 0.082 0.977, valid loss/acc: 0.113 0.965\n","epoch 10 - train loss/acc: 0.077 0.978, valid loss/acc: 0.110 0.967\n","epoch 11 - train loss/acc: 0.069 0.981, valid loss/acc: 0.104 0.969\n","epoch 12 - train loss/acc: 0.066 0.982, valid loss/acc: 0.102 0.969\n","epoch 13 - train loss/acc: 0.060 0.984, valid loss/acc: 0.099 0.971\n","epoch 14 - train loss/acc: 0.057 0.985, valid loss/acc: 0.097 0.971\n","epoch 15 - train loss/acc: 0.055 0.986, valid loss/acc: 0.096 0.970\n","epoch 16 - train loss/acc: 0.051 0.987, valid loss/acc: 0.094 0.971\n","epoch 17 - train loss/acc: 0.045 0.988, valid loss/acc: 0.090 0.972\n","epoch 18 - train loss/acc: 0.042 0.990, valid loss/acc: 0.088 0.973\n","epoch 19 - train loss/acc: 0.040 0.991, valid loss/acc: 0.088 0.973\n","Combination 96 - lr: 1.590, n_epochs: 20, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.401 0.891, valid loss/acc: 0.407 0.885\n","epoch 01 - train loss/acc: 0.300 0.918, valid loss/acc: 0.308 0.913\n","epoch 02 - train loss/acc: 0.255 0.928, valid loss/acc: 0.265 0.923\n","epoch 03 - train loss/acc: 0.230 0.934, valid loss/acc: 0.243 0.930\n","epoch 04 - train loss/acc: 0.212 0.938, valid loss/acc: 0.226 0.936\n","epoch 05 - train loss/acc: 0.195 0.944, valid loss/acc: 0.212 0.939\n","epoch 06 - train loss/acc: 0.186 0.946, valid loss/acc: 0.204 0.943\n","epoch 07 - train loss/acc: 0.172 0.950, valid loss/acc: 0.192 0.944\n","epoch 08 - train loss/acc: 0.161 0.955, valid loss/acc: 0.184 0.948\n","epoch 09 - train loss/acc: 0.152 0.957, valid loss/acc: 0.176 0.949\n","epoch 10 - train loss/acc: 0.146 0.958, valid loss/acc: 0.172 0.951\n","epoch 11 - train loss/acc: 0.138 0.961, valid loss/acc: 0.167 0.952\n","epoch 12 - train loss/acc: 0.133 0.963, valid loss/acc: 0.162 0.953\n","epoch 13 - train loss/acc: 0.126 0.966, valid loss/acc: 0.157 0.953\n","epoch 14 - train loss/acc: 0.121 0.967, valid loss/acc: 0.154 0.955\n","epoch 15 - train loss/acc: 0.118 0.967, valid loss/acc: 0.152 0.956\n","epoch 16 - train loss/acc: 0.114 0.968, valid loss/acc: 0.150 0.956\n","epoch 17 - train loss/acc: 0.108 0.970, valid loss/acc: 0.144 0.958\n","epoch 18 - train loss/acc: 0.105 0.971, valid loss/acc: 0.143 0.957\n","epoch 19 - train loss/acc: 0.102 0.973, valid loss/acc: 0.142 0.958\n","Combination 97 - lr: 1.590, n_epochs: 20, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.392 0.881, valid loss/acc: 0.398 0.876\n","epoch 01 - train loss/acc: 0.293 0.915, valid loss/acc: 0.300 0.912\n","epoch 02 - train loss/acc: 0.252 0.929, valid loss/acc: 0.261 0.924\n","epoch 03 - train loss/acc: 0.223 0.936, valid loss/acc: 0.234 0.931\n","epoch 04 - train loss/acc: 0.202 0.942, valid loss/acc: 0.214 0.940\n","epoch 05 - train loss/acc: 0.183 0.948, valid loss/acc: 0.196 0.942\n","epoch 06 - train loss/acc: 0.172 0.950, valid loss/acc: 0.187 0.945\n","epoch 07 - train loss/acc: 0.158 0.955, valid loss/acc: 0.174 0.948\n","epoch 08 - train loss/acc: 0.148 0.957, valid loss/acc: 0.165 0.951\n","epoch 09 - train loss/acc: 0.138 0.960, valid loss/acc: 0.156 0.954\n","epoch 10 - train loss/acc: 0.130 0.963, valid loss/acc: 0.151 0.955\n","epoch 11 - train loss/acc: 0.125 0.964, valid loss/acc: 0.147 0.956\n","epoch 12 - train loss/acc: 0.119 0.966, valid loss/acc: 0.142 0.957\n","epoch 13 - train loss/acc: 0.111 0.969, valid loss/acc: 0.135 0.959\n","epoch 14 - train loss/acc: 0.107 0.970, valid loss/acc: 0.132 0.962\n","epoch 15 - train loss/acc: 0.102 0.972, valid loss/acc: 0.128 0.962\n","epoch 16 - train loss/acc: 0.097 0.973, valid loss/acc: 0.124 0.962\n","epoch 17 - train loss/acc: 0.092 0.974, valid loss/acc: 0.121 0.962\n","epoch 18 - train loss/acc: 0.089 0.975, valid loss/acc: 0.119 0.964\n","epoch 19 - train loss/acc: 0.087 0.976, valid loss/acc: 0.117 0.964\n","Combination 98 - lr: 1.590, n_epochs: 20, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.381 0.891, valid loss/acc: 0.385 0.888\n","epoch 01 - train loss/acc: 0.311 0.908, valid loss/acc: 0.316 0.906\n","epoch 02 - train loss/acc: 0.259 0.925, valid loss/acc: 0.267 0.921\n","epoch 03 - train loss/acc: 0.233 0.933, valid loss/acc: 0.241 0.929\n","epoch 04 - train loss/acc: 0.214 0.938, valid loss/acc: 0.223 0.934\n","epoch 05 - train loss/acc: 0.195 0.943, valid loss/acc: 0.206 0.940\n","epoch 06 - train loss/acc: 0.182 0.946, valid loss/acc: 0.196 0.943\n","epoch 07 - train loss/acc: 0.165 0.952, valid loss/acc: 0.181 0.948\n","epoch 08 - train loss/acc: 0.154 0.956, valid loss/acc: 0.169 0.951\n","epoch 09 - train loss/acc: 0.144 0.959, valid loss/acc: 0.162 0.953\n","epoch 10 - train loss/acc: 0.133 0.962, valid loss/acc: 0.153 0.956\n","epoch 11 - train loss/acc: 0.125 0.964, valid loss/acc: 0.146 0.957\n","epoch 12 - train loss/acc: 0.118 0.966, valid loss/acc: 0.140 0.959\n","epoch 13 - train loss/acc: 0.111 0.968, valid loss/acc: 0.133 0.962\n","epoch 14 - train loss/acc: 0.105 0.971, valid loss/acc: 0.128 0.962\n","epoch 15 - train loss/acc: 0.103 0.971, valid loss/acc: 0.127 0.963\n","epoch 16 - train loss/acc: 0.096 0.973, valid loss/acc: 0.123 0.962\n","epoch 17 - train loss/acc: 0.091 0.975, valid loss/acc: 0.118 0.966\n","epoch 18 - train loss/acc: 0.086 0.976, valid loss/acc: 0.115 0.966\n","epoch 19 - train loss/acc: 0.083 0.977, valid loss/acc: 0.113 0.965\n","Combination 99 - lr: 1.590, n_epochs: 25, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.248 0.928, valid loss/acc: 0.259 0.925\n","epoch 01 - train loss/acc: 0.183 0.946, valid loss/acc: 0.200 0.942\n","epoch 02 - train loss/acc: 0.155 0.955, valid loss/acc: 0.178 0.948\n","epoch 03 - train loss/acc: 0.138 0.960, valid loss/acc: 0.166 0.952\n","epoch 04 - train loss/acc: 0.118 0.966, valid loss/acc: 0.149 0.955\n","epoch 05 - train loss/acc: 0.109 0.968, valid loss/acc: 0.145 0.956\n","epoch 06 - train loss/acc: 0.112 0.966, valid loss/acc: 0.151 0.953\n","epoch 07 - train loss/acc: 0.094 0.973, valid loss/acc: 0.141 0.959\n","epoch 08 - train loss/acc: 0.086 0.975, valid loss/acc: 0.136 0.959\n","epoch 09 - train loss/acc: 0.079 0.977, valid loss/acc: 0.133 0.960\n","epoch 10 - train loss/acc: 0.079 0.977, valid loss/acc: 0.139 0.959\n","epoch 11 - train loss/acc: 0.072 0.980, valid loss/acc: 0.132 0.959\n","epoch 12 - train loss/acc: 0.067 0.982, valid loss/acc: 0.129 0.961\n","epoch 13 - train loss/acc: 0.066 0.982, valid loss/acc: 0.131 0.961\n","epoch 14 - train loss/acc: 0.061 0.983, valid loss/acc: 0.130 0.960\n","epoch 15 - train loss/acc: 0.061 0.983, valid loss/acc: 0.133 0.960\n","epoch 16 - train loss/acc: 0.062 0.982, valid loss/acc: 0.136 0.959\n","epoch 17 - train loss/acc: 0.054 0.985, valid loss/acc: 0.133 0.960\n","epoch 18 - train loss/acc: 0.050 0.987, valid loss/acc: 0.128 0.961\n","epoch 19 - train loss/acc: 0.049 0.987, valid loss/acc: 0.134 0.960\n","epoch 20 - train loss/acc: 0.044 0.988, valid loss/acc: 0.133 0.960\n","epoch 21 - train loss/acc: 0.042 0.990, valid loss/acc: 0.131 0.962\n","epoch 22 - train loss/acc: 0.041 0.989, valid loss/acc: 0.133 0.960\n","epoch 23 - train loss/acc: 0.043 0.989, valid loss/acc: 0.136 0.961\n","epoch 24 - train loss/acc: 0.043 0.988, valid loss/acc: 0.144 0.958\n","Combination 100 - lr: 1.590, n_epochs: 25, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.239 0.930, valid loss/acc: 0.250 0.925\n","epoch 01 - train loss/acc: 0.167 0.951, valid loss/acc: 0.183 0.946\n","epoch 02 - train loss/acc: 0.133 0.961, valid loss/acc: 0.153 0.953\n","epoch 03 - train loss/acc: 0.110 0.969, valid loss/acc: 0.134 0.958\n","epoch 04 - train loss/acc: 0.103 0.970, valid loss/acc: 0.137 0.957\n","epoch 05 - train loss/acc: 0.085 0.976, valid loss/acc: 0.118 0.964\n","epoch 06 - train loss/acc: 0.074 0.980, valid loss/acc: 0.109 0.967\n","epoch 07 - train loss/acc: 0.072 0.979, valid loss/acc: 0.112 0.967\n","epoch 08 - train loss/acc: 0.057 0.985, valid loss/acc: 0.099 0.970\n","epoch 09 - train loss/acc: 0.052 0.986, valid loss/acc: 0.098 0.971\n","epoch 10 - train loss/acc: 0.047 0.988, valid loss/acc: 0.094 0.972\n","epoch 11 - train loss/acc: 0.044 0.989, valid loss/acc: 0.093 0.972\n","epoch 12 - train loss/acc: 0.039 0.991, valid loss/acc: 0.092 0.973\n","epoch 13 - train loss/acc: 0.035 0.992, valid loss/acc: 0.091 0.973\n","epoch 14 - train loss/acc: 0.037 0.991, valid loss/acc: 0.093 0.972\n","epoch 15 - train loss/acc: 0.031 0.993, valid loss/acc: 0.088 0.973\n","epoch 16 - train loss/acc: 0.029 0.994, valid loss/acc: 0.090 0.973\n","epoch 17 - train loss/acc: 0.029 0.993, valid loss/acc: 0.090 0.972\n","epoch 18 - train loss/acc: 0.025 0.995, valid loss/acc: 0.092 0.972\n","epoch 19 - train loss/acc: 0.022 0.996, valid loss/acc: 0.089 0.974\n","epoch 20 - train loss/acc: 0.023 0.996, valid loss/acc: 0.092 0.973\n","epoch 21 - train loss/acc: 0.020 0.997, valid loss/acc: 0.089 0.973\n","epoch 22 - train loss/acc: 0.019 0.997, valid loss/acc: 0.090 0.974\n","epoch 23 - train loss/acc: 0.019 0.998, valid loss/acc: 0.093 0.972\n","epoch 24 - train loss/acc: 0.016 0.998, valid loss/acc: 0.090 0.974\n","Combination 101 - lr: 1.590, n_epochs: 25, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.248 0.926, valid loss/acc: 0.256 0.922\n","epoch 01 - train loss/acc: 0.177 0.949, valid loss/acc: 0.194 0.941\n","epoch 02 - train loss/acc: 0.133 0.961, valid loss/acc: 0.155 0.953\n","epoch 03 - train loss/acc: 0.106 0.969, valid loss/acc: 0.130 0.960\n","epoch 04 - train loss/acc: 0.089 0.975, valid loss/acc: 0.117 0.964\n","epoch 05 - train loss/acc: 0.078 0.978, valid loss/acc: 0.111 0.967\n","epoch 06 - train loss/acc: 0.067 0.981, valid loss/acc: 0.103 0.969\n","epoch 07 - train loss/acc: 0.061 0.983, valid loss/acc: 0.103 0.970\n","epoch 08 - train loss/acc: 0.057 0.985, valid loss/acc: 0.100 0.971\n","epoch 09 - train loss/acc: 0.050 0.986, valid loss/acc: 0.096 0.972\n","epoch 10 - train loss/acc: 0.042 0.989, valid loss/acc: 0.090 0.973\n","epoch 11 - train loss/acc: 0.042 0.988, valid loss/acc: 0.095 0.973\n","epoch 12 - train loss/acc: 0.036 0.991, valid loss/acc: 0.092 0.971\n","epoch 13 - train loss/acc: 0.033 0.992, valid loss/acc: 0.089 0.974\n","epoch 14 - train loss/acc: 0.031 0.993, valid loss/acc: 0.092 0.973\n","epoch 15 - train loss/acc: 0.026 0.995, valid loss/acc: 0.086 0.974\n","epoch 16 - train loss/acc: 0.024 0.995, valid loss/acc: 0.084 0.976\n","epoch 17 - train loss/acc: 0.023 0.996, valid loss/acc: 0.086 0.974\n","epoch 18 - train loss/acc: 0.021 0.996, valid loss/acc: 0.087 0.975\n","epoch 19 - train loss/acc: 0.019 0.997, valid loss/acc: 0.087 0.974\n","epoch 20 - train loss/acc: 0.017 0.998, valid loss/acc: 0.087 0.976\n","epoch 21 - train loss/acc: 0.016 0.998, valid loss/acc: 0.087 0.975\n","epoch 22 - train loss/acc: 0.014 0.998, valid loss/acc: 0.086 0.975\n","epoch 23 - train loss/acc: 0.014 0.999, valid loss/acc: 0.086 0.975\n","epoch 24 - train loss/acc: 0.012 0.999, valid loss/acc: 0.086 0.977\n","Combination 102 - lr: 1.590, n_epochs: 25, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.313 0.911, valid loss/acc: 0.318 0.909\n","epoch 01 - train loss/acc: 0.241 0.930, valid loss/acc: 0.250 0.926\n","epoch 02 - train loss/acc: 0.214 0.939, valid loss/acc: 0.226 0.934\n","epoch 03 - train loss/acc: 0.188 0.947, valid loss/acc: 0.205 0.941\n","epoch 04 - train loss/acc: 0.160 0.955, valid loss/acc: 0.180 0.947\n","epoch 05 - train loss/acc: 0.160 0.952, valid loss/acc: 0.182 0.945\n","epoch 06 - train loss/acc: 0.143 0.960, valid loss/acc: 0.167 0.953\n","epoch 07 - train loss/acc: 0.132 0.962, valid loss/acc: 0.162 0.951\n","epoch 08 - train loss/acc: 0.120 0.965, valid loss/acc: 0.152 0.956\n","epoch 09 - train loss/acc: 0.112 0.968, valid loss/acc: 0.146 0.957\n","epoch 10 - train loss/acc: 0.106 0.970, valid loss/acc: 0.142 0.958\n","epoch 11 - train loss/acc: 0.101 0.972, valid loss/acc: 0.138 0.960\n","epoch 12 - train loss/acc: 0.095 0.974, valid loss/acc: 0.133 0.960\n","epoch 13 - train loss/acc: 0.095 0.973, valid loss/acc: 0.136 0.960\n","epoch 14 - train loss/acc: 0.089 0.975, valid loss/acc: 0.132 0.961\n","epoch 15 - train loss/acc: 0.084 0.977, valid loss/acc: 0.130 0.961\n","epoch 16 - train loss/acc: 0.082 0.978, valid loss/acc: 0.130 0.962\n","epoch 17 - train loss/acc: 0.077 0.979, valid loss/acc: 0.126 0.962\n","epoch 18 - train loss/acc: 0.075 0.980, valid loss/acc: 0.125 0.962\n","epoch 19 - train loss/acc: 0.073 0.980, valid loss/acc: 0.125 0.963\n","epoch 20 - train loss/acc: 0.069 0.981, valid loss/acc: 0.124 0.963\n","epoch 21 - train loss/acc: 0.070 0.981, valid loss/acc: 0.126 0.963\n","epoch 22 - train loss/acc: 0.065 0.983, valid loss/acc: 0.123 0.964\n","epoch 23 - train loss/acc: 0.064 0.983, valid loss/acc: 0.126 0.964\n","epoch 24 - train loss/acc: 0.060 0.985, valid loss/acc: 0.120 0.965\n","Combination 103 - lr: 1.590, n_epochs: 25, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.293 0.915, valid loss/acc: 0.300 0.911\n","epoch 01 - train loss/acc: 0.226 0.935, valid loss/acc: 0.234 0.931\n","epoch 02 - train loss/acc: 0.186 0.946, valid loss/acc: 0.200 0.941\n","epoch 03 - train loss/acc: 0.171 0.949, valid loss/acc: 0.187 0.945\n","epoch 04 - train loss/acc: 0.142 0.959, valid loss/acc: 0.160 0.953\n","epoch 05 - train loss/acc: 0.129 0.962, valid loss/acc: 0.150 0.955\n","epoch 06 - train loss/acc: 0.114 0.967, valid loss/acc: 0.138 0.958\n","epoch 07 - train loss/acc: 0.105 0.970, valid loss/acc: 0.131 0.960\n","epoch 08 - train loss/acc: 0.097 0.973, valid loss/acc: 0.125 0.961\n","epoch 09 - train loss/acc: 0.087 0.976, valid loss/acc: 0.117 0.965\n","epoch 10 - train loss/acc: 0.082 0.977, valid loss/acc: 0.117 0.964\n","epoch 11 - train loss/acc: 0.076 0.979, valid loss/acc: 0.112 0.965\n","epoch 12 - train loss/acc: 0.070 0.981, valid loss/acc: 0.108 0.967\n","epoch 13 - train loss/acc: 0.065 0.982, valid loss/acc: 0.104 0.968\n","epoch 14 - train loss/acc: 0.062 0.983, valid loss/acc: 0.104 0.968\n","epoch 15 - train loss/acc: 0.057 0.985, valid loss/acc: 0.101 0.968\n","epoch 16 - train loss/acc: 0.054 0.986, valid loss/acc: 0.098 0.971\n","epoch 17 - train loss/acc: 0.052 0.987, valid loss/acc: 0.098 0.969\n","epoch 18 - train loss/acc: 0.048 0.988, valid loss/acc: 0.096 0.970\n","epoch 19 - train loss/acc: 0.046 0.988, valid loss/acc: 0.097 0.970\n","epoch 20 - train loss/acc: 0.043 0.990, valid loss/acc: 0.094 0.970\n","epoch 21 - train loss/acc: 0.041 0.990, valid loss/acc: 0.093 0.972\n","epoch 22 - train loss/acc: 0.039 0.991, valid loss/acc: 0.091 0.973\n","epoch 23 - train loss/acc: 0.036 0.992, valid loss/acc: 0.090 0.973\n","epoch 24 - train loss/acc: 0.035 0.993, valid loss/acc: 0.091 0.972\n","Combination 104 - lr: 1.590, n_epochs: 25, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.312 0.911, valid loss/acc: 0.316 0.910\n","epoch 01 - train loss/acc: 0.236 0.931, valid loss/acc: 0.244 0.929\n","epoch 02 - train loss/acc: 0.201 0.942, valid loss/acc: 0.213 0.938\n","epoch 03 - train loss/acc: 0.170 0.950, valid loss/acc: 0.183 0.946\n","epoch 04 - train loss/acc: 0.146 0.958, valid loss/acc: 0.164 0.952\n","epoch 05 - train loss/acc: 0.132 0.962, valid loss/acc: 0.153 0.955\n","epoch 06 - train loss/acc: 0.121 0.965, valid loss/acc: 0.144 0.957\n","epoch 07 - train loss/acc: 0.111 0.968, valid loss/acc: 0.135 0.961\n","epoch 08 - train loss/acc: 0.100 0.971, valid loss/acc: 0.129 0.964\n","epoch 09 - train loss/acc: 0.096 0.972, valid loss/acc: 0.126 0.963\n","epoch 10 - train loss/acc: 0.084 0.976, valid loss/acc: 0.115 0.966\n","epoch 11 - train loss/acc: 0.079 0.978, valid loss/acc: 0.114 0.966\n","epoch 12 - train loss/acc: 0.073 0.980, valid loss/acc: 0.110 0.968\n","epoch 13 - train loss/acc: 0.069 0.981, valid loss/acc: 0.106 0.968\n","epoch 14 - train loss/acc: 0.064 0.983, valid loss/acc: 0.103 0.969\n","epoch 15 - train loss/acc: 0.063 0.982, valid loss/acc: 0.104 0.969\n","epoch 16 - train loss/acc: 0.056 0.986, valid loss/acc: 0.099 0.970\n","epoch 17 - train loss/acc: 0.054 0.986, valid loss/acc: 0.100 0.970\n","epoch 18 - train loss/acc: 0.049 0.988, valid loss/acc: 0.095 0.971\n","epoch 19 - train loss/acc: 0.048 0.988, valid loss/acc: 0.095 0.971\n","epoch 20 - train loss/acc: 0.045 0.989, valid loss/acc: 0.096 0.971\n","epoch 21 - train loss/acc: 0.043 0.990, valid loss/acc: 0.094 0.973\n","epoch 22 - train loss/acc: 0.041 0.990, valid loss/acc: 0.094 0.972\n","epoch 23 - train loss/acc: 0.040 0.990, valid loss/acc: 0.092 0.973\n","epoch 24 - train loss/acc: 0.035 0.992, valid loss/acc: 0.091 0.973\n","Combination 105 - lr: 1.590, n_epochs: 25, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.385 0.896, valid loss/acc: 0.389 0.892\n","epoch 01 - train loss/acc: 0.297 0.917, valid loss/acc: 0.302 0.915\n","epoch 02 - train loss/acc: 0.252 0.929, valid loss/acc: 0.260 0.927\n","epoch 03 - train loss/acc: 0.226 0.935, valid loss/acc: 0.235 0.934\n","epoch 04 - train loss/acc: 0.211 0.941, valid loss/acc: 0.221 0.936\n","epoch 05 - train loss/acc: 0.193 0.946, valid loss/acc: 0.205 0.941\n","epoch 06 - train loss/acc: 0.184 0.948, valid loss/acc: 0.199 0.943\n","epoch 07 - train loss/acc: 0.173 0.951, valid loss/acc: 0.189 0.945\n","epoch 08 - train loss/acc: 0.163 0.954, valid loss/acc: 0.179 0.948\n","epoch 09 - train loss/acc: 0.155 0.956, valid loss/acc: 0.175 0.948\n","epoch 10 - train loss/acc: 0.149 0.958, valid loss/acc: 0.169 0.951\n","epoch 11 - train loss/acc: 0.142 0.960, valid loss/acc: 0.164 0.952\n","epoch 12 - train loss/acc: 0.138 0.961, valid loss/acc: 0.160 0.954\n","epoch 13 - train loss/acc: 0.130 0.963, valid loss/acc: 0.154 0.954\n","epoch 14 - train loss/acc: 0.127 0.964, valid loss/acc: 0.153 0.955\n","epoch 15 - train loss/acc: 0.121 0.966, valid loss/acc: 0.148 0.957\n","epoch 16 - train loss/acc: 0.118 0.967, valid loss/acc: 0.146 0.958\n","epoch 17 - train loss/acc: 0.117 0.967, valid loss/acc: 0.147 0.957\n","epoch 18 - train loss/acc: 0.111 0.969, valid loss/acc: 0.140 0.958\n","epoch 19 - train loss/acc: 0.107 0.971, valid loss/acc: 0.138 0.959\n","epoch 20 - train loss/acc: 0.105 0.971, valid loss/acc: 0.137 0.960\n","epoch 21 - train loss/acc: 0.102 0.972, valid loss/acc: 0.135 0.960\n","epoch 22 - train loss/acc: 0.099 0.973, valid loss/acc: 0.133 0.959\n","epoch 23 - train loss/acc: 0.095 0.974, valid loss/acc: 0.130 0.961\n","epoch 24 - train loss/acc: 0.094 0.974, valid loss/acc: 0.129 0.961\n","Combination 106 - lr: 1.590, n_epochs: 25, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.395 0.882, valid loss/acc: 0.400 0.876\n","epoch 01 - train loss/acc: 0.297 0.914, valid loss/acc: 0.302 0.910\n","epoch 02 - train loss/acc: 0.254 0.925, valid loss/acc: 0.260 0.923\n","epoch 03 - train loss/acc: 0.222 0.936, valid loss/acc: 0.231 0.933\n","epoch 04 - train loss/acc: 0.200 0.943, valid loss/acc: 0.211 0.939\n","epoch 05 - train loss/acc: 0.183 0.948, valid loss/acc: 0.196 0.943\n","epoch 06 - train loss/acc: 0.168 0.953, valid loss/acc: 0.183 0.947\n","epoch 07 - train loss/acc: 0.156 0.956, valid loss/acc: 0.173 0.950\n","epoch 08 - train loss/acc: 0.144 0.959, valid loss/acc: 0.162 0.954\n","epoch 09 - train loss/acc: 0.139 0.960, valid loss/acc: 0.159 0.953\n","epoch 10 - train loss/acc: 0.128 0.964, valid loss/acc: 0.150 0.957\n","epoch 11 - train loss/acc: 0.121 0.966, valid loss/acc: 0.144 0.957\n","epoch 12 - train loss/acc: 0.116 0.968, valid loss/acc: 0.141 0.959\n","epoch 13 - train loss/acc: 0.110 0.970, valid loss/acc: 0.135 0.959\n","epoch 14 - train loss/acc: 0.106 0.971, valid loss/acc: 0.132 0.961\n","epoch 15 - train loss/acc: 0.099 0.973, valid loss/acc: 0.127 0.962\n","epoch 16 - train loss/acc: 0.095 0.974, valid loss/acc: 0.126 0.962\n","epoch 17 - train loss/acc: 0.094 0.975, valid loss/acc: 0.125 0.962\n","epoch 18 - train loss/acc: 0.087 0.976, valid loss/acc: 0.120 0.964\n","epoch 19 - train loss/acc: 0.083 0.977, valid loss/acc: 0.116 0.964\n","epoch 20 - train loss/acc: 0.081 0.978, valid loss/acc: 0.115 0.965\n","epoch 21 - train loss/acc: 0.077 0.979, valid loss/acc: 0.112 0.966\n","epoch 22 - train loss/acc: 0.076 0.980, valid loss/acc: 0.111 0.967\n","epoch 23 - train loss/acc: 0.072 0.981, valid loss/acc: 0.109 0.967\n","epoch 24 - train loss/acc: 0.069 0.982, valid loss/acc: 0.108 0.968\n","Combination 107 - lr: 1.590, n_epochs: 25, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.401 0.882, valid loss/acc: 0.407 0.878\n","epoch 01 - train loss/acc: 0.310 0.908, valid loss/acc: 0.317 0.906\n","epoch 02 - train loss/acc: 0.264 0.924, valid loss/acc: 0.272 0.921\n","epoch 03 - train loss/acc: 0.234 0.931, valid loss/acc: 0.242 0.930\n","epoch 04 - train loss/acc: 0.210 0.939, valid loss/acc: 0.220 0.936\n","epoch 05 - train loss/acc: 0.195 0.943, valid loss/acc: 0.207 0.939\n","epoch 06 - train loss/acc: 0.175 0.950, valid loss/acc: 0.187 0.945\n","epoch 07 - train loss/acc: 0.160 0.955, valid loss/acc: 0.175 0.949\n","epoch 08 - train loss/acc: 0.154 0.956, valid loss/acc: 0.170 0.949\n","epoch 09 - train loss/acc: 0.144 0.958, valid loss/acc: 0.162 0.951\n","epoch 10 - train loss/acc: 0.130 0.963, valid loss/acc: 0.149 0.956\n","epoch 11 - train loss/acc: 0.121 0.966, valid loss/acc: 0.142 0.956\n","epoch 12 - train loss/acc: 0.117 0.967, valid loss/acc: 0.138 0.958\n","epoch 13 - train loss/acc: 0.109 0.970, valid loss/acc: 0.131 0.960\n","epoch 14 - train loss/acc: 0.103 0.972, valid loss/acc: 0.125 0.962\n","epoch 15 - train loss/acc: 0.097 0.973, valid loss/acc: 0.121 0.963\n","epoch 16 - train loss/acc: 0.093 0.974, valid loss/acc: 0.119 0.965\n","epoch 17 - train loss/acc: 0.090 0.975, valid loss/acc: 0.118 0.965\n","epoch 18 - train loss/acc: 0.084 0.977, valid loss/acc: 0.114 0.966\n","epoch 19 - train loss/acc: 0.080 0.978, valid loss/acc: 0.109 0.966\n","epoch 20 - train loss/acc: 0.076 0.979, valid loss/acc: 0.106 0.968\n","epoch 21 - train loss/acc: 0.073 0.981, valid loss/acc: 0.103 0.968\n","epoch 22 - train loss/acc: 0.069 0.982, valid loss/acc: 0.101 0.969\n","epoch 23 - train loss/acc: 0.067 0.982, valid loss/acc: 0.100 0.970\n","epoch 24 - train loss/acc: 0.064 0.983, valid loss/acc: 0.097 0.970\n","Combination 108 - lr: 1.412, n_epochs: 15, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.257 0.927, valid loss/acc: 0.266 0.924\n","epoch 01 - train loss/acc: 0.194 0.945, valid loss/acc: 0.210 0.939\n","epoch 02 - train loss/acc: 0.163 0.953, valid loss/acc: 0.181 0.946\n","epoch 03 - train loss/acc: 0.139 0.960, valid loss/acc: 0.164 0.952\n","epoch 04 - train loss/acc: 0.123 0.965, valid loss/acc: 0.150 0.956\n","epoch 05 - train loss/acc: 0.111 0.969, valid loss/acc: 0.142 0.958\n","epoch 06 - train loss/acc: 0.108 0.968, valid loss/acc: 0.143 0.958\n","epoch 07 - train loss/acc: 0.096 0.972, valid loss/acc: 0.132 0.961\n","epoch 08 - train loss/acc: 0.089 0.974, valid loss/acc: 0.132 0.961\n","epoch 09 - train loss/acc: 0.085 0.976, valid loss/acc: 0.128 0.963\n","epoch 10 - train loss/acc: 0.081 0.977, valid loss/acc: 0.128 0.963\n","epoch 11 - train loss/acc: 0.072 0.980, valid loss/acc: 0.124 0.964\n","epoch 12 - train loss/acc: 0.073 0.979, valid loss/acc: 0.126 0.963\n","epoch 13 - train loss/acc: 0.067 0.981, valid loss/acc: 0.122 0.964\n","epoch 14 - train loss/acc: 0.061 0.984, valid loss/acc: 0.119 0.965\n","Combination 109 - lr: 1.412, n_epochs: 15, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.242 0.930, valid loss/acc: 0.249 0.928\n","epoch 01 - train loss/acc: 0.179 0.948, valid loss/acc: 0.189 0.945\n","epoch 02 - train loss/acc: 0.139 0.960, valid loss/acc: 0.155 0.954\n","epoch 03 - train loss/acc: 0.112 0.968, valid loss/acc: 0.133 0.961\n","epoch 04 - train loss/acc: 0.106 0.969, valid loss/acc: 0.131 0.960\n","epoch 05 - train loss/acc: 0.087 0.976, valid loss/acc: 0.117 0.965\n","epoch 06 - train loss/acc: 0.079 0.978, valid loss/acc: 0.113 0.966\n","epoch 07 - train loss/acc: 0.069 0.981, valid loss/acc: 0.107 0.967\n","epoch 08 - train loss/acc: 0.062 0.983, valid loss/acc: 0.103 0.969\n","epoch 09 - train loss/acc: 0.056 0.985, valid loss/acc: 0.099 0.970\n","epoch 10 - train loss/acc: 0.051 0.986, valid loss/acc: 0.098 0.970\n","epoch 11 - train loss/acc: 0.047 0.987, valid loss/acc: 0.095 0.971\n","epoch 12 - train loss/acc: 0.043 0.989, valid loss/acc: 0.095 0.971\n","epoch 13 - train loss/acc: 0.044 0.988, valid loss/acc: 0.096 0.971\n","epoch 14 - train loss/acc: 0.038 0.991, valid loss/acc: 0.094 0.972\n","Combination 110 - lr: 1.412, n_epochs: 15, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.247 0.929, valid loss/acc: 0.252 0.925\n","epoch 01 - train loss/acc: 0.179 0.948, valid loss/acc: 0.192 0.944\n","epoch 02 - train loss/acc: 0.139 0.960, valid loss/acc: 0.159 0.954\n","epoch 03 - train loss/acc: 0.111 0.968, valid loss/acc: 0.134 0.960\n","epoch 04 - train loss/acc: 0.094 0.973, valid loss/acc: 0.119 0.966\n","epoch 05 - train loss/acc: 0.081 0.977, valid loss/acc: 0.109 0.966\n","epoch 06 - train loss/acc: 0.071 0.980, valid loss/acc: 0.103 0.968\n","epoch 07 - train loss/acc: 0.066 0.982, valid loss/acc: 0.102 0.969\n","epoch 08 - train loss/acc: 0.055 0.986, valid loss/acc: 0.091 0.972\n","epoch 09 - train loss/acc: 0.052 0.986, valid loss/acc: 0.093 0.972\n","epoch 10 - train loss/acc: 0.044 0.989, valid loss/acc: 0.087 0.973\n","epoch 11 - train loss/acc: 0.040 0.990, valid loss/acc: 0.085 0.974\n","epoch 12 - train loss/acc: 0.035 0.992, valid loss/acc: 0.082 0.975\n","epoch 13 - train loss/acc: 0.037 0.991, valid loss/acc: 0.087 0.973\n","epoch 14 - train loss/acc: 0.030 0.994, valid loss/acc: 0.081 0.975\n","Combination 111 - lr: 1.412, n_epochs: 15, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.322 0.906, valid loss/acc: 0.328 0.904\n","epoch 01 - train loss/acc: 0.246 0.929, valid loss/acc: 0.254 0.928\n","epoch 02 - train loss/acc: 0.210 0.940, valid loss/acc: 0.222 0.938\n","epoch 03 - train loss/acc: 0.187 0.948, valid loss/acc: 0.203 0.942\n","epoch 04 - train loss/acc: 0.170 0.951, valid loss/acc: 0.190 0.947\n","epoch 05 - train loss/acc: 0.155 0.956, valid loss/acc: 0.176 0.949\n","epoch 06 - train loss/acc: 0.143 0.960, valid loss/acc: 0.167 0.952\n","epoch 07 - train loss/acc: 0.139 0.960, valid loss/acc: 0.167 0.953\n","epoch 08 - train loss/acc: 0.127 0.965, valid loss/acc: 0.157 0.955\n","epoch 09 - train loss/acc: 0.124 0.964, valid loss/acc: 0.157 0.953\n","epoch 10 - train loss/acc: 0.117 0.967, valid loss/acc: 0.151 0.955\n","epoch 11 - train loss/acc: 0.110 0.969, valid loss/acc: 0.146 0.956\n","epoch 12 - train loss/acc: 0.103 0.972, valid loss/acc: 0.140 0.958\n","epoch 13 - train loss/acc: 0.101 0.972, valid loss/acc: 0.140 0.958\n","epoch 14 - train loss/acc: 0.098 0.973, valid loss/acc: 0.138 0.959\n","Combination 112 - lr: 1.412, n_epochs: 15, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.299 0.914, valid loss/acc: 0.306 0.911\n","epoch 01 - train loss/acc: 0.240 0.931, valid loss/acc: 0.249 0.928\n","epoch 02 - train loss/acc: 0.201 0.942, valid loss/acc: 0.215 0.936\n","epoch 03 - train loss/acc: 0.173 0.950, valid loss/acc: 0.189 0.945\n","epoch 04 - train loss/acc: 0.156 0.954, valid loss/acc: 0.176 0.947\n","epoch 05 - train loss/acc: 0.138 0.961, valid loss/acc: 0.160 0.953\n","epoch 06 - train loss/acc: 0.131 0.963, valid loss/acc: 0.157 0.955\n","epoch 07 - train loss/acc: 0.115 0.968, valid loss/acc: 0.142 0.959\n","epoch 08 - train loss/acc: 0.105 0.971, valid loss/acc: 0.135 0.960\n","epoch 09 - train loss/acc: 0.097 0.974, valid loss/acc: 0.128 0.962\n","epoch 10 - train loss/acc: 0.090 0.975, valid loss/acc: 0.124 0.963\n","epoch 11 - train loss/acc: 0.085 0.977, valid loss/acc: 0.119 0.966\n","epoch 12 - train loss/acc: 0.079 0.978, valid loss/acc: 0.116 0.965\n","epoch 13 - train loss/acc: 0.077 0.979, valid loss/acc: 0.114 0.965\n","epoch 14 - train loss/acc: 0.072 0.981, valid loss/acc: 0.110 0.967\n","Combination 113 - lr: 1.412, n_epochs: 15, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.338 0.896, valid loss/acc: 0.343 0.894\n","epoch 01 - train loss/acc: 0.245 0.929, valid loss/acc: 0.251 0.925\n","epoch 02 - train loss/acc: 0.212 0.937, valid loss/acc: 0.223 0.933\n","epoch 03 - train loss/acc: 0.172 0.950, valid loss/acc: 0.186 0.946\n","epoch 04 - train loss/acc: 0.152 0.956, valid loss/acc: 0.168 0.951\n","epoch 05 - train loss/acc: 0.136 0.961, valid loss/acc: 0.154 0.956\n","epoch 06 - train loss/acc: 0.120 0.966, valid loss/acc: 0.141 0.958\n","epoch 07 - train loss/acc: 0.111 0.969, valid loss/acc: 0.134 0.961\n","epoch 08 - train loss/acc: 0.102 0.970, valid loss/acc: 0.128 0.963\n","epoch 09 - train loss/acc: 0.091 0.975, valid loss/acc: 0.118 0.965\n","epoch 10 - train loss/acc: 0.082 0.977, valid loss/acc: 0.110 0.968\n","epoch 11 - train loss/acc: 0.076 0.979, valid loss/acc: 0.106 0.970\n","epoch 12 - train loss/acc: 0.070 0.981, valid loss/acc: 0.103 0.970\n","epoch 13 - train loss/acc: 0.066 0.982, valid loss/acc: 0.099 0.971\n","epoch 14 - train loss/acc: 0.061 0.984, valid loss/acc: 0.096 0.972\n","Combination 114 - lr: 1.412, n_epochs: 15, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.411 0.889, valid loss/acc: 0.414 0.887\n","epoch 01 - train loss/acc: 0.304 0.915, valid loss/acc: 0.310 0.912\n","epoch 02 - train loss/acc: 0.267 0.924, valid loss/acc: 0.274 0.921\n","epoch 03 - train loss/acc: 0.241 0.930, valid loss/acc: 0.251 0.927\n","epoch 04 - train loss/acc: 0.219 0.937, valid loss/acc: 0.230 0.933\n","epoch 05 - train loss/acc: 0.205 0.942, valid loss/acc: 0.217 0.938\n","epoch 06 - train loss/acc: 0.194 0.944, valid loss/acc: 0.209 0.939\n","epoch 07 - train loss/acc: 0.183 0.948, valid loss/acc: 0.200 0.941\n","epoch 08 - train loss/acc: 0.172 0.951, valid loss/acc: 0.188 0.946\n","epoch 09 - train loss/acc: 0.165 0.953, valid loss/acc: 0.184 0.947\n","epoch 10 - train loss/acc: 0.158 0.955, valid loss/acc: 0.177 0.948\n","epoch 11 - train loss/acc: 0.153 0.956, valid loss/acc: 0.173 0.949\n","epoch 12 - train loss/acc: 0.147 0.958, valid loss/acc: 0.169 0.951\n","epoch 13 - train loss/acc: 0.139 0.960, valid loss/acc: 0.162 0.953\n","epoch 14 - train loss/acc: 0.137 0.961, valid loss/acc: 0.162 0.954\n","Combination 115 - lr: 1.412, n_epochs: 15, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.410 0.885, valid loss/acc: 0.413 0.881\n","epoch 01 - train loss/acc: 0.306 0.911, valid loss/acc: 0.312 0.908\n","epoch 02 - train loss/acc: 0.266 0.923, valid loss/acc: 0.273 0.919\n","epoch 03 - train loss/acc: 0.242 0.930, valid loss/acc: 0.250 0.926\n","epoch 04 - train loss/acc: 0.218 0.938, valid loss/acc: 0.228 0.934\n","epoch 05 - train loss/acc: 0.200 0.943, valid loss/acc: 0.212 0.939\n","epoch 06 - train loss/acc: 0.184 0.948, valid loss/acc: 0.197 0.944\n","epoch 07 - train loss/acc: 0.175 0.949, valid loss/acc: 0.189 0.946\n","epoch 08 - train loss/acc: 0.167 0.952, valid loss/acc: 0.183 0.948\n","epoch 09 - train loss/acc: 0.153 0.956, valid loss/acc: 0.170 0.951\n","epoch 10 - train loss/acc: 0.143 0.959, valid loss/acc: 0.162 0.953\n","epoch 11 - train loss/acc: 0.138 0.961, valid loss/acc: 0.158 0.955\n","epoch 12 - train loss/acc: 0.129 0.963, valid loss/acc: 0.150 0.957\n","epoch 13 - train loss/acc: 0.123 0.965, valid loss/acc: 0.145 0.958\n","epoch 14 - train loss/acc: 0.118 0.966, valid loss/acc: 0.142 0.960\n","Combination 116 - lr: 1.412, n_epochs: 15, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.397 0.889, valid loss/acc: 0.401 0.889\n","epoch 01 - train loss/acc: 0.310 0.910, valid loss/acc: 0.317 0.906\n","epoch 02 - train loss/acc: 0.272 0.922, valid loss/acc: 0.279 0.918\n","epoch 03 - train loss/acc: 0.247 0.928, valid loss/acc: 0.256 0.926\n","epoch 04 - train loss/acc: 0.225 0.934, valid loss/acc: 0.234 0.933\n","epoch 05 - train loss/acc: 0.205 0.941, valid loss/acc: 0.216 0.937\n","epoch 06 - train loss/acc: 0.190 0.945, valid loss/acc: 0.202 0.941\n","epoch 07 - train loss/acc: 0.181 0.947, valid loss/acc: 0.195 0.943\n","epoch 08 - train loss/acc: 0.164 0.953, valid loss/acc: 0.179 0.949\n","epoch 09 - train loss/acc: 0.154 0.956, valid loss/acc: 0.170 0.949\n","epoch 10 - train loss/acc: 0.144 0.959, valid loss/acc: 0.163 0.952\n","epoch 11 - train loss/acc: 0.135 0.962, valid loss/acc: 0.154 0.956\n","epoch 12 - train loss/acc: 0.128 0.964, valid loss/acc: 0.148 0.957\n","epoch 13 - train loss/acc: 0.120 0.966, valid loss/acc: 0.140 0.959\n","epoch 14 - train loss/acc: 0.115 0.968, valid loss/acc: 0.136 0.960\n","Combination 117 - lr: 1.412, n_epochs: 20, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.248 0.928, valid loss/acc: 0.259 0.924\n","epoch 01 - train loss/acc: 0.187 0.946, valid loss/acc: 0.204 0.941\n","epoch 02 - train loss/acc: 0.158 0.954, valid loss/acc: 0.180 0.946\n","epoch 03 - train loss/acc: 0.141 0.959, valid loss/acc: 0.167 0.950\n","epoch 04 - train loss/acc: 0.122 0.965, valid loss/acc: 0.151 0.955\n","epoch 05 - train loss/acc: 0.110 0.969, valid loss/acc: 0.144 0.957\n","epoch 06 - train loss/acc: 0.102 0.971, valid loss/acc: 0.139 0.957\n","epoch 07 - train loss/acc: 0.097 0.973, valid loss/acc: 0.138 0.959\n","epoch 08 - train loss/acc: 0.086 0.975, valid loss/acc: 0.132 0.960\n","epoch 09 - train loss/acc: 0.080 0.977, valid loss/acc: 0.131 0.962\n","epoch 10 - train loss/acc: 0.078 0.978, valid loss/acc: 0.129 0.962\n","epoch 11 - train loss/acc: 0.076 0.979, valid loss/acc: 0.132 0.961\n","epoch 12 - train loss/acc: 0.064 0.983, valid loss/acc: 0.122 0.966\n","epoch 13 - train loss/acc: 0.063 0.983, valid loss/acc: 0.123 0.963\n","epoch 14 - train loss/acc: 0.062 0.984, valid loss/acc: 0.126 0.964\n","epoch 15 - train loss/acc: 0.056 0.985, valid loss/acc: 0.124 0.964\n","epoch 16 - train loss/acc: 0.056 0.985, valid loss/acc: 0.124 0.965\n","epoch 17 - train loss/acc: 0.057 0.985, valid loss/acc: 0.130 0.962\n","epoch 18 - train loss/acc: 0.050 0.988, valid loss/acc: 0.126 0.962\n","epoch 19 - train loss/acc: 0.050 0.986, valid loss/acc: 0.127 0.962\n","Combination 118 - lr: 1.412, n_epochs: 20, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.246 0.929, valid loss/acc: 0.253 0.925\n","epoch 01 - train loss/acc: 0.177 0.948, valid loss/acc: 0.190 0.943\n","epoch 02 - train loss/acc: 0.152 0.955, valid loss/acc: 0.167 0.951\n","epoch 03 - train loss/acc: 0.115 0.967, valid loss/acc: 0.136 0.959\n","epoch 04 - train loss/acc: 0.097 0.972, valid loss/acc: 0.124 0.963\n","epoch 05 - train loss/acc: 0.091 0.974, valid loss/acc: 0.126 0.962\n","epoch 06 - train loss/acc: 0.076 0.978, valid loss/acc: 0.112 0.966\n","epoch 07 - train loss/acc: 0.066 0.982, valid loss/acc: 0.105 0.968\n","epoch 08 - train loss/acc: 0.059 0.984, valid loss/acc: 0.103 0.969\n","epoch 09 - train loss/acc: 0.057 0.984, valid loss/acc: 0.103 0.969\n","epoch 10 - train loss/acc: 0.050 0.987, valid loss/acc: 0.097 0.970\n","epoch 11 - train loss/acc: 0.045 0.989, valid loss/acc: 0.095 0.971\n","epoch 12 - train loss/acc: 0.044 0.989, valid loss/acc: 0.095 0.972\n","epoch 13 - train loss/acc: 0.040 0.991, valid loss/acc: 0.093 0.971\n","epoch 14 - train loss/acc: 0.035 0.992, valid loss/acc: 0.091 0.972\n","epoch 15 - train loss/acc: 0.034 0.992, valid loss/acc: 0.093 0.973\n","epoch 16 - train loss/acc: 0.031 0.993, valid loss/acc: 0.091 0.972\n","epoch 17 - train loss/acc: 0.028 0.994, valid loss/acc: 0.090 0.973\n","epoch 18 - train loss/acc: 0.026 0.995, valid loss/acc: 0.090 0.973\n","epoch 19 - train loss/acc: 0.024 0.995, valid loss/acc: 0.090 0.973\n","Combination 119 - lr: 1.412, n_epochs: 20, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.250 0.927, valid loss/acc: 0.258 0.925\n","epoch 01 - train loss/acc: 0.186 0.946, valid loss/acc: 0.203 0.939\n","epoch 02 - train loss/acc: 0.147 0.957, valid loss/acc: 0.166 0.951\n","epoch 03 - train loss/acc: 0.116 0.967, valid loss/acc: 0.140 0.959\n","epoch 04 - train loss/acc: 0.097 0.973, valid loss/acc: 0.124 0.962\n","epoch 05 - train loss/acc: 0.083 0.977, valid loss/acc: 0.115 0.965\n","epoch 06 - train loss/acc: 0.072 0.980, valid loss/acc: 0.107 0.967\n","epoch 07 - train loss/acc: 0.062 0.983, valid loss/acc: 0.100 0.969\n","epoch 08 - train loss/acc: 0.059 0.984, valid loss/acc: 0.099 0.969\n","epoch 09 - train loss/acc: 0.053 0.986, valid loss/acc: 0.094 0.971\n","epoch 10 - train loss/acc: 0.046 0.988, valid loss/acc: 0.091 0.973\n","epoch 11 - train loss/acc: 0.042 0.990, valid loss/acc: 0.090 0.972\n","epoch 12 - train loss/acc: 0.038 0.990, valid loss/acc: 0.088 0.974\n","epoch 13 - train loss/acc: 0.034 0.992, valid loss/acc: 0.085 0.974\n","epoch 14 - train loss/acc: 0.031 0.993, valid loss/acc: 0.084 0.974\n","epoch 15 - train loss/acc: 0.028 0.994, valid loss/acc: 0.082 0.975\n","epoch 16 - train loss/acc: 0.024 0.996, valid loss/acc: 0.080 0.975\n","epoch 17 - train loss/acc: 0.023 0.996, valid loss/acc: 0.081 0.977\n","epoch 18 - train loss/acc: 0.021 0.996, valid loss/acc: 0.081 0.976\n","epoch 19 - train loss/acc: 0.019 0.997, valid loss/acc: 0.081 0.976\n","Combination 120 - lr: 1.412, n_epochs: 20, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.319 0.910, valid loss/acc: 0.322 0.907\n","epoch 01 - train loss/acc: 0.241 0.931, valid loss/acc: 0.252 0.928\n","epoch 02 - train loss/acc: 0.205 0.942, valid loss/acc: 0.218 0.938\n","epoch 03 - train loss/acc: 0.180 0.948, valid loss/acc: 0.197 0.944\n","epoch 04 - train loss/acc: 0.165 0.953, valid loss/acc: 0.187 0.946\n","epoch 05 - train loss/acc: 0.152 0.956, valid loss/acc: 0.176 0.949\n","epoch 06 - train loss/acc: 0.145 0.958, valid loss/acc: 0.172 0.949\n","epoch 07 - train loss/acc: 0.130 0.963, valid loss/acc: 0.161 0.954\n","epoch 08 - train loss/acc: 0.122 0.966, valid loss/acc: 0.158 0.954\n","epoch 09 - train loss/acc: 0.114 0.969, valid loss/acc: 0.150 0.956\n","epoch 10 - train loss/acc: 0.109 0.970, valid loss/acc: 0.148 0.956\n","epoch 11 - train loss/acc: 0.102 0.972, valid loss/acc: 0.143 0.958\n","epoch 12 - train loss/acc: 0.101 0.972, valid loss/acc: 0.144 0.958\n","epoch 13 - train loss/acc: 0.095 0.974, valid loss/acc: 0.140 0.958\n","epoch 14 - train loss/acc: 0.089 0.975, valid loss/acc: 0.137 0.958\n","epoch 15 - train loss/acc: 0.087 0.976, valid loss/acc: 0.134 0.959\n","epoch 16 - train loss/acc: 0.083 0.977, valid loss/acc: 0.133 0.959\n","epoch 17 - train loss/acc: 0.079 0.979, valid loss/acc: 0.133 0.960\n","epoch 18 - train loss/acc: 0.076 0.980, valid loss/acc: 0.130 0.961\n","epoch 19 - train loss/acc: 0.074 0.980, valid loss/acc: 0.129 0.962\n","Combination 121 - lr: 1.412, n_epochs: 20, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.317 0.906, valid loss/acc: 0.322 0.904\n","epoch 01 - train loss/acc: 0.237 0.932, valid loss/acc: 0.248 0.929\n","epoch 02 - train loss/acc: 0.196 0.944, valid loss/acc: 0.206 0.940\n","epoch 03 - train loss/acc: 0.169 0.952, valid loss/acc: 0.183 0.946\n","epoch 04 - train loss/acc: 0.150 0.958, valid loss/acc: 0.166 0.951\n","epoch 05 - train loss/acc: 0.139 0.960, valid loss/acc: 0.157 0.954\n","epoch 06 - train loss/acc: 0.122 0.966, valid loss/acc: 0.144 0.959\n","epoch 07 - train loss/acc: 0.113 0.968, valid loss/acc: 0.136 0.959\n","epoch 08 - train loss/acc: 0.105 0.971, valid loss/acc: 0.131 0.962\n","epoch 09 - train loss/acc: 0.096 0.973, valid loss/acc: 0.124 0.963\n","epoch 10 - train loss/acc: 0.091 0.975, valid loss/acc: 0.121 0.965\n","epoch 11 - train loss/acc: 0.082 0.977, valid loss/acc: 0.114 0.965\n","epoch 12 - train loss/acc: 0.078 0.979, valid loss/acc: 0.111 0.968\n","epoch 13 - train loss/acc: 0.074 0.980, valid loss/acc: 0.110 0.968\n","epoch 14 - train loss/acc: 0.068 0.982, valid loss/acc: 0.105 0.968\n","epoch 15 - train loss/acc: 0.066 0.982, valid loss/acc: 0.104 0.969\n","epoch 16 - train loss/acc: 0.061 0.984, valid loss/acc: 0.101 0.969\n","epoch 17 - train loss/acc: 0.057 0.985, valid loss/acc: 0.099 0.970\n","epoch 18 - train loss/acc: 0.055 0.986, valid loss/acc: 0.098 0.971\n","epoch 19 - train loss/acc: 0.052 0.987, valid loss/acc: 0.098 0.971\n","Combination 122 - lr: 1.412, n_epochs: 20, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.309 0.910, valid loss/acc: 0.315 0.907\n","epoch 01 - train loss/acc: 0.245 0.929, valid loss/acc: 0.255 0.926\n","epoch 02 - train loss/acc: 0.204 0.941, valid loss/acc: 0.218 0.938\n","epoch 03 - train loss/acc: 0.173 0.950, valid loss/acc: 0.188 0.946\n","epoch 04 - train loss/acc: 0.160 0.952, valid loss/acc: 0.177 0.948\n","epoch 05 - train loss/acc: 0.135 0.961, valid loss/acc: 0.156 0.954\n","epoch 06 - train loss/acc: 0.119 0.967, valid loss/acc: 0.142 0.958\n","epoch 07 - train loss/acc: 0.106 0.970, valid loss/acc: 0.132 0.961\n","epoch 08 - train loss/acc: 0.098 0.973, valid loss/acc: 0.126 0.963\n","epoch 09 - train loss/acc: 0.092 0.974, valid loss/acc: 0.122 0.963\n","epoch 10 - train loss/acc: 0.082 0.977, valid loss/acc: 0.113 0.967\n","epoch 11 - train loss/acc: 0.079 0.978, valid loss/acc: 0.111 0.968\n","epoch 12 - train loss/acc: 0.071 0.980, valid loss/acc: 0.106 0.968\n","epoch 13 - train loss/acc: 0.066 0.982, valid loss/acc: 0.100 0.969\n","epoch 14 - train loss/acc: 0.061 0.983, valid loss/acc: 0.097 0.970\n","epoch 15 - train loss/acc: 0.058 0.985, valid loss/acc: 0.096 0.971\n","epoch 16 - train loss/acc: 0.054 0.986, valid loss/acc: 0.093 0.972\n","epoch 17 - train loss/acc: 0.050 0.987, valid loss/acc: 0.091 0.973\n","epoch 18 - train loss/acc: 0.047 0.988, valid loss/acc: 0.090 0.973\n","epoch 19 - train loss/acc: 0.047 0.988, valid loss/acc: 0.090 0.973\n","Combination 123 - lr: 1.412, n_epochs: 20, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.417 0.893, valid loss/acc: 0.424 0.889\n","epoch 01 - train loss/acc: 0.309 0.915, valid loss/acc: 0.318 0.911\n","epoch 02 - train loss/acc: 0.273 0.924, valid loss/acc: 0.282 0.921\n","epoch 03 - train loss/acc: 0.242 0.931, valid loss/acc: 0.253 0.929\n","epoch 04 - train loss/acc: 0.221 0.938, valid loss/acc: 0.233 0.936\n","epoch 05 - train loss/acc: 0.209 0.940, valid loss/acc: 0.222 0.936\n","epoch 06 - train loss/acc: 0.194 0.945, valid loss/acc: 0.209 0.942\n","epoch 07 - train loss/acc: 0.182 0.949, valid loss/acc: 0.198 0.943\n","epoch 08 - train loss/acc: 0.176 0.950, valid loss/acc: 0.194 0.945\n","epoch 09 - train loss/acc: 0.163 0.954, valid loss/acc: 0.183 0.947\n","epoch 10 - train loss/acc: 0.158 0.955, valid loss/acc: 0.180 0.947\n","epoch 11 - train loss/acc: 0.151 0.958, valid loss/acc: 0.174 0.950\n","epoch 12 - train loss/acc: 0.146 0.958, valid loss/acc: 0.171 0.950\n","epoch 13 - train loss/acc: 0.141 0.959, valid loss/acc: 0.167 0.951\n","epoch 14 - train loss/acc: 0.136 0.962, valid loss/acc: 0.164 0.952\n","epoch 15 - train loss/acc: 0.129 0.963, valid loss/acc: 0.158 0.954\n","epoch 16 - train loss/acc: 0.127 0.965, valid loss/acc: 0.157 0.954\n","epoch 17 - train loss/acc: 0.123 0.965, valid loss/acc: 0.155 0.955\n","epoch 18 - train loss/acc: 0.118 0.967, valid loss/acc: 0.151 0.956\n","epoch 19 - train loss/acc: 0.115 0.968, valid loss/acc: 0.149 0.957\n","Combination 124 - lr: 1.412, n_epochs: 20, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.398 0.884, valid loss/acc: 0.403 0.880\n","epoch 01 - train loss/acc: 0.305 0.913, valid loss/acc: 0.311 0.909\n","epoch 02 - train loss/acc: 0.265 0.924, valid loss/acc: 0.274 0.920\n","epoch 03 - train loss/acc: 0.242 0.929, valid loss/acc: 0.251 0.927\n","epoch 04 - train loss/acc: 0.217 0.937, valid loss/acc: 0.227 0.933\n","epoch 05 - train loss/acc: 0.200 0.943, valid loss/acc: 0.214 0.937\n","epoch 06 - train loss/acc: 0.188 0.946, valid loss/acc: 0.204 0.939\n","epoch 07 - train loss/acc: 0.177 0.949, valid loss/acc: 0.193 0.945\n","epoch 08 - train loss/acc: 0.161 0.953, valid loss/acc: 0.179 0.948\n","epoch 09 - train loss/acc: 0.153 0.956, valid loss/acc: 0.171 0.949\n","epoch 10 - train loss/acc: 0.147 0.958, valid loss/acc: 0.168 0.951\n","epoch 11 - train loss/acc: 0.138 0.961, valid loss/acc: 0.158 0.954\n","epoch 12 - train loss/acc: 0.131 0.963, valid loss/acc: 0.153 0.954\n","epoch 13 - train loss/acc: 0.125 0.965, valid loss/acc: 0.149 0.956\n","epoch 14 - train loss/acc: 0.119 0.966, valid loss/acc: 0.145 0.958\n","epoch 15 - train loss/acc: 0.116 0.968, valid loss/acc: 0.142 0.958\n","epoch 16 - train loss/acc: 0.111 0.969, valid loss/acc: 0.138 0.960\n","epoch 17 - train loss/acc: 0.109 0.969, valid loss/acc: 0.137 0.959\n","epoch 18 - train loss/acc: 0.102 0.972, valid loss/acc: 0.131 0.962\n","epoch 19 - train loss/acc: 0.098 0.973, valid loss/acc: 0.128 0.963\n","Combination 125 - lr: 1.412, n_epochs: 20, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.442 0.860, valid loss/acc: 0.446 0.854\n","epoch 01 - train loss/acc: 0.309 0.910, valid loss/acc: 0.315 0.907\n","epoch 02 - train loss/acc: 0.269 0.922, valid loss/acc: 0.276 0.918\n","epoch 03 - train loss/acc: 0.247 0.929, valid loss/acc: 0.256 0.926\n","epoch 04 - train loss/acc: 0.222 0.936, valid loss/acc: 0.232 0.933\n","epoch 05 - train loss/acc: 0.205 0.940, valid loss/acc: 0.217 0.937\n","epoch 06 - train loss/acc: 0.188 0.946, valid loss/acc: 0.203 0.941\n","epoch 07 - train loss/acc: 0.174 0.949, valid loss/acc: 0.188 0.945\n","epoch 08 - train loss/acc: 0.162 0.953, valid loss/acc: 0.178 0.947\n","epoch 09 - train loss/acc: 0.151 0.957, valid loss/acc: 0.169 0.950\n","epoch 10 - train loss/acc: 0.140 0.960, valid loss/acc: 0.159 0.953\n","epoch 11 - train loss/acc: 0.132 0.962, valid loss/acc: 0.153 0.955\n","epoch 12 - train loss/acc: 0.125 0.965, valid loss/acc: 0.147 0.957\n","epoch 13 - train loss/acc: 0.118 0.967, valid loss/acc: 0.141 0.958\n","epoch 14 - train loss/acc: 0.116 0.968, valid loss/acc: 0.140 0.959\n","epoch 15 - train loss/acc: 0.109 0.969, valid loss/acc: 0.135 0.960\n","epoch 16 - train loss/acc: 0.102 0.972, valid loss/acc: 0.128 0.962\n","epoch 17 - train loss/acc: 0.098 0.973, valid loss/acc: 0.126 0.963\n","epoch 18 - train loss/acc: 0.093 0.974, valid loss/acc: 0.121 0.965\n","epoch 19 - train loss/acc: 0.089 0.976, valid loss/acc: 0.119 0.965\n","Combination 126 - lr: 1.412, n_epochs: 25, batch_size: 128, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.270 0.923, valid loss/acc: 0.277 0.919\n","epoch 01 - train loss/acc: 0.185 0.948, valid loss/acc: 0.199 0.944\n","epoch 02 - train loss/acc: 0.155 0.956, valid loss/acc: 0.178 0.950\n","epoch 03 - train loss/acc: 0.134 0.962, valid loss/acc: 0.159 0.955\n","epoch 04 - train loss/acc: 0.121 0.965, valid loss/acc: 0.151 0.956\n","epoch 05 - train loss/acc: 0.114 0.968, valid loss/acc: 0.149 0.957\n","epoch 06 - train loss/acc: 0.108 0.969, valid loss/acc: 0.147 0.956\n","epoch 07 - train loss/acc: 0.101 0.970, valid loss/acc: 0.145 0.957\n","epoch 08 - train loss/acc: 0.090 0.975, valid loss/acc: 0.137 0.959\n","epoch 09 - train loss/acc: 0.086 0.975, valid loss/acc: 0.138 0.958\n","epoch 10 - train loss/acc: 0.077 0.979, valid loss/acc: 0.130 0.961\n","epoch 11 - train loss/acc: 0.076 0.979, valid loss/acc: 0.129 0.961\n","epoch 12 - train loss/acc: 0.073 0.979, valid loss/acc: 0.130 0.961\n","epoch 13 - train loss/acc: 0.068 0.981, valid loss/acc: 0.131 0.960\n","epoch 14 - train loss/acc: 0.066 0.982, valid loss/acc: 0.129 0.961\n","epoch 15 - train loss/acc: 0.064 0.982, valid loss/acc: 0.126 0.963\n","epoch 16 - train loss/acc: 0.056 0.986, valid loss/acc: 0.121 0.964\n","epoch 17 - train loss/acc: 0.056 0.985, valid loss/acc: 0.127 0.961\n","epoch 18 - train loss/acc: 0.051 0.987, valid loss/acc: 0.124 0.963\n","epoch 19 - train loss/acc: 0.055 0.985, valid loss/acc: 0.126 0.961\n","epoch 20 - train loss/acc: 0.048 0.988, valid loss/acc: 0.122 0.964\n","epoch 21 - train loss/acc: 0.048 0.988, valid loss/acc: 0.124 0.963\n","epoch 22 - train loss/acc: 0.045 0.989, valid loss/acc: 0.123 0.965\n","epoch 23 - train loss/acc: 0.043 0.989, valid loss/acc: 0.122 0.962\n","epoch 24 - train loss/acc: 0.040 0.990, valid loss/acc: 0.127 0.963\n","Combination 127 - lr: 1.412, n_epochs: 25, batch_size: 128, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.239 0.930, valid loss/acc: 0.246 0.928\n","epoch 01 - train loss/acc: 0.180 0.948, valid loss/acc: 0.194 0.943\n","epoch 02 - train loss/acc: 0.146 0.957, valid loss/acc: 0.164 0.950\n","epoch 03 - train loss/acc: 0.116 0.967, valid loss/acc: 0.139 0.958\n","epoch 04 - train loss/acc: 0.106 0.969, valid loss/acc: 0.133 0.961\n","epoch 05 - train loss/acc: 0.088 0.975, valid loss/acc: 0.120 0.964\n","epoch 06 - train loss/acc: 0.076 0.979, valid loss/acc: 0.112 0.965\n","epoch 07 - train loss/acc: 0.071 0.980, valid loss/acc: 0.108 0.968\n","epoch 08 - train loss/acc: 0.066 0.981, valid loss/acc: 0.109 0.967\n","epoch 09 - train loss/acc: 0.056 0.985, valid loss/acc: 0.099 0.970\n","epoch 10 - train loss/acc: 0.052 0.986, valid loss/acc: 0.100 0.969\n","epoch 11 - train loss/acc: 0.046 0.988, valid loss/acc: 0.094 0.971\n","epoch 12 - train loss/acc: 0.042 0.989, valid loss/acc: 0.096 0.970\n","epoch 13 - train loss/acc: 0.040 0.990, valid loss/acc: 0.094 0.971\n","epoch 14 - train loss/acc: 0.036 0.991, valid loss/acc: 0.093 0.971\n","epoch 15 - train loss/acc: 0.033 0.993, valid loss/acc: 0.093 0.972\n","epoch 16 - train loss/acc: 0.031 0.993, valid loss/acc: 0.092 0.972\n","epoch 17 - train loss/acc: 0.028 0.994, valid loss/acc: 0.091 0.974\n","epoch 18 - train loss/acc: 0.028 0.994, valid loss/acc: 0.093 0.972\n","epoch 19 - train loss/acc: 0.027 0.994, valid loss/acc: 0.097 0.971\n","epoch 20 - train loss/acc: 0.022 0.996, valid loss/acc: 0.091 0.973\n","epoch 21 - train loss/acc: 0.022 0.996, valid loss/acc: 0.094 0.972\n","epoch 22 - train loss/acc: 0.020 0.997, valid loss/acc: 0.091 0.974\n","epoch 23 - train loss/acc: 0.019 0.997, valid loss/acc: 0.091 0.973\n","epoch 24 - train loss/acc: 0.018 0.997, valid loss/acc: 0.091 0.974\n","Combination 128 - lr: 1.412, n_epochs: 25, batch_size: 128, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.269 0.923, valid loss/acc: 0.282 0.917\n","epoch 01 - train loss/acc: 0.184 0.946, valid loss/acc: 0.201 0.941\n","epoch 02 - train loss/acc: 0.148 0.958, valid loss/acc: 0.165 0.952\n","epoch 03 - train loss/acc: 0.124 0.964, valid loss/acc: 0.148 0.956\n","epoch 04 - train loss/acc: 0.111 0.967, valid loss/acc: 0.140 0.958\n","epoch 05 - train loss/acc: 0.093 0.973, valid loss/acc: 0.121 0.962\n","epoch 06 - train loss/acc: 0.079 0.978, valid loss/acc: 0.109 0.966\n","epoch 07 - train loss/acc: 0.071 0.980, valid loss/acc: 0.108 0.967\n","epoch 08 - train loss/acc: 0.063 0.983, valid loss/acc: 0.101 0.970\n","epoch 09 - train loss/acc: 0.057 0.985, valid loss/acc: 0.097 0.970\n","epoch 10 - train loss/acc: 0.053 0.986, valid loss/acc: 0.095 0.971\n","epoch 11 - train loss/acc: 0.049 0.987, valid loss/acc: 0.096 0.969\n","epoch 12 - train loss/acc: 0.044 0.989, valid loss/acc: 0.092 0.972\n","epoch 13 - train loss/acc: 0.041 0.990, valid loss/acc: 0.092 0.972\n","epoch 14 - train loss/acc: 0.038 0.991, valid loss/acc: 0.090 0.971\n","epoch 15 - train loss/acc: 0.035 0.991, valid loss/acc: 0.088 0.973\n","epoch 16 - train loss/acc: 0.031 0.993, valid loss/acc: 0.086 0.974\n","epoch 17 - train loss/acc: 0.029 0.993, valid loss/acc: 0.088 0.973\n","epoch 18 - train loss/acc: 0.030 0.993, valid loss/acc: 0.089 0.973\n","epoch 19 - train loss/acc: 0.025 0.996, valid loss/acc: 0.085 0.975\n","epoch 20 - train loss/acc: 0.023 0.996, valid loss/acc: 0.084 0.975\n","epoch 21 - train loss/acc: 0.021 0.997, valid loss/acc: 0.083 0.975\n","epoch 22 - train loss/acc: 0.019 0.997, valid loss/acc: 0.084 0.974\n","epoch 23 - train loss/acc: 0.019 0.997, valid loss/acc: 0.085 0.974\n","epoch 24 - train loss/acc: 0.018 0.997, valid loss/acc: 0.086 0.974\n","Combination 129 - lr: 1.412, n_epochs: 25, batch_size: 256, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.323 0.908, valid loss/acc: 0.329 0.903\n","epoch 01 - train loss/acc: 0.249 0.927, valid loss/acc: 0.259 0.925\n","epoch 02 - train loss/acc: 0.215 0.938, valid loss/acc: 0.231 0.932\n","epoch 03 - train loss/acc: 0.186 0.947, valid loss/acc: 0.204 0.940\n","epoch 04 - train loss/acc: 0.173 0.950, valid loss/acc: 0.195 0.943\n","epoch 05 - train loss/acc: 0.153 0.956, valid loss/acc: 0.178 0.948\n","epoch 06 - train loss/acc: 0.145 0.958, valid loss/acc: 0.172 0.949\n","epoch 07 - train loss/acc: 0.134 0.962, valid loss/acc: 0.165 0.951\n","epoch 08 - train loss/acc: 0.125 0.964, valid loss/acc: 0.155 0.954\n","epoch 09 - train loss/acc: 0.121 0.966, valid loss/acc: 0.154 0.955\n","epoch 10 - train loss/acc: 0.115 0.967, valid loss/acc: 0.152 0.956\n","epoch 11 - train loss/acc: 0.107 0.970, valid loss/acc: 0.146 0.957\n","epoch 12 - train loss/acc: 0.101 0.971, valid loss/acc: 0.141 0.958\n","epoch 13 - train loss/acc: 0.101 0.971, valid loss/acc: 0.143 0.958\n","epoch 14 - train loss/acc: 0.095 0.974, valid loss/acc: 0.138 0.959\n","epoch 15 - train loss/acc: 0.089 0.975, valid loss/acc: 0.135 0.960\n","epoch 16 - train loss/acc: 0.089 0.974, valid loss/acc: 0.137 0.960\n","epoch 17 - train loss/acc: 0.083 0.977, valid loss/acc: 0.131 0.962\n","epoch 18 - train loss/acc: 0.080 0.978, valid loss/acc: 0.131 0.962\n","epoch 19 - train loss/acc: 0.077 0.979, valid loss/acc: 0.131 0.962\n","epoch 20 - train loss/acc: 0.074 0.980, valid loss/acc: 0.128 0.963\n","epoch 21 - train loss/acc: 0.076 0.979, valid loss/acc: 0.133 0.961\n","epoch 22 - train loss/acc: 0.075 0.979, valid loss/acc: 0.133 0.961\n","epoch 23 - train loss/acc: 0.068 0.982, valid loss/acc: 0.129 0.963\n","epoch 24 - train loss/acc: 0.066 0.982, valid loss/acc: 0.127 0.963\n","Combination 130 - lr: 1.412, n_epochs: 25, batch_size: 256, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.305 0.914, valid loss/acc: 0.311 0.910\n","epoch 01 - train loss/acc: 0.242 0.929, valid loss/acc: 0.249 0.926\n","epoch 02 - train loss/acc: 0.203 0.941, valid loss/acc: 0.214 0.938\n","epoch 03 - train loss/acc: 0.171 0.952, valid loss/acc: 0.185 0.946\n","epoch 04 - train loss/acc: 0.152 0.956, valid loss/acc: 0.170 0.950\n","epoch 05 - train loss/acc: 0.141 0.960, valid loss/acc: 0.161 0.952\n","epoch 06 - train loss/acc: 0.121 0.966, valid loss/acc: 0.145 0.955\n","epoch 07 - train loss/acc: 0.114 0.967, valid loss/acc: 0.139 0.959\n","epoch 08 - train loss/acc: 0.105 0.971, valid loss/acc: 0.133 0.960\n","epoch 09 - train loss/acc: 0.096 0.974, valid loss/acc: 0.128 0.962\n","epoch 10 - train loss/acc: 0.092 0.974, valid loss/acc: 0.124 0.963\n","epoch 11 - train loss/acc: 0.082 0.977, valid loss/acc: 0.115 0.966\n","epoch 12 - train loss/acc: 0.077 0.980, valid loss/acc: 0.112 0.967\n","epoch 13 - train loss/acc: 0.073 0.981, valid loss/acc: 0.110 0.966\n","epoch 14 - train loss/acc: 0.070 0.981, valid loss/acc: 0.108 0.968\n","epoch 15 - train loss/acc: 0.064 0.983, valid loss/acc: 0.104 0.968\n","epoch 16 - train loss/acc: 0.062 0.983, valid loss/acc: 0.105 0.968\n","epoch 17 - train loss/acc: 0.058 0.985, valid loss/acc: 0.101 0.969\n","epoch 18 - train loss/acc: 0.055 0.986, valid loss/acc: 0.100 0.970\n","epoch 19 - train loss/acc: 0.053 0.987, valid loss/acc: 0.100 0.970\n","epoch 20 - train loss/acc: 0.051 0.987, valid loss/acc: 0.098 0.971\n","epoch 21 - train loss/acc: 0.047 0.989, valid loss/acc: 0.096 0.971\n","epoch 22 - train loss/acc: 0.045 0.989, valid loss/acc: 0.094 0.971\n","epoch 23 - train loss/acc: 0.043 0.990, valid loss/acc: 0.094 0.971\n","epoch 24 - train loss/acc: 0.042 0.990, valid loss/acc: 0.096 0.970\n","Combination 131 - lr: 1.412, n_epochs: 25, batch_size: 256, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.313 0.907, valid loss/acc: 0.320 0.904\n","epoch 01 - train loss/acc: 0.246 0.928, valid loss/acc: 0.256 0.924\n","epoch 02 - train loss/acc: 0.201 0.942, valid loss/acc: 0.211 0.938\n","epoch 03 - train loss/acc: 0.184 0.946, valid loss/acc: 0.196 0.941\n","epoch 04 - train loss/acc: 0.154 0.955, valid loss/acc: 0.171 0.949\n","epoch 05 - train loss/acc: 0.131 0.962, valid loss/acc: 0.150 0.954\n","epoch 06 - train loss/acc: 0.120 0.966, valid loss/acc: 0.141 0.957\n","epoch 07 - train loss/acc: 0.112 0.968, valid loss/acc: 0.133 0.961\n","epoch 08 - train loss/acc: 0.099 0.972, valid loss/acc: 0.124 0.962\n","epoch 09 - train loss/acc: 0.091 0.975, valid loss/acc: 0.119 0.964\n","epoch 10 - train loss/acc: 0.088 0.975, valid loss/acc: 0.117 0.964\n","epoch 11 - train loss/acc: 0.079 0.978, valid loss/acc: 0.110 0.967\n","epoch 12 - train loss/acc: 0.077 0.979, valid loss/acc: 0.113 0.966\n","epoch 13 - train loss/acc: 0.067 0.982, valid loss/acc: 0.101 0.970\n","epoch 14 - train loss/acc: 0.063 0.983, valid loss/acc: 0.098 0.970\n","epoch 15 - train loss/acc: 0.059 0.985, valid loss/acc: 0.096 0.971\n","epoch 16 - train loss/acc: 0.060 0.984, valid loss/acc: 0.100 0.971\n","epoch 17 - train loss/acc: 0.052 0.986, valid loss/acc: 0.092 0.973\n","epoch 18 - train loss/acc: 0.049 0.988, valid loss/acc: 0.090 0.973\n","epoch 19 - train loss/acc: 0.048 0.988, valid loss/acc: 0.090 0.972\n","epoch 20 - train loss/acc: 0.043 0.990, valid loss/acc: 0.087 0.973\n","epoch 21 - train loss/acc: 0.042 0.990, valid loss/acc: 0.087 0.973\n","epoch 22 - train loss/acc: 0.039 0.991, valid loss/acc: 0.086 0.974\n","epoch 23 - train loss/acc: 0.036 0.992, valid loss/acc: 0.084 0.975\n","epoch 24 - train loss/acc: 0.035 0.992, valid loss/acc: 0.083 0.975\n","Combination 132 - lr: 1.412, n_epochs: 25, batch_size: 512, num_hiddens: 32\n","epoch 00 - train loss/acc: 0.409 0.890, valid loss/acc: 0.413 0.888\n","epoch 01 - train loss/acc: 0.314 0.910, valid loss/acc: 0.318 0.907\n","epoch 02 - train loss/acc: 0.267 0.923, valid loss/acc: 0.273 0.922\n","epoch 03 - train loss/acc: 0.237 0.932, valid loss/acc: 0.245 0.930\n","epoch 04 - train loss/acc: 0.216 0.940, valid loss/acc: 0.226 0.936\n","epoch 05 - train loss/acc: 0.202 0.944, valid loss/acc: 0.214 0.938\n","epoch 06 - train loss/acc: 0.187 0.947, valid loss/acc: 0.200 0.942\n","epoch 07 - train loss/acc: 0.176 0.951, valid loss/acc: 0.191 0.944\n","epoch 08 - train loss/acc: 0.166 0.954, valid loss/acc: 0.183 0.948\n","epoch 09 - train loss/acc: 0.161 0.954, valid loss/acc: 0.179 0.949\n","epoch 10 - train loss/acc: 0.153 0.956, valid loss/acc: 0.172 0.950\n","epoch 11 - train loss/acc: 0.146 0.959, valid loss/acc: 0.166 0.952\n","epoch 12 - train loss/acc: 0.140 0.960, valid loss/acc: 0.161 0.952\n","epoch 13 - train loss/acc: 0.133 0.963, valid loss/acc: 0.156 0.955\n","epoch 14 - train loss/acc: 0.127 0.964, valid loss/acc: 0.152 0.956\n","epoch 15 - train loss/acc: 0.128 0.964, valid loss/acc: 0.152 0.957\n","epoch 16 - train loss/acc: 0.120 0.966, valid loss/acc: 0.147 0.957\n","epoch 17 - train loss/acc: 0.117 0.968, valid loss/acc: 0.144 0.959\n","epoch 18 - train loss/acc: 0.113 0.969, valid loss/acc: 0.142 0.958\n","epoch 19 - train loss/acc: 0.108 0.970, valid loss/acc: 0.138 0.959\n","epoch 20 - train loss/acc: 0.105 0.972, valid loss/acc: 0.137 0.960\n","epoch 21 - train loss/acc: 0.103 0.972, valid loss/acc: 0.136 0.961\n","epoch 22 - train loss/acc: 0.099 0.973, valid loss/acc: 0.132 0.962\n","epoch 23 - train loss/acc: 0.097 0.974, valid loss/acc: 0.131 0.962\n","epoch 24 - train loss/acc: 0.095 0.975, valid loss/acc: 0.130 0.962\n","Combination 133 - lr: 1.412, n_epochs: 25, batch_size: 512, num_hiddens: 64\n","epoch 00 - train loss/acc: 0.396 0.888, valid loss/acc: 0.403 0.882\n","epoch 01 - train loss/acc: 0.309 0.912, valid loss/acc: 0.318 0.906\n","epoch 02 - train loss/acc: 0.266 0.923, valid loss/acc: 0.275 0.920\n","epoch 03 - train loss/acc: 0.247 0.928, valid loss/acc: 0.257 0.925\n","epoch 04 - train loss/acc: 0.219 0.937, valid loss/acc: 0.231 0.934\n","epoch 05 - train loss/acc: 0.202 0.941, valid loss/acc: 0.216 0.938\n","epoch 06 - train loss/acc: 0.184 0.947, valid loss/acc: 0.200 0.942\n","epoch 07 - train loss/acc: 0.172 0.951, valid loss/acc: 0.188 0.945\n","epoch 08 - train loss/acc: 0.161 0.954, valid loss/acc: 0.180 0.948\n","epoch 09 - train loss/acc: 0.152 0.956, valid loss/acc: 0.173 0.948\n","epoch 10 - train loss/acc: 0.142 0.960, valid loss/acc: 0.165 0.952\n","epoch 11 - train loss/acc: 0.135 0.962, valid loss/acc: 0.158 0.954\n","epoch 12 - train loss/acc: 0.125 0.965, valid loss/acc: 0.150 0.957\n","epoch 13 - train loss/acc: 0.119 0.967, valid loss/acc: 0.146 0.958\n","epoch 14 - train loss/acc: 0.113 0.969, valid loss/acc: 0.140 0.959\n","epoch 15 - train loss/acc: 0.111 0.969, valid loss/acc: 0.140 0.960\n","epoch 16 - train loss/acc: 0.105 0.971, valid loss/acc: 0.134 0.961\n","epoch 17 - train loss/acc: 0.100 0.972, valid loss/acc: 0.130 0.962\n","epoch 18 - train loss/acc: 0.094 0.974, valid loss/acc: 0.126 0.964\n","epoch 19 - train loss/acc: 0.091 0.975, valid loss/acc: 0.124 0.965\n","epoch 20 - train loss/acc: 0.088 0.976, valid loss/acc: 0.122 0.966\n","epoch 21 - train loss/acc: 0.085 0.977, valid loss/acc: 0.119 0.967\n","epoch 22 - train loss/acc: 0.083 0.977, valid loss/acc: 0.119 0.966\n","epoch 23 - train loss/acc: 0.078 0.979, valid loss/acc: 0.115 0.966\n","epoch 24 - train loss/acc: 0.076 0.980, valid loss/acc: 0.112 0.967\n","Combination 134 - lr: 1.412, n_epochs: 25, batch_size: 512, num_hiddens: 128\n","epoch 00 - train loss/acc: 0.388 0.892, valid loss/acc: 0.391 0.891\n","epoch 01 - train loss/acc: 0.310 0.910, valid loss/acc: 0.314 0.908\n","epoch 02 - train loss/acc: 0.264 0.923, valid loss/acc: 0.271 0.920\n","epoch 03 - train loss/acc: 0.238 0.931, valid loss/acc: 0.247 0.930\n","epoch 04 - train loss/acc: 0.219 0.936, valid loss/acc: 0.228 0.935\n","epoch 05 - train loss/acc: 0.199 0.944, valid loss/acc: 0.211 0.939\n","epoch 06 - train loss/acc: 0.183 0.948, valid loss/acc: 0.195 0.944\n","epoch 07 - train loss/acc: 0.170 0.951, valid loss/acc: 0.184 0.948\n","epoch 08 - train loss/acc: 0.158 0.954, valid loss/acc: 0.174 0.948\n","epoch 09 - train loss/acc: 0.147 0.958, valid loss/acc: 0.165 0.951\n","epoch 10 - train loss/acc: 0.140 0.959, valid loss/acc: 0.158 0.953\n","epoch 11 - train loss/acc: 0.134 0.961, valid loss/acc: 0.154 0.955\n","epoch 12 - train loss/acc: 0.126 0.965, valid loss/acc: 0.146 0.957\n","epoch 13 - train loss/acc: 0.119 0.966, valid loss/acc: 0.141 0.959\n","epoch 14 - train loss/acc: 0.113 0.968, valid loss/acc: 0.138 0.959\n","epoch 15 - train loss/acc: 0.108 0.969, valid loss/acc: 0.134 0.960\n","epoch 16 - train loss/acc: 0.100 0.972, valid loss/acc: 0.126 0.962\n","epoch 17 - train loss/acc: 0.100 0.972, valid loss/acc: 0.126 0.963\n","epoch 18 - train loss/acc: 0.093 0.974, valid loss/acc: 0.122 0.963\n","epoch 19 - train loss/acc: 0.095 0.973, valid loss/acc: 0.124 0.964\n","epoch 20 - train loss/acc: 0.085 0.976, valid loss/acc: 0.116 0.966\n","epoch 21 - train loss/acc: 0.082 0.977, valid loss/acc: 0.112 0.967\n","epoch 22 - train loss/acc: 0.081 0.978, valid loss/acc: 0.114 0.966\n","epoch 23 - train loss/acc: 0.076 0.979, valid loss/acc: 0.108 0.968\n","epoch 24 - train loss/acc: 0.073 0.980, valid loss/acc: 0.106 0.968\n"]}]},{"cell_type":"code","metadata":{"id":"TTCqVT4S0Tm5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652292528959,"user_tz":-540,"elapsed":61764,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"a780d9e3-e643-42fd-dd08-5d4e4155120b"},"source":["# the best combination of hyperparameters from cross-validation\n","model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)\n","lr, n_epochs, batch_size = 1.590, 25, 128\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.246 0.926, valid loss/acc: 0.259 0.921\n","epoch 01 - train loss/acc: 0.175 0.948, valid loss/acc: 0.189 0.944\n","epoch 02 - train loss/acc: 0.137 0.960, valid loss/acc: 0.156 0.955\n","epoch 03 - train loss/acc: 0.113 0.968, valid loss/acc: 0.138 0.958\n","epoch 04 - train loss/acc: 0.091 0.974, valid loss/acc: 0.120 0.964\n","epoch 05 - train loss/acc: 0.079 0.978, valid loss/acc: 0.114 0.966\n","epoch 06 - train loss/acc: 0.072 0.980, valid loss/acc: 0.110 0.966\n","epoch 07 - train loss/acc: 0.059 0.984, valid loss/acc: 0.100 0.970\n","epoch 08 - train loss/acc: 0.057 0.984, valid loss/acc: 0.102 0.969\n","epoch 09 - train loss/acc: 0.052 0.985, valid loss/acc: 0.099 0.970\n","epoch 10 - train loss/acc: 0.045 0.988, valid loss/acc: 0.094 0.972\n","epoch 11 - train loss/acc: 0.037 0.991, valid loss/acc: 0.089 0.972\n","epoch 12 - train loss/acc: 0.034 0.992, valid loss/acc: 0.089 0.973\n","epoch 13 - train loss/acc: 0.032 0.993, valid loss/acc: 0.089 0.973\n","epoch 14 - train loss/acc: 0.027 0.995, valid loss/acc: 0.086 0.975\n","epoch 15 - train loss/acc: 0.026 0.995, valid loss/acc: 0.089 0.973\n","epoch 16 - train loss/acc: 0.026 0.995, valid loss/acc: 0.092 0.973\n","epoch 17 - train loss/acc: 0.021 0.997, valid loss/acc: 0.087 0.973\n","epoch 18 - train loss/acc: 0.022 0.995, valid loss/acc: 0.091 0.973\n","epoch 19 - train loss/acc: 0.017 0.998, valid loss/acc: 0.084 0.974\n","epoch 20 - train loss/acc: 0.016 0.998, valid loss/acc: 0.084 0.975\n","epoch 21 - train loss/acc: 0.014 0.998, valid loss/acc: 0.085 0.974\n","epoch 22 - train loss/acc: 0.014 0.999, valid loss/acc: 0.085 0.974\n","epoch 23 - train loss/acc: 0.012 0.999, valid loss/acc: 0.085 0.975\n","epoch 24 - train loss/acc: 0.012 0.999, valid loss/acc: 0.085 0.975\n"]}]},{"cell_type":"code","metadata":{"id":"hpPsAlXU0T_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652292532067,"user_tz":-540,"elapsed":266,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"9efde9f4-84a2-41d3-dba8-d875aca76166"},"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Final test loss = 0.070, acc = 0.978\n"]}]},{"cell_type":"markdown","metadata":{"id":"5hh1PZpk_g0I"},"source":["# Extra Credit (Optional)"]},{"cell_type":"code","metadata":{"id":"0R0n6y9_AgXc"},"source":["def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","    \"\"\"\n","    initializes parameters with He Initialization.\n","\n","    Question (e)\n","    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n","    \n","    Inputs\n","    - input_dim\n","    - num_hiddens\n","    - num_classes\n","    Returns\n","    - params: a dictionary with the initialized parameters.\n","    \"\"\"\n","    params = {}\n","    mu, sigma1, sigma2 = 0, np.sqrt(2/input_dim), np.sqrt(2/num_hiddens)\n","    params[\"W1\"] = np.random.normal(mu,sigma1,size=(input_dim,num_hiddens))\n","    params[\"b1\"] = np.zeros((num_hiddens,))\n","    params[\"W2\"] = np.random.normal(mu,sigma2,size=(num_hiddens,num_classes))\n","    params[\"b2\"] = np.zeros((num_classes,))\n","\n","    return params\n","\n","def forward_relu(self, X):\n","    \"\"\"\n","    Defines and performs the feed forward step of a two-layer neural network.\n","    Specifically, the network structue is given by\n","\n","        y = softmax(relu(X W1 + b1) W2 + b2)\n","\n","    where X is the input matrix of shape (N, D), y is the class distribution matrix\n","    of shape (N, C), N is the number of examples (either the entire dataset or\n","    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","    Question (e)\n","\n","    Inputs\n","        X: the input matrix of shape (N, D)\n","\n","    Returns\n","        y: the output of the model\n","        ff_dict: a dictionary containing all the fully connected units and activations.\n","    \"\"\"\n","    ff_dict = {}\n","    ff_dict[\"s1\"] = relu(X.dot(self.params[\"W1\"])) + self.params[\"b1\"]\n","    ff_dict[\"s2\"] = softmax(ff_dict[\"s1\"].dot(self.params[\"W2\"])) + self.params[\"b2\"]\n","    y = ff_dict[\"s2\"]\n","\n","    return y, ff_dict\n","\n","def backward_relu(self, X, Y, ff_dict):\n","    \"\"\"\n","    Performs backpropagation over the two-layer neural network, and returns\n","    a dictionary of gradients of all model parameters.\n","\n","    Question (e)\n","\n","    Inputs:\n","        - X: the input matrix of shape (B, D), where B is the number of examples\n","            in a mini-batch, D is the feature dimensionality.\n","        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","            where B is the number of examples in a mini-batch, C is the number\n","            of classes.\n","        - ff_dict: the dictionary containing all the fully connected units and\n","            activations.\n","\n","    Returns:\n","        - grads: a dictionary containing the gradients of corresponding weights\n","            and biases.\n","    \"\"\"\n","    grads = {}\n","    grads_softmax = ff_dict[\"s2\"] - np.where(Y==1,1,0)\n","    grads[\"W2\"] = ff_dict[\"s1\"].T.dot(grads_softmax)\n","    grads[\"b2\"] = np.sum(grads_softmax, axis=0)\n","\n","    grads_s1 = grads_softmax.dot(self.params[\"W2\"].T)\n","    grads_relu = grads_s1 * np.where(ff_dict[\"s1\"]>0,1,0)\n","    grads[\"W1\"] = X.T.dot(grads_relu)\n","    grads[\"b1\"] = np.sum(grads_relu, axis=0)\n","\n","    return grads\n","\n","TwoLayerNNRelu = copy.copy(TwoLayerNN)\n","TwoLayerNNRelu.initialize_parameters = initialize_parameters\n","TwoLayerNNRelu.feed_forward = forward_relu\n","TwoLayerNNRelu.back_propagate = backward_relu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qY3T98XvEIP"},"source":["### \n","# Question (e)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-jJRXqsBxzh","executionInfo":{"status":"ok","timestamp":1652291890730,"user_tz":-540,"elapsed":282,"user":{"displayName":"전현성","userId":"03887945282849978477"}}},"source":["# model instantiation\n","model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=64, num_classes=10)"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC8f80a0w53m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652291923134,"user_tz":-540,"elapsed":31243,"user":{"displayName":"전현성","userId":"03887945282849978477"}},"outputId":"6ffe4621-3136-46e3-db29-9531727b5b84"},"source":["# train the model\n","lr, n_epochs, batch_size = 1.5, 20, 256\n","history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.289 0.917, valid loss/acc: 0.294 0.912\n","epoch 01 - train loss/acc: 0.230 0.934, valid loss/acc: 0.240 0.930\n","epoch 02 - train loss/acc: 0.191 0.945, valid loss/acc: 0.204 0.940\n","epoch 03 - train loss/acc: 0.172 0.950, valid loss/acc: 0.189 0.943\n","epoch 04 - train loss/acc: 0.145 0.958, valid loss/acc: 0.166 0.949\n","epoch 05 - train loss/acc: 0.131 0.963, valid loss/acc: 0.152 0.956\n","epoch 06 - train loss/acc: 0.119 0.967, valid loss/acc: 0.145 0.957\n","epoch 07 - train loss/acc: 0.106 0.971, valid loss/acc: 0.134 0.961\n","epoch 08 - train loss/acc: 0.100 0.972, valid loss/acc: 0.129 0.961\n","epoch 09 - train loss/acc: 0.090 0.974, valid loss/acc: 0.121 0.965\n","epoch 10 - train loss/acc: 0.083 0.977, valid loss/acc: 0.116 0.964\n","epoch 11 - train loss/acc: 0.078 0.978, valid loss/acc: 0.114 0.966\n","epoch 12 - train loss/acc: 0.072 0.980, valid loss/acc: 0.108 0.968\n","epoch 13 - train loss/acc: 0.069 0.981, valid loss/acc: 0.108 0.968\n","epoch 14 - train loss/acc: 0.063 0.983, valid loss/acc: 0.103 0.969\n","epoch 15 - train loss/acc: 0.060 0.984, valid loss/acc: 0.102 0.970\n","epoch 16 - train loss/acc: 0.058 0.984, valid loss/acc: 0.103 0.969\n","epoch 17 - train loss/acc: 0.054 0.986, valid loss/acc: 0.100 0.969\n","epoch 18 - train loss/acc: 0.050 0.987, valid loss/acc: 0.097 0.971\n","epoch 19 - train loss/acc: 0.047 0.988, valid loss/acc: 0.097 0.971\n"]}]},{"cell_type":"code","metadata":{"id":"4i__6TfpCqOc"},"source":["Y_hat, _ = model_relu.forward(X_test)\n","test_loss = model_relu.compute_loss(Y_test, Y_hat)\n","test_acc = model_relu.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":null,"outputs":[]}]}